Tensor Decomposition via Simultaneous Power Iteration

(Supplementary Material)

Po-An Wang 1 Chi-Jen Lu 1

A. Technical Lemmas
For a matrix A, let σmax(A) and σmin(A) denote its largest
and smallest singular values, respectively. Then we will
need the following lemma relating such singular values of
a matrix and its sub-matrix.
Lemma A.1. (Corollary 3.1.3 in (Hom & Johnson, 1991))
Let A and B be matrices such that B is derived from A by
deleting some of its rows and/or columns. Then σmax(A) ≥
σmax(B) and σmin(A) ≤ σmin(B).
For a matrix Z, let Z(cid:12)2 = Z (cid:12) Z denote the Hadamard
(entry-wise) product of Z with itself. Then we will need the
following lemma relating the singular values of matrices Z
and Z(cid:12)2.
Lemma A.2. For any matrix Z, σmin(Z(cid:12)2) ≥ (σmin(Z))2
and σmax(Z(cid:12)2) ≤ (σmax(Z))2.

Proof. One can relate the singular values of the Hadamard
product Z(cid:12)2 to those of the Kronecker product Z ⊗ Z.
In particular, as Z (cid:12) Z can be obtain from Z ⊗ Z by
deleting some rows and columns, Lemma A.1 tells us that
σmin(Z (cid:12) Z) ≥ σmin(Z ⊗ Z) and σmax(Z (cid:12) Z) ≤
σmax(Z ⊗ Z). Then the lemma follows as the Kronecker
product Z⊗Z is known to have the property that σmin(Z⊗
Z) = (σmin(Z))2 and σmax(Z ⊗ Z) = (σmax(Z))2.1

We will need the following two tail bounds. The ﬁrst is for
the sum of the squares of independent standard normal ran-
dom variables, known as the χ-square distribution, which
follows from the bound in (Laurent & Massart, 2000).
Lemma A.3. Let z1, . . . , zL be a sequence of i.i.d. random
variables, each from the distribution N (0, 1). Then for any
1Academia Sinica, Taiwan. Correspondence to: Po-An Wang

<poanwang@iis.sinica.edu.tw>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

1See e.g. Theorem 4.2.12 in (Hom & Johnson, 1991) for the
case of square matrices; the extension to rectangular matrices is
straightforward.

δ ∈ (0, 1), we have

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

L

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≥ δ

i − 1
z2

 ≤ 2−Ω(cid:0)δ2L(cid:1)

.

(cid:88)

i∈[L]

Pr

The second is the following matrix version of the Bernstein
inequality (see e.g. Theorem 1.6 in (Tropp, 2012)).
Lemma A.4. Consider a ﬁnite sequence Z1, . . . , Zn of in-
dependent, random, matrices in Rd×k. Assume that each
random matrix satisﬁes E[Zi] = 0 and (cid:107)Zi(cid:107) ≤ R almost
surely. Deﬁne the variance parameter

σ2 = max

E[ZiZ(cid:62)
i ]

E[Z(cid:62)

i Zi]

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ,

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) n(cid:88)

i=1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:41)

.

(cid:40)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) n(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≥ t
(cid:34)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) n(cid:88)
(cid:35)

Zi

i=1

i=1

Then, for all t ≥ 0,

Pr

≤ (d + k) · 2

−t2

σ2+Rt/3 .

We will also need the following two matrix perturbation
bounds.
Lemma A.5. (Theorem 2.5 in (Stewart & Sun, 1990)) Let

A, E ∈ Rk×k be given. If A is invertible, and(cid:13)(cid:13)A−1E(cid:13)(cid:13) <

1 then ¯A := A + E is invertible, and

(cid:13)(cid:13) ¯A−1 − A−1(cid:13)(cid:13) ≤ (cid:107)E(cid:107)(cid:13)(cid:13)A−1(cid:13)(cid:13)2

1 − (cid:107)A−1E(cid:107) .

Lemma A.6. (Lemma 2.2 in (Schmitt, 1992)) Given any
A, ¯A ∈ Rk×k with smallest singular values σ > 0 and
¯σ > 0, respectively, we have

(cid:13)(cid:13)(cid:13) ¯A

1

2 − A

1
2

(cid:13)(cid:13)(cid:13) ≤

(cid:13)(cid:13) ¯A − A(cid:13)(cid:13)

¯σ 1

2 + σ 1

2

.

B. Proofs in Section 3
B.1. Proof of Lemma 1

Recall that Q(t) is derived from Y (t) by the QR decom-
position Y (t) = Q(t) · R(t) via the Gram-Schmidt process,

Tensor Decomposition via Simultaneous Power Iteration

[m]

, Q(cid:48) for Q(t)

which has the same effect as performing k copies of the QR
[m], for m ∈ [k], to
decomposition on the k sub-matrices Y (t)
[m], for m ∈ [k].
obtain the k sub-matrices Q(t)
Let us ﬁx any m ∈ [k] and t ≥ 0. To simply our no-
tation, we will drop the indices of m and t in the fol-
lowing. We will write Q for Q(t−1)
[m], Y for
Y (t)
[m], and ˆΦ for ˆΦ(t)
[m]. We will write U for U[m], with
the vector u1, . . . , um as its columns, while we will use
V to denote the d × (d − m) matrix having the vectors
um+1, . . . , ud as its columns. We will write tan, cos, sin
for tanm, cosm, sinm, respectively. Furthermore, for a ma-
trix A, let σmin(A) and σmax(A) denote its smallest and
largest singular values, respectively.
Recall that our goal is to bound tan(Q(cid:48)) in terms of
tan(Q). As discussed before, Q(cid:48) is derived from Y by a
QR decomposition, with Y = Q(cid:48)R for some matrix R. To
achieve our goal, we will ﬁrst show that R is invertible so
that Q(cid:48) = Y R−1, and then relate tan(Q(cid:48)) to the singular
values σmax(V (cid:62)Y ) and σmin(U(cid:62)Y ), followed by bound-
ing these two singular values.
First, from the condition (3), we have cos Q > 0 which
implies that Q has full rank and consists of orthonormal
columns. Our key lemma is the following, which we will
prove later in Subsection B.1.1.
Lemma B.1. The following two bounds hold:

(cid:0)U(cid:62)Y(cid:1) ≥ λm cos2(Q) − (cid:107) ˆΦ(cid:107), and
(cid:0)V (cid:62)Y(cid:1) ≤ λm+1 sin2(Q) + (cid:107) ˆΦ(cid:107).
(cid:0)U(cid:62)Y(cid:1) ≥ λm cos2(Q) − (cid:52) cos2(Q) > 0,

Using this lemma and the assumption (cid:107) ˆΦ(cid:107) ≤ (cid:52) cos2(Q)
in (3), we have

σmin
as ∆ ≤ λm
2 . This implies that Y has full rank, R−1 exists,
Q(cid:48) = Y R−1 has orthonormal columns, and (U(cid:62)Q(cid:48))−1 ex-
ists. Then from standard properties of principal angles (see
e.g. (Zhu & Knyazev, 2012)), we know that

=(cid:13)(cid:13)(V (cid:62)Q(cid:48))(U(cid:62)Q(cid:48))−1(cid:13)(cid:13) ,
(cid:13)(cid:13)(V (cid:62)Y R−1)(U(cid:62)Y R−1)−1(cid:13)(cid:13) = (cid:13)(cid:13)(V (cid:62)Y )(U(cid:62)Y )−1(cid:13)(cid:13)

σmax(V (cid:62)Q(cid:48))
σmin(U(cid:62)Q(cid:48))

tan(Q(cid:48)) =

which equals

• σmin
• σmax

≤ σmax(V (cid:62)Y )
σmin(U(cid:62)Y )

.

while using the assumption (cid:107) ˆΦ(cid:107) ≤ (cid:52) cos2(Q), we can
bound the enumerator by

σmin(U(cid:62)Y ) ≥ λm cos2(Q) − (cid:52) cos2(Q).

Combining these bounds together, we obtain

tan(Q(cid:48)) ≤ λm+1 + (cid:52)β

λm − (cid:52) tan2(Q) +

(cid:52)β

λm − (cid:52) .

Then the rest of the analysis is identical to that of Hardt &
Price (2014) (for the proof of their Lemma 2.2). Specif-
ically, we can rewrite the righthand side above as the
weighted average of two terms

with α =

(1 − α) · λm+1 + (cid:52)β
λm+1 + 2(cid:52) tan2(Q) + α · β,
(cid:26) λm+1 + (cid:52)β
(cid:27)

λm+1+3(cid:52), which can be upper-bounded by

(cid:52)

λm+1 + 2(cid:52) tan2(Q), β

,

max

(cid:26) λm+1

(cid:27)

and similarly, we can also have

λm+1 + (cid:52) , β

λm+1 + (cid:52)β
λm+1 + 2(cid:52) ≤ max
λm+1+(cid:52) ≤ (
tan(Q(cid:48)) ≤ max(cid:8)max{ρ, β} tan2(Q), β(cid:9) .

λm+1+4(cid:52) )1/4 = ( λm+1

Since
thus have the desired bound

λm+1

λm+1

λm

.

)1/4 = ρ, we

To ﬁnish the proof, it remains to prove Lemma B.1, which
we do next.

B.1.1. PROOF OF LEMMA B.1

Recall from (2) that for any column Yj of Y and for any
target vector ui,
u(cid:62)
i Yj = λi

+ u(cid:62)

ˆΦj.

i

These equations can be summarized as

(cid:1)2
(cid:0)u(cid:62)
U(cid:62)Y = Λ(cid:0)U(cid:62)Q(cid:1)(cid:12)2
V (cid:62)Y = ¯Λ(cid:0)V (cid:62)Q(cid:1)(cid:12)2

i Qj

+ U(cid:62) ˆΦ, and
+ V (cid:62) ˆΦ,

Then by Lemma B.1, together with the assumption from
(3) that (cid:107) ˆΦ(cid:107) ≤ (cid:52)β = (cid:52)β sin2(Q) + (cid:52)β cos2(Q), we can
bound the denominator by
σmax(V (cid:62)Y ) ≤ (λm+1 + (cid:52)β) sin2(Q) + (cid:52)β cos2(Q),

σmin

using the notation Λ for the m × m diagonal matrix with
λ1, . . . , λm at its diagonal, ¯Λ for the (d − m) × (d − m)
diagonal matrix with λm+1, . . . , λd at its diagonal, and
A(cid:12)2 = A (cid:12) A for the Hadamard (entry-wise) product of
matrix A with itself. From this, we have

(cid:0)U(cid:62)Y(cid:1) = σmin

(cid:16)
(cid:16)

+ U(cid:62) ˆΦ

(cid:17)
Λ(cid:0)U(cid:62)Q(cid:1)(cid:12)2
Λ(cid:0)U(cid:62)Q(cid:1)(cid:12)2(cid:17) −(cid:13)(cid:13)(cid:13)U(cid:62) ˆΦ
(cid:13)(cid:13)(cid:13)
(cid:16)(cid:0)U(cid:62)Q(cid:1)(cid:12)2(cid:17) −(cid:13)(cid:13)(cid:13) ˆΦ
(cid:13)(cid:13)(cid:13) ,

≥ σmin
≥ σmin (Λ) σmin

Tensor Decomposition via Simultaneous Power Iteration

as well as

σmax

+

+ V (cid:62) ˆΦ

(cid:0)V (cid:62)Y(cid:1) = σmax

≤ σmax
≤ σmax

(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13) ˆΦ
(cid:13)(cid:13)(cid:13) .

(cid:16)¯Λ(cid:0)V (cid:62)Q(cid:1)(cid:12)2
(cid:16)¯Λ(cid:0)V (cid:62)Q(cid:1)(cid:12)2(cid:17)
(cid:0)¯Λ(cid:1) σmax
(cid:16)(cid:0)U(cid:62)Q(cid:1)(cid:12)2(cid:17) ≥(cid:0)σmin
(cid:16)(cid:0)V (cid:62)Q(cid:1)(cid:12)2(cid:17) ≤(cid:0)σmax
(cid:0)¯Λ(cid:1) = λm and σmax (Λ) = λm+1, Lemma B.1

(cid:17)
(cid:13)(cid:13)(cid:13)V (cid:62) ˆΦ
(cid:16)(cid:0)V (cid:62)Q(cid:1)(cid:12)2(cid:17)
(cid:0)U(cid:62)Q(cid:1)(cid:1)2
(cid:0)V (cid:62)Q(cid:1)(cid:1)2

= cos2(Q),

= sin2(Q).

+

since Q has orthonormal columns, and moreover

From Lemma A.2, we have

σmin

σmax

As σmin
follows.

B.2. Proof of Theorem 2
Suppose we have Q(0) such that for every m ∈ [k],
tanm(Q(0)) ≤ 1 and hence cosm(Q(0)) ≥ 1√
. We would
2
like to apply Lemma 1 repeatedly with β = ε
2 to achieve
tanm(Q(t)) ≤ ε
2 for every m. To be able to do this, we
need to verify that for every t, the condition (3) in Lemma 1
is satisﬁed. For this, we ﬁrst claim that (cid:107) ˆΦ(t)
2 . This
holds since for any vector x = (x1, . . . , xm) of unit length,
|xj|·(cid:107)Φ(cid:107)

(cid:13)(cid:13)(cid:13)xjΦ(Id, Q(t)

(cid:13)(cid:13)(cid:13) ≤ (cid:88)

(cid:13)(cid:13)(cid:13) ≤ (cid:88)

(cid:13)(cid:13)(cid:13) ˆΦ(t)

[m](cid:107) ≤ ∆ε

j , Q(t)
j )

[m]x

j∈[m]

j∈[m]

which by the Cauchy-Schwarz inequality is at most

√

m(cid:107)x(cid:107) · (cid:107)Φ(cid:107) =

√

m · (cid:107)Φ(cid:107) ≤ ∆ε
2

.

Moreover, we can assume without loss of generality that
2 , ρ} = ρ because otherwise, we immediately have
max{ ε
tanm(Q(1)) ≤ ε
2 for every m, as the condition (3) is sat-
isﬁed for t = 1. Then a simple induction shows that for
(cid:111)
every t ≥ 1, the condition (3) in Lemma 1 holds and
m(Q(t−1))

, ρ tan2

tanm(Q(t)) ≤ max
≤ max

, ρ2t−1(cid:111)
for every m. Thus, we have ρ2t−1 ≤ ε
2 whenever t ≥ N − 1, for some
(cid:18) 1
(cid:18)
= O

N = O

(cid:32)

(cid:33)

log

log

2

ε

(cid:110) ε
(cid:110) ε

2

2 and tanm(Q(t)) ≤

(cid:19)(cid:19)

log

1
ε

γ

,

is close

log 1
ε
log 1
ρ
ρ ≥ Ω(log 1

by noting that log 1
Next, we show that for any t ≥ N, each Q(t)
i
enough to ui. For this, we rely on the following.

1−γ ) ≥ Ω(γ).

(cid:113)

Proposition B.1. For any t ≥ N, we have u(cid:62)

i Q(t)

i ≥

1 − ε2

2 for every i ∈ [k].

Proof. Let us ﬁx any i ∈ [k]. In the following, we ﬁrst
i )2 ≥ 1− ε2
2 for any t ≥ N − 1, and then
show that (u(cid:62)
i Q(t)
i ≥ 0 for any t ≥ N, which together
we show that u(cid:62)
i Q(t)
prove the proposition.
First, consider any t ≥ N − 1, which from the discus-
i )2 ≥
sion above has tani(Q(t)) ≤ ε
cos2
(cid:17)2
(cid:17)2

i (Q(t)) ≤ (cid:13)(cid:13)(cid:13)u(cid:62)
(cid:13)(cid:13)(cid:13)u(cid:62)

2. Note that (u(cid:62)

(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)2

i (Q(t)) − sin2

i−1(Q(t)), because

(cid:16)
(cid:16)

i Q(t)

i Q(t)

i Q(t)

[i−1]

cos2

+

[i]

=
≤ sin2

i

u(cid:62)
i Q(t)
u(cid:62)
i Q(t)

i

i−1(Q(t)) +

.

i (Q(t)) =

1
1+tan2

i (Q(t)) and

From this and the fact that cos2
sin2

i−1(Q(t)) ≤ tan2

(cid:16)

u(cid:62)
i Q(t)

i

(cid:17)2 ≥ 1

i−1(Q(t)), we get
− ε2
4

1 + ε2
4

≥ 1 − ε2
2

.

Next, consider any t ≥ N and our goal is to show that
u(cid:62)
i Q(t)

i > 0. Recall that Q(t)

is derived from

i

(cid:16)

(cid:88)

j

Y (t)
i =

j Q(t−1)
u(cid:62)

i

λj

(cid:17)2 · uj + ˆΦ(t−1)

i

by subtracting from it its projection to some unit vector z
in the column space of Q(t)
[i−1] and then scaling it to unit
length. Thus, the sign of u(cid:62)
i Q(t)

is the same as that of

u(cid:62)

i

i

(cid:17)
(cid:17)
(cid:17)(cid:0)u(cid:62)
i z(cid:1)
(cid:17)2 −(cid:13)(cid:13)(cid:13) ˆΦ(t−1)

z
z(cid:62)Y (t)

i

i

i

(cid:16)

z(cid:62)Y (t)

i −(cid:16)
i −(cid:16)
(cid:16)
i Y (t)
(cid:19)
(cid:18)
i Q(t−1)
u(cid:62)
1 − ε2
2

Y (t)
= u(cid:62)
≥ λi
≥ λi

i

,

− 5ε
8
)2 ≥ 1− ε2

(cid:13)(cid:13)(cid:13) − sini−1(Q(t))

i

∆ε

2 for t−1 ≥ N−1, (cid:107) ˆΦ(t−1)

i Q(t−1)
8, and sini−1(Q(t)) ≤ tani−1(Q(t)) ≤ ε

(cid:107) ≤
since (u(cid:62)
2 for t ≥
2 ≤ ε
N. Finally, as λi ≥ λk ≥ 2ε, the last line above is positive,
which implies that u(cid:62)

i Q(t)

i > 0.

i

i (cid:107) = (cid:107)ui(cid:107) = 1, this proposition immediately im-

As (cid:107)Q(t)
plies that for any t ≥ N and i ∈ [k],

(cid:13)(cid:13)(cid:13)Q(t)

i − ui

(cid:113)

(cid:13)(cid:13)(cid:13) =

2 − 2u(cid:62)

i Q(t)

i ≤ ε,

Tensor Decomposition via Simultaneous Power Iteration

r∈[d]

that the matrix Φ(ui, Id, Id) has norm (cid:107)Φ(ui, Id, Id)(cid:107) ≤
(cid:107)Φ(cid:107) ≤ ∆
˜λr · ˜ur ⊗ ˜ur,
for some orthonormal vectors ˜ur’s as well as some values
˜λr’s, each with |˜λr| ≤ ∆
3d. Then by a similar analysis as
above, together with a union bound, we can have with prob-
200d2 = 1 − 1
ability at least 1 − d ·

3d and can be decomposed as(cid:80)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) ≤ (cid:88)
≤ (cid:88)

(cid:88)
(cid:0)˜u(cid:62)
(cid:17)

(cid:12)(cid:12)(cid:12) · 1
·(cid:16)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

(cid:12)(cid:12)(cid:12)˜λr

Φ(ui, wj, wj)

(cid:88)

200d that

(cid:1)2

j∈[L]

r∈[d]

r wj

1 +

L

L

1

j∈[L]
γ
4

∆
3d

r∈[d]
≤ ∆
2

.

By combining the two bounds above, we can conclude that
for any i ∈ [d], the sum in (1) differs from λi by at most
4 (λiγ + 2∆) with probability at least 1 − 1
100d. Then the
1
lemma immediately follows by a union bound.

B.4. Proof of Lemma 3

Consider any ¯w satisfying the condition (5) in Lemma 2.
By deﬁnition, ¯M = ¯T (Id, Id, ¯w) can be decomposed as

T (Id, Id, ¯w) + Φ(Id, Id, ¯w).

i Q(t)

i Q(t)

i ≥ (u(cid:62)

i )2 ≥ 1 − ε2
2 .

as u(cid:62)
Finally, let us show that for any t ≥ N, each λi can be
, Q(t)
approximated well by ˆλi = ¯T (Q(t)
i ). Fix any
i
t ≥ N and i ∈ [k]. Note that |λi − ˆλi| is at most

, Q(t)

i

i

i

Q(t)

, Q(t)

, Q(t)

, Q(t)
, Q(t)
The second term above is at most (cid:107)Φ(cid:107)(cid:107)Q(t)
i (cid:107)3 ≤ ε
the ﬁrst term above is at most

Q(t)

i

i

i

4, and

(cid:17)(cid:12)(cid:12)(cid:12) .

(cid:16)

i

(cid:12)(cid:12)(cid:12)Φ
(cid:17)(cid:12)(cid:12)(cid:12) +
(cid:12)(cid:12)(cid:12)(cid:12)λj
(cid:16)
(cid:88)
(cid:17)3(cid:19)
(cid:88)

j(cid:54)=i

+

λj

j(cid:54)=i

u(cid:62)
j Q(t)

i

(cid:17)3(cid:12)(cid:12)(cid:12)(cid:12)
1 −(cid:16)

(cid:18)

(cid:16)

(cid:12)(cid:12)(cid:12)λi − T
(cid:12)(cid:12)(cid:12)(cid:12)λi − λi
(cid:16)
(cid:18)

≤ λi

i

u(cid:62)
i Q(t)

(cid:17)3(cid:12)(cid:12)(cid:12)(cid:12) +
1 −(cid:16)
(cid:88)

u(cid:62)
i Q(t)

i

(cid:17)2(cid:19)

u(cid:62)
i Q(t)

i

≤ λi

3ε2
4

+

λj

ε2
2

,

j(cid:54)=i

where the ﬁrst inequality uses the fact that for j (cid:54)= i,
(u(cid:62)
j Q(t)
i )2 =
1 − (u(cid:62)
i )2, while the second inequality uses the bound
(u(cid:62)
i Q(t)
4 . As a result, we have

i )3 ≤ (u(cid:62)
i Q(t)
i )3 ≥ (1 − ε2

i (cid:107)2 − (u(cid:62)

i Q(t)

j Q(t)

i )2 ≤ (cid:107)Q(t)
2 )3/2 ≥ 1 − 3ε2
3ε2
4

λj

+

(cid:12)(cid:12)(cid:12)λi − ˆλi
(cid:12)(cid:12)(cid:12) ≤(cid:88)
using the assumption(cid:80)

j

the proof of the theorem.

≤ ε,

ε
4

(cid:88)

j∈[L]

1
L

B.3. Proof of Lemma 2
From the deﬁnition of ¯w, we can express u(cid:62)

i ¯w as

1
L

(cid:88)
(cid:88)

j∈[L]

1
L

j∈[L]

T (ui, wj, wj) +

The ﬁrst term above equals

(cid:0)u(cid:62)

λi

i wj

(cid:1)2

= λi · 1
L

Φ(ui, wj, wj).

(1)

(cid:0)u(cid:62)

i wj

(cid:1)2

,

(cid:88)

j∈[L]

and note that the sum has a χ-square distribution be-
cause each u(cid:62)
i wj is an independent random variable with
the standard normal distribution N (0, 1).2 Then from
Lemma A.3, we know that for δ = γ
4 , there exists some
L ≤ O( 1
γ2 log d) such that the ﬁrst term in (1) differs from
λi by at most λiγ
The second term in (1) can be bounded in a similar way
as follows. Since (cid:107)ui(cid:107) = 1 and Φ is symmetric, we know
2This is because each component of wj has the distribution
r ui,r · 0 = 0
i,r · 1 = 1, where ui,r denotes the r’th com-

4 with probability at least 1 − 1

i wj has mean(cid:80)

and variance(cid:80)

N (0, 1), and the distribution of u(cid:62)

200d2 .

r u2

ponent of ui.

j λj ≤ 1 from (1). This completes

The ﬁrst matrix can be expressed as

(cid:88)

i∈[d]

(cid:0)u(cid:62)
i ¯w(cid:1) · ui ⊗ ui,

T (Id, Id, ¯w) =

λi

with ¯λi = λi(u(cid:62)
vector, respectively. Note that as ∆ ≤ λi−λi+1

i ¯w) and ui as its i’th eigenvalue and eigen-

, we have

¯λi ≥ λ2
≥ λ2

≥ λ2

as well as
¯λi+1 ≤ λ2
≤ λ2

i+1 +

i+1 + λ2

≤ λ2

i+1 +

i − λiλi+1

8

,

4

i+1

i+1

− λ2

i − 1
4
i − λ2

i − λ2
4
i − λ2
8

(cid:0)λ2
i γ + 2λi∆(cid:1)
(cid:1)
i − 3(cid:0)λ2
(cid:0)λ2
i+1γ + 2λi+1∆(cid:1)
3(cid:0)λ2

i − λ2
λ2
4λ2
i
i − λ2
8

(cid:1)

1
4

i+1

i+1

i+1

+

,

λi+1λi − λ2

i+1

8

which together imply that

¯λi − ¯λi+1 ≥ λ2

i − λ2
4

i+1

≥ ∆2.

Tensor Decomposition via Simultaneous Power Iteration

It remains to bound the norm of Φ(Id, Id, ¯w), which is

where (cid:107) ¯w(cid:107)2 =(cid:80)

i∈[d]

(cid:18)

(cid:88)

i∈[d]

(cid:107)Φ(Id, Id, ¯w)(cid:107) ≤ (cid:107)Φ(cid:107) · (cid:107) ¯w(cid:107),

(cid:0)u(cid:62)
i ¯w(cid:1)2 is at most
(cid:19)2 ≤ (cid:88)

i∈[d]

λi +

1
4

(λiγ + 2∆)

It remains to show that we can have an initial Z (0), such
that for each m ∈ [k], Z (0)
[m] satisﬁes the condition required
by Lemma B.2. For this, we need the following bound from
(Mitliagkas et al., 2013).
Proposition B.2. For any δ, we have

(cid:20)

(cid:21)

(2λi)2 ≤ 4.

Pr

cosk(Z (0)) ≤ δ√
dk

≤ O(δ) + 2−Ω(d).

This implies that (cid:107)Φ(Id, Id, ¯w)(cid:107) ≤ 2(cid:107)Φ(cid:107), which completes
the proof of the lemma.

B.5. Proof of Lemma 4
Suppose we have a matrix ¯M = M + ¯Φ, where

(cid:88)

i∈[d]

M =

¯λi · ui ⊗ ui

with ¯λi − ¯λi+1 ≥ ∆2 for every i ∈ [k] and (cid:107) ¯Φ(cid:107) ≤ α1∆2√
dk
for a small enough constant α1. The key observation is that
although we run one copy of the matrix power method of
Hardt & Price (2014) to update the whole d×k matrix Z (s),
we can actually see our algorithm as running k copies of
the matrix power method on k sub-matrices Z (s)
[1] , . . . , Z (s)
[k]
simultaneously. This allows us to apply their analysis im-
mediately.
More precisely, although our QR decomposition at each
step s is applied to the whole d × k matrix Y (s) to ob-
tain our d × k matrix Z (s), the Gram-Schmidt process we
use has the effect that each d× m sub-matrix Z (s)
[m] can also
be seen as obtained from the d × m matrix Y (s)
[m] by a QR
decomposition. Thus, our algorithm can be seen as running
k copies of the algorithm of (Hardt & Price, 2014) simulta-
neously, and we can apply the following lemma of theirs3
simultaneously for every m ∈ [k] with X (s) being our d×k
matrix Z (s)
[m].
Lemma B.2. Fix any m ∈ [k]. Suppose that the initial
m = ¯Φ · X (s) at each step s is such
X (0) and the noise G(s)
that

(cid:13)(cid:13)(cid:13)U(cid:62)
(cid:13)(cid:13)(cid:13)G(s)

[m]G(s)
m

5

m

(cid:13)(cid:13)(cid:13) ≤ (cid:0)¯λm − ¯λm+1
(cid:13)(cid:13)(cid:13) ≤ (cid:0)¯λm − ¯λm+1

5

(cid:1) cosm(X (0))
(cid:1) 

2 . Then for γm = 1 − ¯λm+1

, there exists
) such that for any t ≥ S

¯λm

for some  < 1
some S = O( 1
we have tanm(X (t)) ≤ .

γm

log tanm(X (0))



3It corresponds to Theorem 2.3 in (Hardt & Price, 2014). Al-
though it is stated there for m = k, it in fact works for any value
of k and hence m.

By applying this proposition, with δ = 10α0 for a small
enough constant α0, we can have cosk(Z (0)) ≥ 10α0√
dk
with high probability. From Lemma A.1, we know that
for any m ∈ [k], cosm(Z (0)) = σmin(U(cid:62)
[m]) ≥
σmin(U(cid:62)Z (0)) = cosk(Z (0)). Thus, with high probabil-
for every m ∈ [k].
ity we in fact have cosm(Z (0)) ≥ 10α0√
Given such an initial Z (0), we can have for every s and m
that

[m]Z (0)

dk

5(cid:107)G(s)

m (cid:107) ≤ 5(cid:107) ¯Φ(cid:107) ≤ 10α0∆2√
dk

which satisﬁes the two conditions needed by Lemma B.2,
with  = 1
3. Then we can repeatedly apply Lemma B.2,
simultaneously for every m ∈ [k], and a simple induc-
tion shows that for some S = O( 1
γ log d), we have
tanm(Z (s)) ≤  < 1 for any m ∈ [k] and s ≥ S. This
completes the proof of our Lemma 4

C. Proofs in Section 5
C.1. Proof of Lemma 5

First, we claim that tank(Q) < 1 with high probability.
To show this, note that by Proposition B.2, we have with
high probability that cosk(Z) > 4α0√
for a small enough
constant α0. In the following, let us assume that we indeed
have such a matrix Z, and note that it has tank(Z) <
.
dk
4α0
Then we need the following.
Lemma C.1. (Lemma 2.2 in (Hardt & Price, 2014)) Let
Z, G ∈ Rd×k satisfy

√

dk

4(cid:13)(cid:13)U(cid:62)G(cid:13)(cid:13) ≤ (λk − λk+1) cosk(Z)
(cid:17)1/4

4(cid:107)G(cid:107) ≤ (λk − λk+1) β

(cid:16) λk+1

for some β < 1. Then for ρ =
tank(M Z + G) ≤ max{β, max{β, ρ} tank(Z)}.
Recall that we assume λk+1 = 0, and to apply the lemma,
let β = 4α0√

and G = ¯ΦZ. Note that

, we have

λk

dk

(cid:107)G(cid:107) ≤(cid:13)(cid:13) ¯Φ(cid:13)(cid:13) ≤ λkβ

,

4

which satisﬁes both requirements of the lemma, and thus
with ¯Y = M Z + G, we have

tank(Q) = tank( ¯Y ) ≤ β tank(Z) < 1.

Next, let us bound σmin(P ) and σmax(P ). Recall that

P = Q(cid:62)M Q = Q(cid:62)U ΛU(cid:62)Q,

(cid:0)Q(cid:62)U(cid:1) σmin (Λ) σmin
(cid:0)Q(cid:62)U(cid:1) σmax (Λ) σmax

which implies that
σmin(P ) ≥ σmin
σmax(P ) ≤ σmax
Since the matrix Q has orthonormal columns, we have
≥ 1
2

(cid:0)U(cid:62)Q(cid:1))2 = (cosk(Q))2 =

(cid:0)U(cid:62)Q(cid:1) and
(cid:0)U(cid:62)Q(cid:1) .

1 + tan2

(σmin

1

,

k(Q)

as well as

σmax

(cid:0)U(cid:62)Q(cid:1) ≤ (cid:107)U(cid:107)(cid:107)Q(cid:107) = 1.

Finally, as σmax (Λ) = λ1 and σmin (Λ) = λk, we have

σmin(P ) ≥ λk
2

and σmax(P ) ≤ λ1.

C.2. Proof of Lemma 6

First, from the deﬁnition, we have

(cid:13)(cid:13) ¯P − P(cid:13)(cid:13) =(cid:13)(cid:13)Q(cid:62) ¯M Q − Q(cid:62)M Q(cid:13)(cid:13) ≤ (cid:107)Q(cid:107)2(cid:13)(cid:13) ¯Φ(cid:13)(cid:13) ≤ 

2

as Q has orthonormal columns so that (cid:107)Q(cid:107)2 ≤ 1. There-
fore, given the assumption that 0 <  ≤ σmin(P )
, we have

σmin( ¯P ) ≥ σmin(P ) −(cid:13)(cid:13) ¯P − P(cid:13)(cid:13) > 0,
(cid:13)(cid:13) ¯P − P(cid:13)(cid:13)(cid:13)(cid:13)P −1(cid:13)(cid:13)2
1 −(cid:13)(cid:13) ¯P − P(cid:13)(cid:13)(cid:107)P −1(cid:107)

(cid:13)(cid:13) ¯P −1 − P −1(cid:13)(cid:13) ≤

which implies that ¯P is invertible.
Then according to Lemma A.5, we have

≤ 2(σmin(P ))−2,

as(cid:13)(cid:13)P −1(cid:13)(cid:13) = (σmin(P ))−1 and (cid:107) ¯P − P(cid:107) ≤  ≤ σmin(P )
(cid:13)(cid:13)(cid:13) ¯P − 1

(cid:13)(cid:13) ¯P −1 − P −1(cid:13)(cid:13)

Combining this with Lemma A.6, we obtain

(cid:13)(cid:13)(cid:13) ≤

2 − P − 1

2 + (σmin(P −1)) 1

(σmin( ¯P −1)) 1

.

2

2

2

≤ 2(σmin(P ))−2(σmax(P ))

1
2 ,

since σmin( ¯P −1) ≥ 0 and σmin(P −1) = (σmax(P ))−1.

C.3. Proof of Theorem 3
First, given ε ∈ (0, 1
},
for a small enough constant α0, we know from Lemma 5
and Lemma 6 that with high probability,

2 ) and (cid:107) ¯Φ(cid:107) ≤ α0ε min{ λk√

, λ3
k√
λ1

dk

(2)
(3)
(4)
(5)

The term in (2) is at most

as (cid:107)Φ(cid:107) ≤ α0λ

Assume from now on that the above three conditions hold.
Next, observe that

(cid:13)(cid:13) ¯T(cid:0) ¯W , ¯W , ¯W(cid:1) − T (W, W, W )(cid:13)(cid:13)
≤ (cid:13)(cid:13) ¯T(cid:0) ¯W , ¯W , ¯W(cid:1) − T(cid:0) ¯W , ¯W , ¯W(cid:1)(cid:13)(cid:13) +
(cid:13)(cid:13)T(cid:0) ¯W , ¯W , ¯W(cid:1) − T(cid:0)W, ¯W , ¯W(cid:1)(cid:13)(cid:13) +
(cid:13)(cid:13)T(cid:0)W, ¯W , ¯W(cid:1) − T(cid:0)W, W, ¯W(cid:1)(cid:13)(cid:13) +
(cid:13)(cid:13)T(cid:0)W, W, ¯W(cid:1) − T (W, W, W )(cid:13)(cid:13) .
(cid:13)(cid:13)Φ(cid:0) ¯W , ¯W , ¯W(cid:1)(cid:13)(cid:13) ≤ (cid:107)Φ(cid:107)(cid:13)(cid:13) ¯W(cid:13)(cid:13)3 ≤ ε
(cid:13)(cid:13)(cid:13) ≤(cid:13)(cid:13)(cid:13) ¯P − 1
(cid:13)(cid:13) ¯W(cid:13)(cid:13) ≤ (cid:107)Q(cid:107)(cid:13)(cid:13)(cid:13) ¯P − 1
(cid:13)(cid:13)(cid:13) ,
(cid:13)(cid:13)(cid:13) ≤ 4λ
(cid:13)(cid:13)(cid:13) ¯P − 1
(cid:13)(cid:13)(cid:13) +
(cid:13)(cid:13)(cid:13)P − 1
≤ (cid:13)(cid:13)(cid:13) ¯P − 1

(cid:13)(cid:13)T(cid:0) ¯W − W, ¯W , ¯W(cid:1)(cid:13)(cid:13) ≤ (cid:107)T(cid:107)(cid:13)(cid:13) ¯W − W(cid:13)(cid:13)(cid:13)(cid:13) ¯W(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13) 16λ−1

k ε for a small enough constant α0 and

2 − P − 1

2 − P − 1

− 1
k .

4

3
2

k

2

2

2

2

2

2

which can be upper-bounded by

The term in (3) is at most

≤ ε
4

.

Similarly, the term in (4) can be upper-bounded by

4
and the term in (5) can be upper-bounded by

(cid:107)T(cid:107)(cid:13)(cid:13) ¯W − W(cid:13)(cid:13)(cid:107)W(cid:107)(cid:13)(cid:13) ¯W(cid:13)(cid:13) ≤ ε
(cid:107)T(cid:107)(cid:13)(cid:13) ¯W − W(cid:13)(cid:13)(cid:107)W(cid:107)2 ≤ ε
(cid:13)(cid:13) ¯T(cid:0) ¯W , ¯W , ¯W(cid:1) − T (W, W, W )(cid:13)(cid:13) ≤ ε

4

.

As a result, we can conclude that

with high probability, which proves the theorem.

Tensor Decomposition via Simultaneous Power Iteration

• σmin(P ) ≥ λk
2 − P − 1
• (cid:107) ¯P − 1

2 , and
2(cid:107) ≤ λkε
64 .

• σmax(P ) ≤ λ1,

L =

c0 log k

∆2

, S =

c0 log k

γ

, N = c0 log

log

1
ε

D. Proofs in Section 6
Our streaming algorithm for orthogonal tensors with g of
the form g(x) = x ⊗ x ⊗ x is summarized in Algorithm 2.
We will use the parameters

(cid:19)

(cid:18) 1

γ

Tensor Decomposition via Simultaneous Power Iteration

Algorithm 2 Streaming robust tensor power method

a stream of data {x1, x2, . . . ,}, parameters

Input:
L, S, N, index sets {Bs}S
Initialization Phase
Let ¯w = 0 ∈ Rd.
for τ = 1 to L do

s=1 ,{Jt}N
t=1.

Update ¯w = ¯w + 1

L xτ .
k ∼ N d (0, 1).

, . . . , Y (0)

end for
Sample Y (0)
Factorize Y (0) as Z (0)R(0) by QR decomposition.
for s = 1 to S do
for τ ∈ Bs do

1

(cid:0)x(cid:62)
τ ¯w(cid:1) xτ x(cid:62)

Update Y (s) = Y (s) + 1|Bs|

end for
Factorize Y (s) as Z (s)R(s) by QR decomposition.

τ Z (s−1).

end for
Tensor power phase
Let Q(0) = Z (S).
for t = 1 to N do
for τ ∈ Jt do
Update Y (t)

i = Y (t)
i = λ(t)

i + 1|Jt| xτ
i + 1|Jt|

(cid:16)

x(cid:62)
τ Q(t)

(cid:17)3

i

x(cid:62)
τ Q(t)

i

Update λ(t)

(cid:16)

(cid:17)2

,∀i ∈ [k].

,∀i ∈ [k].

end for
Factorize Y (t) as Q(t)R(t) by QR decomposition.

end for
Output: ˆui = Q(N )

i

and ˆλi = λ(N )

i

,∀i ∈ [k]

for a large enough constant c0. Moreover, we partition the
time steps into consecutive blocks: with the ﬁrst block [L]
for ﬁnding the vector ¯w, the next S blocks B1, . . . , BS for
the matrix power method in the initialization phase, fol-
lowed by N blocks J1, . . . , JN for the tensor power phase,
with their sizes |Bs| and |Jt| given in (6) and (6) respec-
tively. The proofs of related lemmas in Section 6 are given
next.

D.1. Proof of Lemma 7
First, from the assumption that T = Ex[x ⊗ x ⊗ x], where
Ex[·] denotes the expectation over the distribution of x, we
have the following.

Proposition D.1. Ex[(cid:107)x(cid:107)2x] =(cid:80)

i∈[d] λiui.

Proof. Recall that if we sample w according to the dis-
tribution N d(0, 1),
then for any u ∈ Rd, we have
Ew[(u(cid:62)w)2] = (cid:107)u(cid:107)2, where Ew[·] denotes the expectation
over w. Then we have
Ew[T (Id, w, w)] =

(cid:104)(cid:0)u(cid:62)
i w(cid:1)2(cid:105)

(cid:88)

(cid:88)

λiEw

λiui,

ui =

i∈[d]

i∈[d]

as (cid:107)ui(cid:107)2 = 1. On the other hand, from the assumption that

Pr

x

x

(cid:105)(cid:105)
(cid:105)(cid:105)

The proposition follows by combining these two equalities.

T = Ex[x ⊗ x ⊗ x], we also have
Ew[T (Id, w, w)] = Ew
= Ex
= Ex

(cid:104)Ex
(cid:104)(cid:0)x(cid:62)w(cid:1)2
(cid:104)(cid:0)x(cid:62)w(cid:1)2
(cid:104)Ew
(cid:2)(cid:107)x(cid:107)2x(cid:3) .
(cid:80)L
τ =1((cid:107)xτ(cid:107)2xτ ), for some
This suggests us to take ¯w = 1
L to be determined next. This is because for any i ∈ [k],
L
i ((cid:107)xτ(cid:107)2xτ ) falls in [−1, 1] and
the random variable zτ = u(cid:62)
(cid:88)
has expected value
(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:12)(cid:12) > δ(cid:3) = Pr
2−Ω(cid:0)δ2L(cid:1) ≤ 1
δ2 ) = O( 1

for some L = O( 1
∆2 log k). Then by a union
bound, with probability 0.99 we have ¯w satisfying |u(cid:62)
i ¯w −
λi| ≤ δ for every i ∈ [k]. As ¯w can clearly be computed in
O(d) space, the lemma follows.

τ =1
which by Hoeffding inequality is at most

Pr(cid:2)(cid:12)(cid:12)u(cid:62)

for each τ, so that for δ = 1

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) > δ

E[zτ ] = u(cid:62)

4 (λiγ + 2∆),

i ¯w − λi

zτ − λi

L(cid:88)

λiui = λi

(cid:35)

i∈[d]

100k

L

i

D.2. Proof of Lemma 8

The streaming algorithm for this lemma can be found in the
initialization phase of our Algorithm 2, which is based on
that of Li et al. (2016).
Recall that Li et al. (2016) considered the matrix case,
in which each vector xτ in the stream has the expecta-
tion E[xτ ⊗ xτ ] = M for some d × d matrix M to
be decomposed. To apply their result, let us make the
connection by seeing T (Id, Id, ¯w) as their matrix M and
τ ¯w) · xτ ⊗ xτ as their esti-
Mτ = g(xτ )(Id, Id, ¯w) = (x(cid:62)
mator xτ ⊗ xτ , by noting that
E[Mτ ] = E[g(xτ )(Id, Id, ¯w)] = E[g(xτ )](Id, Id, ¯w) = M.
Since (cid:107) ¯w(cid:107) ≤ 1, (cid:107)Mτ(cid:107) ≤ (cid:107) ¯w(cid:107)(cid:107)xτ(cid:107)3 ≤ 1, and (cid:107)M(cid:107) ≤
(cid:107)T(cid:107)(cid:107) ¯w(cid:107) ≤ 1, we have

(cid:107)Mτ − M(cid:107) ≤ (cid:107)Mτ(cid:107) + (cid:107)M(cid:107) ≤ 2.

Thus, we have from the matrix Bernstein inequality
(Lemma A.4) that

(cid:34)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

|B|

(cid:88)

τ∈B

(cid:35)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≥ δ

Mτ − M

≤ 2d2−Ω(δ2|B|),

Tensor Decomposition via Simultaneous Power Iteration

(cid:80)

for any block B of time steps, and this allows us to apply
the analysis of (Li et al., 2016).
Following (Li et al., 2016), we use the parameters

(cid:26)

(cid:113)

(cid:27)

εs = ε0ρs and βs = min

ρ/

1 + ε2

s−1, ρεs−1

,

√

dk
α0

with ε0 =
the time steps into S = O( 1
block Bs having size

for a small enough constant α0, and divide
γ log d) blocks, with the s’th

|Bs| ≤ c0 log(ds)

∆4β2
s

,

(6)

for a large enough constant c0. Then according to the anal-
ysis in (Li et al., 2016) together with that in our proof of
Lemma 4, one can show that tanm(Z (s)) ≤ εs for every
s ≤ S, so that we can have tanm(Z (S)) ≤ 1, for ev-
ery m ∈ [k]. Moreover, from the analysis in (Li et al.,
2016), we know that the number of samples needed can be
bounded by

|Bs| ≤ O

0 log(dS)

∆4γ

= O

(cid:19)

(cid:18) dk log(dS)

(cid:19)

∆4γ

.

(cid:18) ε2

S(cid:88)

s=1

Finally, note that for each update,
the matrix product
τ Z (s−1), which can be com-
Mτ Z (s−1) equals (x(cid:62)
puted in O(kd) space. Thus, the algorithm works in O(kd)
space, and the lemma follows.

τ ¯w)xτ x(cid:62)

D.3. Proof of Theorem 4

According to Lemma 7 and Lemma 8, let us assume that
we have obtained some Z ∈ Rd×k such that tanm(Z) < 1
for every m ∈ [k]. Now let us focus on the tensor power
phase.
Consider a ﬁxed iteration t. We would like to show that
tanm(Q(t)) ≤ βt with high probability, using Lemma 1.
For this, we need to show that the condition (3) there is
satisﬁed with high probability. For j ∈ [k], let qj denote
Q(t−1)
j = Φ(Id, qj, qj), which now
equals

, and recall that ˆΦ(t)

j

(cid:0)x(cid:62)

τ qj

(cid:1)2

(cid:88)

τ∈Jt

1
|Jt|

xτ − T (Id, qj, qj) .

j

as its j’th column.

2 with probability at most

Let ˆΦ(t) be the d × k matrix with ˆΦ(t)
Then we have the following.
Lemma D.1. (cid:107) ˆΦ(t)(cid:107) > ∆βt
200t2 .
Proof. Let us see ˆΦ(t) as the average of |Jt| i.i.d. random
matrices, so that we can apply the matrix Bernstein inequal-
ity (Lemma A.4). More precisely, for τ ∈ Jt, let Aτ denote
the d × k matrix with
τ qj

xτ − T (Id, qj, qj)

(cid:0)x(cid:62)

(cid:1)2

1

as its j’th column, so that ˆΦ(t) = 1|Jt|
we have E[Aτ ] = 0 for each τ, because

τ∈Jt

Aτ . Note that

E(cid:104)(cid:0)x(cid:62)

(cid:1)2

τ qj

xτ

(cid:105)

= E [(xτ ⊗ xτ ⊗ xτ ) (Id, qj, qj)]
= (E [xτ ⊗ xτ ⊗ xτ ]) (Id, qj, qj)
= T (Id, qj, qj) .

which can be upper-bounded by

where the ﬁrst term above is at most

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:88)

j∈[k]

as the qj’s are orthonormal, while the second term above is

Moreover, we claim that (cid:107)Aτ(cid:107) ≤ 2. This is because for any
v = (v1, . . . , vk) ∈ Rk with (cid:107)v(cid:107) = 1, (cid:107)Aτ v(cid:107) is at most

j∈[k]

vj

xτ

τ qj

τ qj

τ qj

j∈[k]

j∈[k]

(cid:88)

(cid:88)

vjT (Id, qj, qj)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) +
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:88)
(cid:88)
(cid:1)2
(cid:0)x(cid:62)
(cid:1)2 (cid:107)xτ(cid:107) ≤ (cid:88)
|vj|(cid:0)x(cid:62)
(cid:0)x(cid:62)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:34)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:1)2 ≤ (cid:107)xτ(cid:107) ≤ 1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:0)u(cid:62)
(cid:1)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ (cid:88)
(cid:35)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) >

≤ 1
200t2

(cid:0)u(cid:62)

(cid:88)

(cid:88)

(cid:88)

λi ≤ 1

(cid:1)2

j∈[k]

j∈[k]

i∈[d]

i∈[d]

i∈[d]

∆βt

i qj

i qj

λi

λi

ui

ui

vj

vj

(cid:80)
using a similar argument and the assumption that
i∈[d] λi ≤ 1. Then we can apply Lemma A.4, and con-

clude that

2

Pr

Aτ

τ∈Jt

m(Q(0)) =

|Jt|
for our choice of |Jt|.
[m](cid:107) ≤ (cid:107) ˆΦ(t)(cid:107) for any m ∈ [k], and recall that
Note that (cid:107) ˆΦ(t)
we start with cos2
2. There-
fore, given this lemma, we can then apply Lemma 1 re-
peatedly and a simple induction shows that the probabil-
ity that tanm(Q(t)) > βt for some m and t is at most
200t2 ≤ 0.01. Thus, with high probability we have
tanm(Q(t)) ≤ βt for every m and t. Let N be the number
2 for t ≥ N − 1.
such that βt > ε
Note that

2 for t ≤ N − 2 and β = ε
(cid:32)

m(Q(0))+1 ≥ 1

(cid:80)

(cid:33)

tan2

1

1

t

(cid:19)(cid:19)

(cid:18)

N = O

log

= O

log

log

1
ε

,

(cid:18) 1

γ

log 1
ε
log 1
ρ

and recall from the proof of Theorem 2 that from Q(N ) we
can obtain the required ˆui’s and ˆλi’s.

Tensor Decomposition via Simultaneous Power Iteration

Tropp, Joel A. User-friendly tail bounds for sums of ran-
dom matrices. Foundations of Computational Mathe-
matics, 12(4):389–434, 2012.

Zhu, Peizhen. and Knyazev, Andrew V. Angles be-
tween subspaces and their tangents. Arxiv preprint at
arXiv:1209.0523, 2012.

It remains to bound the number of samples needed in this
phase, which is

N(cid:88)

t=1

|Jt| ≤ O

(cid:18) log(dN )

(cid:19) N(cid:88)

∆2

t=1

1
β2
t

.

As βt = max{ρ2t−1, ε
and βt = βN−2ρ2t−2N−2 ≥ ε
Therefore,

2 for t ≥ N − 1
2}, we have βt = ε
2 ρ2t−2N−2 for t ≤ N − 2.
(cid:19)
(cid:18)
N−2(cid:88)

ρ2(2N−2−2t) ≤ O

1

,

ε2(1 − ρ4)

t=1

1
β2
t

≤ 8

ε2 +

4
ε2

N(cid:88)

t=1

where

1

1−ρ4 = 1+ρ4

maxi∈[k]
have

λ2
i+1
λ2
i

1−ρ8 ≤
= mini∈[k]
N(cid:88)

|Jt| ≤ O

t=1

1−ρ8 and 1 − ρ8 = 1 −
2
i −λ2
λ2
= γ. As a result, we
λ2
i

i+1

(cid:18) log(dN )

(cid:19)

∆2γε2

.

Combining this with the number of samples for the initial-
ization phase, including that for ﬁnding ¯w, we have the
stated sample complexity bound of the theorem.

References
Hardt, Moritz and Price, Eric. The noisy power method: A
meta algorithm with applications. In Advances in Neural
Information Processing Systems, pp. 2861–2869, 2014.

Hom, Roger A and Johnson, Charles R. Topics in matrix
analysis. Cambridge University Press, New York, 1991.

Laurent, B. and Massart, P. Adaptive estimation of a
Annals of

quadratic functional by model selection.
Statistics, 28(5):1302–1338, 2000.

Li, Chun-Liang, Lin, Hsuan-Tien, and Lu, Chi-Jen. Rivalry
of two families of algorithms for memory-restricted
streaming PCA. In Proceedings of the 19th International
Conference on Artiﬁcial Intelligence and Statistics (AIS-
TATS), pp. 473–481, 2016.

Mitliagkas, Ioannis, Caramanis, Constantine, and Jain, Pra-
teek. Memory limited, streaming PCA. In Advances in
Neural Information Processing Systems, pp. 2886–2894,
2013.

Schmitt, Bernhard A.

Perturbation bounds for matrix
square roots and pythagorean sums. Linear algebra and
its applications, 174:215–227, 1992.

Stewart, Gilbert W. and Sun, Ji-Guang. Matrix Perturba-

tion Theory. Academic Press, 1990.

