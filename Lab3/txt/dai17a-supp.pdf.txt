Stochastic Generative Hashing

Supplementary Material

A. Distributional Derivative of Stochastic Neuron
Before we prove the lemma 3, we ﬁrst introduce the chain rule of distributional derivative.
Lemma 6 (Grubb, 2008) Let u ∈ D(cid:48)(Ω), we have

1. (Chain Rule I) The distribution derivative of v = u ◦ f for any f (x) ∈ C1 : Ω(cid:48) → Ω is given by Dv = Du ∂f
∂x .

2. (Chain Rule II) The distribution derivative of v = f ◦ u for any f (x) ∈ C1(R) with f(cid:48) bounded is given by

Dv = f(cid:48)(u)Du.

Proof of Lemma 3. Without loss of generality, we ﬁrst consider 1-dimension case. Given (cid:96)(˜h) : R → R, ξ ∼ U(0, 1),
˜h : Ω → {0, 1}. For ∀φ ∈ C∞

0 (Ω), we have(cid:90)

D(cid:96)(˜h(x))

dx = −

φ(cid:48)(x)(cid:96)(x)dx

φ(x)

(cid:18)(cid:90) 0
(cid:32)

−∞

(cid:16)
(cid:12)(cid:12)(cid:12)(cid:12)0

(cid:17)

(cid:90)

(cid:90) ∞
(cid:12)(cid:12)(cid:12)(cid:12)∞

0

(cid:33)

= −

φ(cid:48)(x)(cid:96)(0)dx +

φ(cid:48)(x)(cid:96)(1)dx

(cid:96)(1)

−∞

φ(x)

(cid:96)(0) + φ(x)

= −
= ((cid:96)(1) − (cid:96)(0)) φ(0)
0 (Ω). We obtain
D(cid:96)(˜h) = ((cid:96)(1) − (cid:96)(0))δ(h) := ∆(cid:96)(h).

0

(cid:19)

(cid:104)

where the last equation comes from φ ∈ C∞

(cid:104)

(cid:104)

(cid:105)

(cid:105)

We generalize the conclusion to l-dimension case with expectation over ξ, i.e., ˜h(·, ξ) : Ω → {0, 1}l, we have the partial
distributional derivative for k-th coordinate as

(cid:104)

(cid:105)

(cid:105)

DkE{ξi}l

i=1

(cid:96)(˜h(z, ξ))

= E{ξi}l

i=1

Therefore, we have the distributional derivative w.r.t. W as

DE{ξi}l

i=1

(cid:96)(˜h(σ(W (cid:62)x), ξ))

= E{ξi}l
chain rule I = E{ξi}l

.

(cid:105)

i=1,i(cid:54)=k

((cid:96)(˜h1

= E{ξi}l

Dk(cid:96)(˜h(z, ξ))

Dk(cid:96)(˜h(σ(W (cid:62)x), ξ))

(cid:104)
(cid:104)
∆˜h(cid:96)(˜h(σ(W (cid:62)x), ξ))σ(W (cid:62)x) •(cid:0)1 − σ(W (cid:62)x)(cid:1) x(cid:62)(cid:105)

(cid:96)(˜h(σ(W (cid:62)x), ξ))∇W σ(W (cid:62)x)

k) − (cid:96)(˜h0
k))
(cid:105)

D˜hk

i=1

i=1

(cid:104)

.

= Eξ

To derive the approximation of the distributional derivative, we exploit the mean value theorem and Taylor expansion.
Speciﬁcally, for a continuous and differential loss function (cid:96)(·), there exists  ∈ (0, 1)

Moreover, for general smooth functions, we rewrite the ∂˜hi
(cid:96)(˜h)|˜hi= = ∂˜hk
(cid:96)(˜h)|˜hi= = ∂˜hk

∂˜hk
∂˜hk

we have an approximator as

(cid:96)(˜h)|˜hk= ≈ σ(w(cid:62)

k x)∂˜hk

∂˜hk

(cid:96)(˜h)|˜hk=1 + (1 − σ(w(cid:62)

(cid:96)(˜h)|˜hk= =

∂˜hk

.

(cid:105)

∆˜h(cid:96)(˜h)

(cid:104)
(cid:96)(˜h)|˜hi= by Taylor expansion, i.e.,
(cid:96)(˜h)|˜hi=1 + O()
(cid:96)(˜h)|˜hi=0 + O().

k

(cid:104)∇˜h(cid:96)(˜h, ξ)
(cid:105)
(cid:104)∇˜h(cid:96)(˜h(σ(W (cid:62)x), ξ))σ(W (cid:62)x) • (1 − σ(W (cid:62)x))x(cid:62)(cid:105)

(cid:96)(˜h)|˜hk=0 = Eξ

k x))∂˜hk

.

.

Plugging into the distributional derivative estimator (7), we obtain a simple biased gradient estimator,

DW ˜H(Θ; x) ≈ ˜DW ˜H(Θ; x) := Eξ

(13)

(14)

Stochastic Generative Hashing

B. Convergence of Distributional SGD
Lemma 7 (Ghadimi and Lan, 2013) Under the assumption that H is L-Lipschitz smooth and the variance of the stochastic
distributional gradient (8) is bounded by σ2, the proposed distributional SGD outputs {Θi}t
t(cid:88)

i=1,

(cid:18)

Lσ2

(cid:54) ˜H(Θ0) − ˜H(Θ
∗

) +

γ2
i ,

2

i=1

t(cid:88)
where Θt = {Wt, Ut, βt, ρt}.

i=1

where γi ∼ O(cid:0)1/

γi − L
2

E

γ2
i

(cid:19)

(cid:20)(cid:13)(cid:13)(cid:13)∇Θ ˜H(Θi)
(cid:13)(cid:13)(cid:13)2(cid:21)
t(cid:1) from trajectory {Θi}t
(cid:13)(cid:13)(cid:13)2(cid:21)
(cid:20)(cid:13)(cid:13)(cid:13)∇Θ ˜H(ΘR)

E

i=1, we have
∼ O

(cid:18) 1√

(cid:19)

.

t

Proof of Theorem 5. Lemma 7 implies that by randomly sampling a search point ΘR with probability P (R = i) =
(cid:80)t
2γi−Lγ2
i=1 2γi−Lγ2

√

i

i

Lemma 8 Under the assumption that the variance of the approximate stochastic distributional gradient (10) is bounded
by σ2, the proposed distributional SGD outputs {Θi}t

where Θ∗ denotes the optimal solution.

Proof Denote the optimal solution as Θ∗, we have

(cid:107)Θi+1 − Θ∗(cid:107)2 =

Taking expectation on both sides and denoting aj = (cid:107)Θj − Θ∗(cid:107)2, we have

Therefore,

(cid:33)

γ2
i σ2

,

γiE(cid:104)

t(cid:88)

i=1

(Θi − Θ∗)

2

+

i=1

t(cid:88)

i=1 such that

(cid:62) ˜∇Θ ˜H(Θi)

(cid:105) (cid:54) 1

(cid:32)
E(cid:104)(cid:107)Θ0 − Θ∗(cid:107)2(cid:105)
(cid:13)(cid:13)(cid:13)Θi − γi(cid:98)˜∇Θ ˜H(Θi, xi) − Θ∗)
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:98)˜∇Θ ˜H(Θi, xi)
(cid:32)

(cid:13)(cid:13)(cid:13)2 − 2γi (Θi − Θ∗)
(cid:33)

(cid:62) ˜∇Θ ˜H(Θi)

(Θi − Θ∗)

i σ2.

+ γ2

i

= (cid:107)Θi − Θ∗(cid:107)2 + γ2

E [ai+1] (cid:54) E [ai] − 2γiE(cid:104)
γiE(cid:104)
t(cid:88)

(Θi − Θ∗)

E [a0] +

γ2
i σ2

.

(cid:62) ˜∇Θ ˜H(Θi)

(cid:105) (cid:54) 1

2

(cid:105)
t(cid:88)

i=1

i=1

(cid:62)(cid:98)˜∇Θ ˜H(Θi, xi).

Theorem 9 Under the assumption that the variance of the approximate stochastic distributional gradient (10) is bounded
by σ2, for the solution ΘR sampled from the trajectory {Θi}t
where γi ∼

i=1 with probability P (R = i) =

γi(cid:80)t

i=1 γi

O(cid:0)1/

t(cid:1), we have

√

E(cid:104)

(ΘR − Θ

∗

(cid:62) ˜∇Θ ˜H(ΘR)

)

(cid:105) ∼ O

(cid:18) 1√

(cid:19)

,

t

where Θ∗ denotes the optimal solution.

γi ∼ O(cid:0)1/

√

t(cid:1) from trajectory {Θi}t

Proof The lemma 8 implies by randomly sampling a search point ΘR with probability P (R = i) =

E(cid:104)

i=1, we have
(cid:62) ˜∇Θ ˜H(ΘR)

(cid:105) (cid:54)

(ΘR − Θ∗)

E(cid:104)(cid:107)Θ0 − Θ∗(cid:107)2(cid:105)
2(cid:80)t

+(cid:80)t

i=1 γi

i=1 γ2

i σ2

∼ O

(cid:19)

(cid:18) 1√

.

t

γi(cid:80)t

i=1 γi

where

Stochastic Generative Hashing

C. More Experiments
C.1. Convergence of Distributional SGD and Reconstruction Error Comparison

(a) MNIST

(b) GIST-1M

Figure 4: L2 reconstruction error convergence on MNIST and GIST-1M of ITQ and SGH over the course of training with
varying of the length of the bits (8, 16, 32, 64, respectively). The x-axis represents the number of examples seen by the
training algorithm. For ITQ, it sees the training dataset once in one iteration.

We shows the reconstruction error comparison between ITQ and SGH on MNIST and GIST-1M in Figure 4. The results
are similar to the performance on SIFT-1M. Because SGH optimizes a more expressive objective than ITQ (without
orthogonality) and do not use alternating optimization, it ﬁnd better solution with lower reconstruction error.

C.2. Training Time Comparison

(a) MNIST

(b) GIST-1M

Figure 5: Training time comparison between BA and SGH on MNIST and GIST-1M.

We shows the training time comparison between BA and SGH on MNIST and GIST-1M in Figure 5. The results are
similar to the performance on SIFT-1M. The proposed distributional SGD learns the model much faster.

C.3. More Evaluation on L2NNS Retrieval Tasks

We also use different RecallK@N to evaluate the performances of our algorithm and the competitors. We ﬁrst evaluated
the performance of the algorithms with Recall 1@N in Figure 6. This is an easier task comparing to K = 10. Under such
measure, the proposed SGH still achieve the state-of-the-art performance.
In Figure 7, we set K, N = 100 and plot the recall by varying the length of the bits on MNIST, SIFT-1M, and GIST-1M.
This is to show the effects of length of bits in different baselines. Similar to the Recall10@N, the proposed algorithm still
consistently achieves the state-of-the-art performance under such evaluation measure.

00.511.522.5number of samples visited#10651015202530L2 reconstruction errorMNIST L2 reconstruction error8 bits ITQ16 bits ITQ32 bits ITQ64 bits ITQ8 bits SGH16 bits SGH32 bits SGH64 bits SGH00.511.522.5number of samples visited#1060.40.50.60.70.80.911.11.2L2 reconstruction errorGIST L2 reconstruction error8 bits ITQ16 bits ITQ32 bits ITQ64 bits ITQ8 bits SGH16 bits SGH32 bits SGH64 bits SGH8163264bits of hashing codes020040060080010001200Training Time (sec)MNIST Training TimeBASGH8163264bits of hashing codes00.511.52Training Time (sec)#104GIST Training TimeBASGHStochastic Generative Hashing

D. Stochastic Generative Hashing For Maximum Inner Product Search
In Maximum Inner Product Search (MIPS) problem, we evaluate the similarity in terms of inner product which can avoid
the scaling issue, i.e., the length of the samples in reference dataset and the queries may vary. The proposed model can also
be applied to the MIPS problem. In fact, the Gaussian reconstruction model also preserve the inner product neighborhoods.
Denote the asymmetric inner product as x(cid:62)U hy, we claim
Proposition 10 The Gaussian reconstruction error is a surrogate for asymmetric inner product preservation.

Proof We evaluate the difference between inner product and the asymmetric inner product,

(cid:107)x(cid:62)y − x(cid:62)U(cid:62)hy(cid:107)2 = (cid:107)x(cid:62)(cid:0)y − U(cid:62)hy

(cid:1)(cid:107)2 (cid:54) (cid:107)x(cid:107)2(cid:107)y − U(cid:62)hy(cid:107)2,

which means minimizing the Gaussian reconstruction, i.e., − log p(x|h), error will also lead to asymmetric inner product
preservation.
We emphasize that our method is designed for hashing problems primarily. Although it can be used for MIPS problem, it is
different from the product quantization and its variants whose distance are calculated based on lookup table. The proposed
distributional SGD can be extended to quantization. This is out of the scope of this paper, and we will leave it as the future
work.

D.1. MIPS Retrieval Comparison

To evaluate the performance of the proposed SGH on MIPS problem, we tested the algorithm on WORD2VEC dataset for
MIPS task. Besides the hashing baselines, since KMH is the Hamming distance generalization of PQ, we replace the KMH
with product quantization (Jegou et al., 2011). We trained the SGH with 71,291 samples and evaluated the performance
with 10,000 query. Similarly, we vary the length of binary codes from 16, 32 to 64, and evaluate the performance by Recall
10@N. We calculated the ground-truth via retrieval through the original inner product. The performances are illustrated
in Figure 8. The proposed algorithm outperforms the competitors signiﬁcantly, demonstrating the proposed SGH is also
applicable to MIPS task.

E. Generalization
We generalize the basic model to translation and scale invariant extension, semi-supervised extension, as well as coding
with h ∈ {−1, 1}l.

E.1. Translation and Scale Invariant Reduced-MRFs

As we known, the data may not zero-mean, and the scale of each sample in dataset can be totally different. To eliminate
the translation and scale effects, we extend the basic model to translation and scale invariant reduced-MRFs by introducing
parameter α to separate the translation effect and the latent variable z to model the scale effect in each sample x, therefore,
the potential function becomes

1

E(x, h, z) = −β(cid:62)h +

2ρ2 (x − α − U(cid:62)(z · h))(cid:62)(x − α − U(cid:62)(z · h)),

(15)
where · denotes element-wise product, α ∈ Rd and z ∈ Rl. Comparing to (2), we replace U(cid:62)h with U(cid:62)(z · h) + α so that
the translation and scale effects in both dimension and sample are modeled explicitly.
We treat the α as parameters and z as latent variable. Assume the independence in posterior for computational efﬁciency,
we approximate the posterior p(z, h|x) with q(h|x; Wh)q(z|x; Wz), where Wh, Wz denotes the parameters in the posterior
approximation. With similar derivation, we obtain the learning objective as

N(cid:88)

i=1

max

U,α,β,ρ;Wh,Wz

1
N

Eq(h|xi)q(z|xi) [−E(x, h, z) − log q(h|xi) − log q(z|xi)] .

(16)

Obviously, the proposed distributional SGD is still applicable to this optimization.

E.2. Semi-supervised Extension

Although we only focus on learning the hash function in unsupervised setting, the proposed model can be easily extended to
exploit the supervision information by introducing pairwise model, e.g., (Zhang et al., 2014a; Zhu et al., 2016). Speciﬁcally,

we are provided the (partial) supervision information for some pairs of data, i.e., S = {xi, xi, yij}M

i,j, where

(cid:40)

1
0

yij =

Stochastic Generative Hashing

if xi ∈ NN (xj) or xj ∈ NN (xi)
o.w.

,

(17)

(18)

(19)

(20)

and NN (x) stands for the set of nearest neighbors of x. Besides the original Gaussian reconstruction model in the basic
model in (2), we introduce the pairwise model p(yij|hi, hj) = B(σ(h(cid:62)
i hj)) into the framework, which results the joint
distribution over x, y, h as

p(xi, xj, hi, hj, yij) = p(xi|hi)p(xj|hj)p(hi)p(hj)p(yij|hi, hj)1S (ij),

where 1S (ij) is an indicator that outputs 1 when (xi, xj) ∈ S, otherwise 0. Plug the extended model into the Helmholtz
free energy, we have the learning objective as,

(cid:16)Eq(hi|xi)q(hj|xj ) [log p(xi, xj, hi, hj)] + Eq(hi|xi)q(hj|xj ) [1S (ij) log p(yij|hi, hj)]

N 2(cid:88)

max
U,β,ρ;W

1
N 2

i,j=1

−Eq(hi|xi)q(hj|xi) [log q(hj|xj)q(hj|xi)]

(cid:17)

,

Obviously, the proposed distributional SGD is still applicable to the semi-supervised extension.
E.3. {±1}-Binary Coding
In the main text, we mainly focus on coding with {0, 1}. In fact, the proposed model is applicable to coding with {−1, 1}
with minor modiﬁcation. Moreover, the proposed distributional SGD is still applicable. We only discuss the basic model
here, the model can also be extended to scale-invariant and semi-supervised variants.
If we set h ∈ {−1, 1}l, the potential function of basic reduced-MRFs (2) does not have any change, i.e.,

(cid:0)x(cid:62)x + h(cid:62)U(cid:62)U h − 2x(cid:62)U h(cid:1) .

E(x, h) = −β(cid:62)h +
We need to modify the parametrization of q(h|x) as

1
2ρ2

l(cid:89)

q(h|x) =

σ(w(cid:62)

i x)

1+hi

2

(cid:0)1 − σ(w(cid:62)

i x)(cid:1) 1−hi

2

.

Therefore, the stochastic neuron becomes

i=1

f (z, ξ) :=

.

1
−1

if σ(z) (cid:62) ξ
if σ(z) < ξ

(cid:40)
(cid:2)∆f (cid:96)(f (z, ξ))∇zσ(z)x(cid:62)(cid:3) ,
(cid:2)∇f (cid:96)(f (z, ξ))∇zσ(z)x(cid:62)(cid:3) .

With similar derivation, we have the distributional derivative of the objective w.r.t. W as

where [∆f (cid:96)(f (z, ξ))]k = (cid:96)(f 1

k ) − (cid:96)(f−1

∇W Lsn = Eξ
k ). Furthermore, we have a similar biased gradient estimator as
˜∇W Lsn = Eξ

Plug these modiﬁcation into the model and algorithm, we can learn a {−1, 1}-encoding function.

Stochastic Generative Hashing

Figure 6: L2NNS comparison on MNIST, SIFT-1M, SIFT-1B, and GIST-1M with the length of binary bits from 16 to
64. We evaluate the performance with Recall 1@M, where M increasing to 1000.

02004006008001000M - number of retrieved items00.10.20.30.40.50.60.70.80.9RecallMNIST 16 bit Recall 1@MSGHBASpHSHITQKMHGH02004006008001000M - number of retrieved items00.10.20.30.40.50.60.70.80.91RecallMNIST 32 bit Recall 1@MSGHBASpHSHITQKMHGH02004006008001000M - number of retrieved items00.10.20.30.40.50.60.70.80.91RecallMNIST 64 bit Recall 1@MSGHBASpHSHITQKMHGH02004006008001000M - number of retrieved items00.050.10.150.20.250.30.35RecallSIFT1M 16 bit Recall 1@MSGHBASpHSHITQKMHGH02004006008001000M - number of retrieved items00.10.20.30.40.50.60.7RecallSIFT1M 32 bit Recall 1@MSGHBASpHSHITQKMHGH02004006008001000M - number of retrieved items00.10.20.30.40.50.60.70.80.9RecallSIFT1M 64 bit Recall 1@MSGHBASpHSHITQKMHGH02004006008001000M - number of retrieved items012345678Recall#10-3SIFT1B 16 bit Recall 1@MSGHITQ02004006008001000M - number of retrieved items00.020.040.060.080.10.12RecallSIFT1B 32 bit Recall 1@MSGHITQ02004006008001000M - number of retrieved items00.050.10.150.20.250.30.350.4RecallSIFT1B 64 bit Recall 1@MSGHITQ02004006008001000M - number of retrieved items00.050.10.150.20.25RecallGIST 16 bit Recall 1@MSGHBASpHSHITQKMHGH02004006008001000M - number of retrieved items00.050.10.150.20.250.30.350.4RecallGIST 32 bit Recall 1@MSGHBASpHSHITQKMHGH02004006008001000M - number of retrieved items00.10.20.30.40.50.6RecallGIST 64 bit Recall 1@MSGHBASpHSHITQKMHGHStochastic Generative Hashing

(a) L2NNS on MNIST

(b) L2NNS on SIFT-1M

(c) L2NNS on GIST-1M

Figure 7: L2NNS comparison on MNIST, SIFT-1M, and GIST-1M with Recall 100@100 for the length of bits from 8 to
64.

Figure 8: MIPS comparison on WORD2VEC with the length of binary bits from 16 to 64. We evaluate the performance with
Recall 10@M, where M increasing to 1000.

8163264bits of hashing code00.10.20.30.40.5Recall 100@100SGHBASpHSHITQKMHGH8163264bits of hashing code00.050.10.150.20.25Recall 100@100SGHBASpHSHITQKMHGH8163264bits of hashing code00.020.040.060.08Recall 100@100SGHBASpHSHITQKMHGH02004006008001000M - number of retrieved items00.050.10.150.20.250.30.350.40.45RecallWORD2VEC 16 bit Recall 10@MSGHBASpHSHITQPQ02004006008001000M - number of retrieved items00.10.20.30.40.50.60.7RecallWORD2VEC 32 bit Recall 10@MSGHBASpHSHITQPQ02004006008001000M - number of retrieved items00.10.20.30.40.50.60.70.8RecallWORD2VEC 64 bit Recall 10@MSGHBASpHSHITQPQ