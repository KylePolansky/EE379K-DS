No Spurious Local Minima in Nonconvex Low Rank Problems:

A Uniﬁed Geometric Analysis
Rong Ge∗
Yi Zheng‡

Chi Jin†

Abstract

In this paper we develop a new framework that captures the common landscape underlying the common non-
convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we
show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no high-
order saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have
global converge, and efﬁciently optimize these non-convex objective functions in practice. Our framework connects
and simpliﬁes the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion.
The framework naturally leads to new results for asymmetric matrix completion and robust PCA.

1 Introduction

Non-convex optimization is one of the most powerful tools in machine learning. Many popular approaches, from
traditional ones such as matrix factorization [Hotelling, 1933] to modern deep learning [Bengio, 2009] rely on opti-
mizing non-convex functions. In practice, these functions are optimized using simple algorithms such as alternating
minimization or gradient descent. Why such simple algorithms work is still a mystery for many important problems.
One way to understand the success of non-convex optimization is to study the optimization landscape: for the
objective function, where are the possible locations of global optima, local optima and saddle points. Recently, a line
of works showed that several natural problems including tensor decomposition [Ge et al., 2015], dictionary learning
[Sun et al., 2015a], matrix sensing [Bhojanapalli et al., 2016, Park et al., 2016] and matrix completion [Ge et al., 2016]
have well-behaved optimization landscape: all local optima are also globally optimal. Combined with recent results
(e.g. Ge et al. [2015], Carmon et al. [2016], Agarwal et al. [2016], Jin et al. [2017]) that are guaranteed to ﬁnd a local
minimum for many non-convex functions, such problems can be efﬁciently solved by basic optimization algorithms
such as stochastic gradient descent.

In this paper we focus on optimization problems that look for low rank matrices using partial or corrupted observa-
tions. Such problems are studied extensively [Fazel, 2002, Rennie and Srebro, 2005, Cand`es and Recht, 2009] and has
many applications in recommendation systems [Koren, 2009], see survey by Davenport and Romberg [2016]. These
optimization problems can be formalized as follows:

min

M∈Rd1×d2
s.t.

f (M),

rank(M) = r.

(1)

Here M is an d1 × d2 matrix and f is a convex function of M. The non-convexity of this problem stems from the low
rank constraint. Several interesting problems, such as matrix sensing [Recht et al., 2010], matrix completion [Cand`es
and Recht, 2009] and robust PCA [Cand`es et al., 2011] can all be framed as optimization problems of this form(see
Section 3).

∗Duke University. Email: rongge@cs.duke.edu
†University of California, Berkeley. Email: chijin@cs.berkeley.edu
‡Duke University. Email: sheng.zheng@duke.edu

1

In practice, Burer and Monteiro [2003] heuristic is often used – replace M with an explicit low rank representation

M = UV(cid:62), where U ∈ Rd1×r and V ∈ Rd2×r. The new optimization problem becomes

min

U∈Rd1×r,V∈Rd2×r

f (UV(cid:62)) + Q(U, V).

(2)

Here Q(U, V) is a (optional) regularizer. Despite the objective being non-convex, for all the problems mentioned
above, simple iterative updates from random or even arbitrary initial point ﬁnd the optimal solution in practice. It is
then natural to ask: Can we characterize the similarities between the optimization landscape of these problems?
We show this is indeed possible:

Theorem 1 (informal). The objective function of matrix sensing, matrix completion and robust PCA have similar
optimization landscape. In particular, for all these problems, 1) all local minima are also globally optimal; 2) any
saddle point has at least one strictly negative eigenvalue in its Hessian.

More precise theorem statements appear in Section 3. Note that there were several cases (matrix sensing [Bho-
janapalli et al., 2016, Park et al., 2016], symmetric matrix completion [Ge et al., 2016]) where similar results on the
optimization landscape were known. However the techniques in previous works are tailored to the speciﬁc problems
and hard to generalize. Our framework captures and simpliﬁes all these previous results, and also gives new results on
asymmetric matrix completion and robust PCA.
The key observation in our analysis is that for matrix sensing, matrix completion, and robust PCA (when ﬁxing
sparse estimate), function f (in Equation (1)) is a quadratic function over the matrix M. Hence the Hessian H of f
with respect to M is a constant. More importantly, the Hessian H in all above problems has similar properties (that it
approximately preserves norm, similar to the RIP properties used in matrix sensing [Recht et al., 2010]), which allows
their optimization landscapes to be characterized in a uniﬁed way. Speciﬁcally, our framework gives principled way
of deﬁning a direction of improvement for all points that are not globally optimal.
Another crucial property of our framework is the interaction between the regularizer and the Hessian H. Intuitively,
the regularizer makes sure the solution is in a nice region B (e.g. set of incoherent matrices for matrix completion),
and only within B the Hessian has the norm preserving property. On the other hand, regularizer should not be too large
to severely distort the landscape. This interaction is crucial for matrix completion, and is also very useful in handling
noise and perturbations. In Section 4, we discuss ideas required to apply this framework to matrix sensing, matrix
completion and robust PCA.
(where the desired matrix is of the form UU(cid:62)). See Section 5 for more details.

Using this framework, we also give a way to reduce asymmetric matrix problems to symmetric PSD problems

In addition to the results of no spurious local minima, our framework also implies that any saddle point has at least
one strictly negative eigenvalue in its Hessian. Formally, we proved all above problems satisfy a robust version of
this claim — strict saddle property (see Deﬁnition 2), which is one of crucial sufﬁcient conditions to admit efﬁcient
optimization algorithms, and thus following corollary (see Section 6 for more details).

Corollary 2 (informal). For matrix sensing, matrix completion and robust PCA, simple local search algorithms can
ﬁnd the desired low rank matrix UV(cid:62) = M(cid:63) from an arbitrary starting point in polynomial time with high probability.

For simplicity, we present most results in the noiseless setting, but our results can also be generalized to handle

noise. As an example, we show how to do this for matrix sensing in Section C.

1.1 Related Works
The landscape of low rank matrix problems have recently received a lot of attention. Ge et al. [2016] showed symmetric
matrix completion has no spurious local minimum. At the same time, Bhojanapalli et al. [2016] proved similar result
for symmetric matrix sensing. Park et al. [2016] extended the matrix sensing result to asymmetric case. All of these
works guarantee global convergence to the correct solution.

There has been a lot of work on the local convergence analysis for various algorithms and problems. For matrix
sensing or matrix completion, the works [Keshavan et al., 2010a,b, Hardt and Wootters, 2014, Hardt, 2014, Jain et al.,
2013, Chen and Wainwright, 2015, Sun and Luo, 2015, Zhao et al., 2015, Zheng and Lafferty, 2016, Tu et al., 2015]

2

showed that given a good enough initialization, many simple local search algorithms, including gradient descent and
alternating least squares, succeed. Particularly, several works (e.g. Sun and Luo [2015], Zheng and Lafferty [2016])
accomplished this by showing a geometric property which is very similar to strong convexity holds in the neighborhood
of optimal solution. For robust PCA, there are also many analysis for local convergence [Lin et al., 2010, Netrapalli
et al., 2014, Yi et al., 2016, Zhang et al., 2017].

Several works also try to unify the analysis for similar problems. Bhojanapalli et al. [2015] gave a framework
for local analysis for these low rank problems. Belkin et al. [2014] showed a framework of learning basis functions,
which generalizes tensor decompositions. Their techniques imply the optimization landscape for all such problems are
very similar. For problems looking for a symmetric PSD matrix, Li and Tang [2016] showed for objective similar to
(2) (but in the symmetric setting), restricted smoothness/strong convexity on the function f sufﬁces for local analysis.
However, their framework does not address the interaction between regularizer and the function f, hence cannot be
directly applied to problems such as matrix completion or robust PCA.

Organization We will ﬁrst introduce notations and basic optimality conditions in Section 2. Then Section 3 intro-
duces the problems and our results. For simplicity, we present our framework for the symmetric case in Section 4, and
brieﬂy discuss how to reduce asymmetric problem to symmetric problem in Section 5. We discuss how our geometric
result implies efﬁcient algorithms in Section 6. We then show how our geometric results imply fast runtime of popular
local search algorithms in Section 6. For clean presentation, many proofs are deferred to appendix .

2 Preliminaries

In this section we introduce notations and basic optimality conditions.

inner-product, and for matrices we use (cid:104)M, N(cid:105) = (cid:80)

2.1 Notations
We use bold letters for matrices and vectors. For a vector v we use (cid:107)v(cid:107) to denote its (cid:96)2 norm. For a matrix M we
use (cid:107)M(cid:107) to denote its spectral norm, and (cid:107)M(cid:107)F to denote its Frobenius norm. For vectors we use (cid:104)u, v(cid:105) to denote
i,j MijNij to denote the trace of MN(cid:62). We will always use
r to denote its
M(cid:63) to denote the optimal low rank solution. Further, we use σ(cid:63)
r-th singular value and κ(cid:63) = σ(cid:63)
We use ∇f to denote the gradient and ∇2f to denote its Hessian. Since function f can often be applied to both
M (as in (1)) and U, V (as in (2)), we use ∇f (M) to denote gradient with respect to M and ∇f (U, V) to denote
gradient with respect to U, V. Similar notation is used for Hessian. The Hessian ∇2f (M) is a crucial object in our
framework. It can be interpreted as a linear operator on matrices. This linear operator can be viewed as a d1d2 × d1d2
the notation M : H : N to denote the quadratic form (cid:104)M,H(N)(cid:105). Similarly, the Hessian of objective (2) is a linear
operator on a pair of matrices U, V, which we usually denote as ∇2f (U, V).

(cid:1) matrix in the symmetric case) that applies to the vectorized version of matrices. We use

1 to denote its largest singular value, σ(cid:63)

matrix (or(cid:0)d+1

(cid:1) ×(cid:0)d+1

1/σ(cid:63)

r be the condition number.

2

2

2.2 Optimality Conditions
Local Optimality Suppose we are optimizing a function f (x) with no constraints on x. In order for a point x to be
a local minimum, it must satisfy the ﬁrst and second order necessary conditions. That is, we must have ∇f (x) = 0
and ∇2f (x) (cid:23) 0.
Deﬁnition 1 (Optimality Condition). Suppose x is a local minimum of f (x), then we have

∇f (x) = 0, ∇2f (x) (cid:23) 0.

Intuitively, if one of these conditions is violated, then it is possible to ﬁnd a direction that decreases the func-
tion value. Ge et al. [2015] characterized the following strict-saddle property, which is a quantitative version of the
optimality conditions, and can lead to efﬁcient algorithms to ﬁnd local minima.

3

Deﬁnition 2. We say function f (·) is (θ, γ, ζ)-strict saddle. That is, for any x, at least one of followings holds:

1. (cid:107)∇f (x)(cid:107) ≥ θ.
2. λmin(∇2f (x)) ≤ −γ.
3. x is ζ-close to X (cid:63) – the set of local minima.
Intuitively, this deﬁnition says for any point x, it either violates one of the optimality conditions signiﬁcantly (ﬁrst
two cases), or is close to a local minima. Note that ζ and θ are often closely related. For a function with strict-saddle
property, it is possible to efﬁciently ﬁnd a point near a local minimum.

Local vs. Global However, of course ﬁnding a local minimum is not sufﬁcient in many case. In this paper we are
also going to prove that all local minima are also globally optimal, and they correspond to the desired solutions.

3 Low Rank Problems and Our Results

In this section we introduce matrix sensing, matrix completion and robust PCA. For each problem we give the results
obtained by our framework. The proof ideas are illustrated later in Sections 4 and 5.

3.1 Matrix Sensing
Matrix sensing [Recht et al., 2010] is a generalization of compressed sensing [Candes et al., 2006]. In the matrix
sensing problem, there is an unknown low rank matrix M(cid:63) ∈ Rd1×d2. We make linear observations on this matrix:
let A1, A2, ..., Am ∈ Rd1×d2 be m sensing matrices, the algorithm is given {Ai}’s and the corresponding bi =
(cid:104)Ai, M(cid:63)(cid:105). The goal is now to ﬁnd the unknown matrix M(cid:63). In order to ﬁnd M(cid:63), we need to solve the following
nonconvex optimization problem

min

M∈Rd1×d2 ,rank(M)=r

f (M) =

1
2m

((cid:104)M, Ai(cid:105) − bi)2.

m(cid:88)

i=1

We can transform this constraint problem to an unconstraint problem by expressing M as M = UV(cid:62) where

U ∈ Rd1×r and V ∈ Rd2×r. We also need an additional regularizer (common for all asymmetric problems):

m(cid:88)

i=1

min
U,V

1
2m

((cid:104)UV(cid:62), Ai(cid:105) − bi)2 +

(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2
F .

1
8

(3)

The regularizer has been widely used in previous works [Zheng and Lafferty, 2016, Park et al., 2016]. In Section 5
we show how this regularizer can be viewed as a way to deal with the additional invariants in asymmetric case, and
reduce the asymmetric case to the symmetric case. A crucial concept in standard sensing literature is Restrict Isometry
Property (RIP), which is deﬁned as follows:
Deﬁnition 3. A group of sensing matrices {A1, .., Am} satisﬁes the (r, δ)-RIP condition, if for every matrix M of
rank at most r,

(1 − δ)(cid:107)M(cid:107)2

F ≤ 1
m

(cid:104)Ai, M(cid:105)2 ≤ (1 + δ)(cid:107)M(cid:107)2
F .

(cid:80)m
i=1(cid:104)Ai,·(cid:105)2 approximately perserve norms for all low rank matrices. When
the sensing matrices are chosen to be i.i.d. matrices with independent Gaussian entries, if m ≥ c(d1 + d2)r for
large enough constant c, the sensing matrices satisfy the (2r, 1
20 )-RIP condition [Candes and Plan, 2011]. Using our
framework we can show:

Intuitively, RIP says operator 1
m

i=1

m(cid:88)

4

Theorem 3. When measurements {Ai} satisfy (2r, 1
minima satisfy UV(cid:62) = M(cid:63) 2) the function is (, Ω(σ(cid:63)

20 )-RIP, for matrix sensing objective (3) we have 1) all local
r ), O( 
σ(cid:63)
r

))-strict saddle.

This in particular says 1) no spurious local minima existsl; 2) whenever at some point (U, V) so that the gradi-
ent is small and the Hessian does not have signiﬁcant negative eigenvalue, then the distance to global optimal (see
Deﬁnition 6 and Deﬁnition 7) is guaranteed to be small. Such a point can be found efﬁciently (see Section 6).

3.2 Matrix Completion
Matrix completion is a popular technique in recommendation systems and collaborative ﬁltering [Koren, 2009, Rennie
and Srebro, 2005]. In this problem, again we have an unknown low rank matrix M(cid:63). We observe each entry of the
matrix M(cid:63) independently with probability p. Let Ω ⊂ [d1] × [d2] be a set of observed entries. For any matrix M,
we use MΩ to denote the matrix whose entries outside of Ω are set to 0. That is, [MΩ]i,j = Mi,j if (i, j) ∈ Ω, and
[MΩ]i,j = 0 otherwise. We further use (cid:107)M(cid:107)Ω to denote (cid:107)MΩ(cid:107)F . Matrix completion can be viewed as a special case
of matrix sensing, where the sensing matrices only have one nonzero entry. However such matrices do not satisfy the
RIP condition.

In order to solve matrix completion, we try to optimize the following:

min

M∈Rd1×d2 ,rank(M)=r

(cid:107)M − M(cid:63)(cid:107)2
Ω.

1
2p

A well-known problem in matrix completion is that when the true matrix M(cid:63) is very sparse, then we are very
likely to observe only 0 entries, and has no chance to learn the other entries of M(cid:63). To avoid this case, previous works
have assumed following incoherence condition:
Deﬁnition 4. A rank r matrix M ∈ Rd1×d2 is µ-incoherent, if for the rank-r SVD XDY(cid:62) of M, we have for all
i ∈ [d1], j ∈ [d2]

j Y(cid:107) ≤(cid:112)µr/d1.

(cid:107)e(cid:62)

We assume the unknown optimal low rank matrix M(cid:63) is µ-incoherent.
In the non-convex program, we try to make sure the decomposition UV(cid:62) is also incoherent by adding a regularizer

(cid:107)e(cid:62)

i X(cid:107) ≤(cid:112)µr/d2,
d1(cid:88)

d2(cid:88)

Q(U, V) = λ1

((cid:107)e(cid:62)

i U(cid:107) − α1)4

+ + λ2

((cid:107)e(cid:62)

j V(cid:107) − α2)4
+.

Here λ1, λ2, α1, α2 are parameters that we choose later, (x)+ = max{x, 0}. Using this regularizer, we can now
transform the objective function to the unconstraint form

i=1

j=1

min
U,V

1
2p

(cid:107)UV(cid:62) − M(cid:63)(cid:107)2

Ω +

1
8

(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2

F + Q(U, V).

(4)

Using the framework, we can show following:

Theorem 4. Let d = max{d1, d2}, when sample rate p ≥ Ω( µ4r6(κ(cid:63))6 log d
and λ1 = Θ( d1
local minima satisfy UV(cid:62) = M(cid:63) 2) The objective is (, Ω(σ(cid:63)

2 = Θ( µrσ(cid:63)
µrκ(cid:63) ). With probability at least 1 − 1/poly(d), for Objective Function (4) we have 1) all

))-strict saddle for polynomially small .

min{d1,d2} ), choose α2

µrκ(cid:63) ), λ2 = Θ( d2

), α2

1 = Θ( µrσ(cid:63)

1

d1

1

)

d2

r ), O( 
σ(cid:63)
r

3.3 Robust PCA
Robust PCA [Cand`es et al., 2011] is a generalization to the standard Principled Component Analysis.
In Robust
PCA, we are given an observation matrix Mo, which is an true underlying matrix M(cid:63) corrupted by a sparse noise S(cid:63)
(Mo = M(cid:63) + S(cid:63)). In some sense the goal is to decompose the matrix M into these two components. There are many
models on how many entries can be perturbed, and how they are distributed. In this paper we work in the setting where
M(cid:63) is µ-incoherent, and the rows/columns of S(cid:63) can have at most α-fraction non-zero entries.

5

In order to express robust PCA as an optimization problem, we need constraints on both M and S:

1
2

(cid:107)M + S − Mo(cid:107)2
F .

min
s.t. rank(M) ≤ r, S is sparse.

(5)

There can be several ways to specify the sparsity of S. In this paper we restrict attention to the following set:

(cid:27)

(cid:26)

S ∈ Rd1×d2 | S has at most α-fraction non-zero entries each column/row, and (cid:107)S(cid:107)∞ ≤ 2

Sα =
Assuming the true sparse matrix S(cid:63) is in Sα. Note that the inﬁnite norm requirement on S(cid:63) is without loss of
. Any entry larger
In objective function, we allow S to be γ times denser (in Sγα) where γ is a parameter we choose later. Now the

generality, because by incoherence M(cid:63) cannot have entries with absolute value more than µrσ(cid:63)
1√
d1d2
than that is obviously in the support of S(cid:63) and can be truncated.

µrσ(cid:63)
1√
d1d2

.

constraint optimization problem can be tranformed to the unconstraint problem

min
U,V

f (U, V) +

(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2
F ,

1
8

f (U, V) := min
S∈Sγα

(cid:107)UV(cid:62) + S − Mo(cid:107)2
F .

1
2

(6)

Of course, we can also think of this as a joint minimization problem of U, V, S. However we choose to present it
this way in order to allow extension of the strict-saddle condition. Since f (U, V) is not twice-differetiable w.r.t U, V,
it does not admit Hessian matrix, so we use the following generalized version of strict-saddle
Deﬁnition 5. We say function f (·) is (θ, γ, ζ)-pseudo strict saddle if for any x, at least one of followings holds:

1. (cid:107)∇f (x)(cid:107) ≥ θ.
2. ∃gx(·) so that ∀y, gx(y) ≥ f (y); gx(x) = f (x); λmin(∇2gx(x)) ≤ −γ.
3. x is ζ-close to X (cid:63) – the set of local minima.
Note that in this deﬁnition, the upperbound in 2 can be viewed as similar to the idea of subgradient. For functions
with non-differentiable points, subgradient is deﬁned so that it still offers a lowerbound for the function. In our case
this is very similar – although Hessian is not deﬁned, we can use a smooth function that upperbounds the current
function (upper-bound is required for minimization). In the case of robust PCA the upperbound is obtained by a ﬁxed
S. Using this formalization we can prove
Theorem 5. There is an absolute constant c > 0, if γ > c, and γα· µr· (κ(cid:63))5 ≤ 1
we have 1) all local minima satisﬁes UV(cid:62) = M(cid:63); 2) objective function is (, Ω(σ(cid:63)
for polynomially small .

c holds, for objective function Eq.(6)
))-pseudo strict saddle

√
r ), O( 
σ(cid:63)
r

κ(cid:63)

4 Framework for Symmetric Positive Deﬁnite Problems

In this section we describe our framework in the simpler setting where the desired matrix is positive semideﬁnite. In
particular, suppose the true matrix M(cid:63) we are looking for can be written as M(cid:63) = U(cid:63)(U(cid:63))(cid:62) where U(cid:63) ∈ Rd×r. For
objective functions that is quadratic over M, we denote its Hessian as H and we can write the objective as

min

M∈Rd×d

sym ,rank(M)=r

(M − M(cid:63)) : H : (M − M(cid:63)),

1
2

(7)

6

We call this objective function f (M). Via Burer-Monteiro factorization, the corresponding unconstraint optimization
problem, with regularization Q can be written as

min

U∈Rn×r

1
2

(UU(cid:62) − M(cid:63)) : H : (UU(cid:62) − M(cid:63)) + Q(U).

(8)

In this section, we also denote f (U) as objective function with respect to parameter U, abuse the notation of f (M)

previously deﬁned over M.

Direction of Improvement The optimality condition (Deﬁnition 1) implies if the gradient is non-zero, or if we can
ﬁnd a negative direction of the Hessian (that is a direction v, so that v(cid:62)∇2f (x)v < 0), then the point is not a local
minimum. A common technique in characterizing the optimization landscape is therefore trying to explicitly ﬁnd this
negative direction. We call this the direction of improvement. Different works [Bhojanapalli et al., 2016, Ge et al.,
2016] have chosen very different directions of improvement.

In our framework, we show it sufﬁces to choose a single direction ∆ as the direction of improvement. Intuitively,
this direction should bring us close to the true solution U(cid:63) from the current point U. Due to rotational symmetry
(U and UR behave the same for the objective if R is a rotation matrix), we need to carefully deﬁne the difference
between U and U(cid:63).
Deﬁnition 6. Given matrices U, U(cid:63) ∈ Rd×r, deﬁne their difference ∆ = U − U(cid:63)R, where R ∈ Rr×r is chosen as
R = argminZ(cid:62)Z=ZZ(cid:62)=I (cid:107)U − U(cid:63)Z(cid:107)2
F .

Note that this deﬁnition tries to “align” U and U(cid:63) before taking their difference, and therefore is invariant under
rotations. In particular, this deﬁnition has the nice property that as long as M = UU(cid:62) is close to M(cid:63) = U(cid:63)(U(cid:63))(cid:62),
we have ∆ is small (we defer the proof to Appendix):
Lemma 6. Given matrices U, U(cid:63) ∈ Rd×r, let M = UU(cid:62) and M(cid:63) = U(cid:63)(U(cid:63))(cid:62), and let ∆ be deﬁned as in
Deﬁnition 6, then we have (cid:107)∆∆(cid:62)(cid:107)2

F ≤ 2(cid:107)M − M(cid:63)(cid:107)2

(cid:107)M − M(cid:63)(cid:107)2
F .

r(cid:107)∆(cid:107)2

F ≤

F , and σ(cid:63)

√
1
2−1)

2(

Now we can state the main Lemma:

Lemma 7 (Main). For the objective (8), let ∆ be deﬁned as in Deﬁnition 6 and M = UU(cid:62). Then, for any U ∈ Rd×r,
we have

∆ : ∇2f (U) : ∆ =∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63))

(9)
To see why this lemma is useful, let us look at the simplest case where Q(U) = 0 and H is identity. In this case,

+ 4(cid:104)∇f (U), ∆(cid:105) + [∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)]

if gradient is zero, by Eq. (9)

∆ : ∇2f (U) : ∆ = (cid:107)∆∆(cid:107)2

F − 3(cid:107)M − M(cid:63)(cid:107)2

F

By Lemma 6 this is no more than −(cid:107)M − M(cid:63)(cid:107)2

F . Therefore, all stationary point with M (cid:54)= M∗ must be saddle

points, and we immediately conclude all local minimum satisﬁes UU(cid:62) = M(cid:63)!
Interaction with Regularizer For problems such as matrix completion, the Hessian H does not preserve the norm
for all low rank matrices. In these cases we need to use additional regularizer. In particular, conceptually we need the
following steps:

1. Show that the regularizer Q ensures for any U such that ∇f (U) = 0, U ∈ B for some set B.
2. Show that whenever U ∈ B, the Hessian operator H behaves similarly as identity: for some c > 0 we have:

∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63)) < −c(cid:107)∆(cid:107)2
F .

3. Show that the regularizer does not contribute a large positive term to ∆ : ∇2f (U) : ∆. This means we show an

upperbound for 4(cid:104)∇f (U), ∆(cid:105) + [∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)].

Interestingly, these steps are not just useful for handling regularizers. Any deviation to the original model (such
as noise, or if the optimal matrix is not exactly low rank) can be viewed as an additional “regularizer” function Q(U)
and argued in the same framework. See e.g. Section C.

7

4.1 Matrix Sensing
Matrix sensing is the ideal setting for this framework. For symmetric matrix sensing, the objective function is

m(cid:88)

i=1

min

U∈Rd×r

1
2m

((cid:104)Ai, UU(cid:62)(cid:105) − bi)2.

(10)

Recall that matrices {Ai : i = 1, 2, ..., m} are known sensing matrices, and bi = (cid:104)Ai, M(cid:63)(cid:105) is the result of i-th
observation. The intended solution is the unknown low rank matrix M(cid:63) = U(cid:63)(U(cid:63))(cid:62). For any low rank matrix M,
the Hessian operator satisﬁes

m(cid:88)

M : H : M =

(cid:104)Ai, M(cid:105)2.

i=1

Therefore if the sensing matrices satisfy the RIP property (Deﬁnition 3), the Hessian operator is close to identity for
all low rank matrices! In the symmetric case there is no regularizer, so the landscape for symmetric matrix sensing
follows immediately from our main Lemma 7.
Theorem 8. When measurement {Ai} satisﬁes (2r, 1
minima U satisfy UU(cid:62) = M(cid:63); 2) the function is (, Ω(σ(cid:63)
Proof. For point U with small gradient satisfying (cid:107)∇f (U)(cid:107)F ≤ , by (2r, δ2r)-RIP property:

10 )-RIP, for matrix sensing objective (10) we have 1) all local

))-strict saddle.

r ), O( 
σ(cid:63)
r

∆ : ∇2f (U) : ∆ =∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63)) + 4(cid:104)∇f (U), ∆(cid:105)

F + 4(cid:107)∆(cid:107)F

F − 3(1 − δ2r)(cid:107)M − M(cid:63)(cid:107)2

≤(1 + δ2r)(cid:107)∆∆(cid:62)(cid:107)2
≤ − (1 − 5δ2r)(cid:107)M − M(cid:63)(cid:107)2
≤ − 0.4σ(cid:63)
F + 4(cid:107)∆(cid:107)F
The second last inequality is due to Lemma 6 that (cid:107)∆∆(cid:62)(cid:107)2
F, and last inequality is due to δ2r = 1
and second part of Lemma 6. This means if U is not close to U(cid:63), that is, if (cid:107)∆(cid:107)F ≥ 20
, we have ∆ : ∇2f (U) :
∆ ≤ −0.2σ(cid:63)
)-strict saddle property. Take  = 0, we know all stationary points with
(cid:107)∆(cid:107)F (cid:54)= 0 are saddle points. This means all local minima are global minima (satisfying UU(cid:62) = M(cid:63)), which ﬁnishes
the proof.

F ≤ 2(cid:107)M − M(cid:63)(cid:107)2

F. This proves (, 0.2σ(cid:63)

F + 4(cid:107)∆(cid:107)F

r(cid:107)∆(cid:107)2

r(cid:107)∆(cid:107)2

r , 20
σ(cid:63)
r

σ(cid:63)
r

10

4.2 Matrix Completion
For matrix completion, we need to ensure the incoherence condition (Deﬁnition 4).
In order to do that, we add
a regularizer Q(U) that penalize the objective function when some row of U is too large. We choose the same

regularizer as Ge et al. [2016]: Q(U) = λ(cid:80)d

+. The objective is then

i=1((cid:107)Ui(cid:107) − α)4

min

U∈Rd×r

1
2p

(cid:107)M(cid:63) − UU(cid:62)(cid:107)2

Ω + Q(U).

(11)

Using our framework, we ﬁrst need to show that the regularizer ensures all rows of U are small (step 1).

Lemma 9. There exists an absolute constant c, when sample rate p ≥ Ω( µr
d ) and λ = Θ( d
we have for any points U with (cid:107)∇f (U)(cid:107)F ≤  for polynomially small , with probability at least 1 − 1/poly(d):

d log d), α2 = Θ( µrσ(cid:63)

1

µrκ(cid:63) ),

(cid:18) (µr)1.5κ(cid:63)σ(cid:63)

1

(cid:19)

d

(cid:107)e(cid:62)

i U(cid:107)2 ≤ O

max

i

This is a slightly stronger version of Lemma 4.7 in Ge et al. [2016]. Next we show under this regularizer, we can

still select the direction ∆, and the ﬁrst part of Equation (9) is signiﬁcantly negative when ∆ is large (step 2):

8

Lemma 10. When sample rate p ≥ Ω( µ3r4(κ(cid:63))4 log d
at least 1 − 1/poly(d), for all U with (cid:107)∇f (U)(cid:107)F ≤  for polynomially small  we have
∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63)) ≤ −0.3σ(cid:63)

), by choosing α2 = Θ( µrσ(cid:63)

d

r(cid:107)∆(cid:107)2

F

1

d ) and λ = Θ( d

µrκ(cid:63) ) with probability

This lemma follows from several standard concentration inequalities, and is made possible because of the incoher-

ence bound we proved in the previous lemma.

Finally we show the additional regularizer related term in Equation (9) is bounded (step 3).

Lemma 11. By choosing α2 = Θ( µrσ(cid:63)

d ) and λα2 ≤ O(σ(cid:63)

1

r ), we have:

[∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)] ≤ 0.1σ(cid:63)

r(cid:107)∆(cid:107)2

F

1
4

Combining these three lemmas, it is easy to see

Theorem 12. When sample rate p ≥ Ω( µ3r4(κ(cid:63))4 log d
µrκ(cid:63) ). Then with
probability at least 1−1/poly(d), for matrix completion objective (11) we have 1) all local minima satisfy UU(cid:62) = M(cid:63)
2) the function is (, Ω(σ(cid:63)

))-strict saddle for polynomially small .

), by choosing α2 = Θ( µrσ(cid:63)

d ) and λ = Θ( d

d

1

r ), O( 
σ(cid:63)
r

Notice that our proof is different from Ge et al. [2016], as we focus on the direction ∆ for both ﬁrst and second
order conditions while they need to select different directions for the Hessian. The framework allowed us to get a
simpler proof, generalize to asymmetric case and also improved the dependencies on rank.

4.3 Robust PCA
In the robust PCA problem, for any given matrix M the objective function try to ﬁnd the optimal sparse perturbation
S. In the symmetric PSD case, recall we observe Mo = M(cid:63) + S(cid:63), we deﬁne the set of sparse matrices to be

(cid:26)

(cid:27)

.

(12)

Sα =

S ∈ Rd×d | S has at most α-fraction non-zero entries each column/row, and (cid:107)S(cid:107)∞ ≤ 2

µrσ(cid:63)
1

d

Note the projection onto set Sα be computed in polynomial time (using a max ﬂow algorithm).

We assume S(cid:63) ∈ Sα, the objective can be written as

min
U

f (U),

where

f (U) := min
S∈Sγα

(cid:107)UU(cid:62) + S − Mo(cid:107)2
F .

1
2

Here γ is a slack parameter that we choose later.
2(cid:107)UU(cid:62) + S − Mo(cid:107)2

Note that now the objective function f (U) is not quadratic, so we cannot use the framework directly. However, if
F is a quadratic function with Hessian equal to identity. We can still
we ﬁx S, then fS(U) := 1
apply our framework to this function. In this case, since the Hessian is identity for all matrices, we can skip the ﬁrst
step. The problem becomes a matrix factorization problem:

(13)
The difference here is that the matrix A (which is M(cid:63) + S(cid:63) − S) is not equal to M(cid:63) and is in general not low rank.
We can use the framework to analyze this problem (and treat the residue A − M(cid:63) as the “regularizer” Q(U)).
Lemma 13. Let A ∈ Rd×d be a symmetric PSD matrix, and matrix factorization objective to be:

U∈Rd×r

min

(cid:107)A − UU(cid:62)(cid:107)2
F .

1
2

f (U) = (cid:107)UU(cid:62) − A(cid:107)2

F

where σr(A) ≥ 15σr+1(A).
objective is (, Ω(σ(cid:63)

r ), O( 
σ(cid:63)
r

))-strict saddle.

then 1) all local minima satisﬁes UU(cid:62) = Pr(A) (best rank-r approximation), 2)

9

To deal with the case S not ﬁxed (but as minimizer of Eq.(12)), we let U†(U†)(cid:62) be the best rank r-approximation
of M(cid:63) + S(cid:63) − S. The next lemma shows when U is close to U† up to some rotation, U will actually be already close
to U(cid:63) up to some rotation.
Lemma 14. There is an absolute constant c, assume γ > c, and γα· µr· (κ(cid:63))5 ≤ 1
c . Let U†(U†)(cid:62) be the best rank r-
approximation of M(cid:63) + S(cid:63) − S, where S is the minimizer as in Eq.(12). Assume minR(cid:62)R=RR(cid:62)=I (cid:107)U − U†R(cid:107)F ≤ .
Let ∆ be deﬁned as in Deﬁnition 6, then (cid:107)∆(cid:107)F ≤ O(

κ(cid:63)) for polynomially small .

√

The proof of Lemma 14 is inspired by Yi et al. [2016] and uses the property of the optimally chosen sparse set S.

Combining these two lemmas we get our main result:
Theorem 15. There is an absolute constant c, if γ > c, and γα · µr · (κ(cid:63))5 ≤ 1
we have 1) all local minima satisﬁes UU(cid:62) = M(cid:63); 2) objective function is (, Ω(σ(cid:63)
for polynomially small .

c holds, for objective function Eq.(12)
))-pseudo strict saddle

√
r ), O( 
σ(cid:63)
r

κ(cid:63)

5 Handling Asymmetric Matrices

In this section we show how to reduce problems on asymmetric matrices to problems on symmetric PSD matrices.

Let M(cid:63) = U(cid:63)V(cid:63)(cid:62), and M = UV(cid:62), and objective function:
f (U, V) = 2(M − M(cid:63)) : H0 : (M − M(cid:63)) +
1
2

(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2

F + Q0(U, V)

Note this is a scaled version of objectives introduced in Sec.3 (multiplied by 4), and scaling will not change the
property of local minima, global minima and saddle points.
We view the problem as if it is trying to ﬁnd a (d1 + d2) × r matrix, whose ﬁrst d1 rows are equal to U, and last

d2 rows are equal to V.
Deﬁnition 7. Suppose M(cid:63) is the optimal solution, and its SVD is X(cid:63)D(cid:63)Y(cid:63)(cid:62). Let U(cid:63) = X(cid:63)(D(cid:63)) 1
M = UV(cid:62) is the current point, we reduce the problem into a symmetric case using following notations.

2 , V(cid:63) = Y(cid:63)(D(cid:63)) 1
2 ,

W =

, W(cid:63) =

, N = WW(cid:62), N(cid:63) = W(cid:63)W(cid:63)(cid:62)

(14)

(cid:18)U

(cid:19)

V

(cid:18)U(cid:63)

(cid:19)

V(cid:63)

Further, ∆ is deﬁned to be the difference between W and W(cid:63) up to rotation as in Deﬁnition 6.

We will also transform the Hessian operators to operate on (d1 + d2) × r matrices. In particular, deﬁne Hessian

H1,G such that for all W we have:

N : H1 : N = M : H0 : M
N : G : N = (cid:107)U(cid:62)U − V(cid:62)V(cid:107)2

F

Now, let Q(W) = Q(U, V), and we can rewrite the objective function f (W) as

1
2

[(N − N(cid:63)) : 4H1 : (N − N(cid:63)) + N : G : N] + Q(W)

(15)
We know H0 perserves the norm of low rank matrices M. To reduce asymmetric problems to symmetric problem,
intuitively, we also hope H0 to approximately preserve the norm of N. However this is impossible as by deﬁnition, H0
only acts on M, which is the off-diagonal blocks of N. We can expect N : H0 : N to be close to the norm of UV(cid:62),
but for all matrices U, V with the same UV(cid:62), the matrix N can have very different norms. The easiest example is
to consider U = diag(1/, ) and V = diag(, 1/): while UV(cid:62) = I no matter what  is, the norm of N is of order
1/2 and can change drastically. The regularizer is exactly there to handle this case: the Hessian G of the regularizer
will be related to the norm of the diagonal components, therefore allowing the full Hessian H = 4H1 + G to still be
approximately identity.

Now we can formalize the reduction as the following main Lemma:

10

Lemma 16. For the objective (15), let ∆, N, N(cid:63) be deﬁned as in Deﬁnition 7. Then, for any W ∈ R(d1+d2)×r, we
have

∆ : ∇2f (W) : ∆ ≤∆∆(cid:62) : H : ∆∆(cid:62) − 3(N − N(cid:63)) : H : (N − N(cid:63))

+ 4(cid:104)∇f (W), ∆(cid:105) + [∆ : ∇2Q(W) : ∆ − 4(cid:104)∇Q(W), ∆(cid:105)]

(16)
F for some matrix M = UV(cid:62), let W and

where H = 4H1 + G. Further, if H0 satisﬁes M : H0 : M ∈ (1 ± δ)(cid:107)M(cid:107)2
N be deﬁned as in (14), then N : H : N ∈ (1 ± 2δ)(cid:107)N(cid:107)2
F .

Intuitively, this lemma shows the same direction of improvement works as before, and the regularizer is exactly

what it requires to maintain the norm-preserving property of the Hessian.

Below we prove Theorem 3, which show for matrix sensing 1) all local minima satisfy M = M(cid:63); 2) strict saddle

(cid:80)m
i=1(cid:104)Ai, M(cid:105)2 and regularization Q(W) = 0. Since H0 is
Similar to the symmetric case, for point W with small gradient satisfying (cid:107)∇f (W)(cid:107)F ≤ , by (2r, 1/10)-RIP

property is satisﬁed. Other proofs are deferred to appendix.
Proof of Theorem 3. In this case, M : H0 : M = 1
(2r, 1/20)-RIP, by Lemma 16, we have H = 4H1 + G satisfying (2r, 1/10)-RIP.
property of H (let δ = 1/10) we have

m

∆ : ∇2f (U) : ∆ =∆∆(cid:62) : H : ∆∆(cid:62) − 3(N − N(cid:63)) : H : (N − N(cid:63)) + 4(cid:104)∇f (U), ∆(cid:105)

F + 4(cid:107)∆(cid:107)F

F − 3(1 − δ)(cid:107)N − N(cid:63)(cid:107)2

≤(1 + δ)(cid:107)∆∆(cid:62)(cid:107)2
≤ − (1 − 5δ)(cid:107)N − N(cid:63)(cid:107)2
F + 4(cid:107)∆(cid:107)F
≤ − 0.4σ(cid:63)
The second last inequality is due to Lemma 6 that (cid:107)∆∆(cid:62)(cid:107)2
F, and last inequality is due to δ = 1
and second part of Lemma 6. This means if W is not close to W(cid:63), that is, if (cid:107)∆(cid:107)F ≥ 20
, we have ∆ : ∇2f (W) :
∆ ≤ −0.2σ(cid:63)
)-strict saddle property. Take  = 0, we know all stationary points
with (cid:107)∆(cid:107)F (cid:54)= 0 are saddle points. This means all local minima satisfy WW(cid:62) = N(cid:63), which in particular implies
UV(cid:62) = M(cid:63) because M(cid:63) is a submatrix of N(cid:63).

F ≤ 2(cid:107)N − N(cid:63)(cid:107)2

F. This proves (, 0.2σ(cid:63)

r(cid:107)∆(cid:107)2

r , 20
σ(cid:63)
r

F + 4(cid:107)∆(cid:107)F

r(cid:107)∆(cid:107)2

σ(cid:63)
r

10

6 Runtime

In this section we give the precise statement of Corollary 2:
properties we prove.

the runtime of algorithms implied by the geometric

In order to translate the geometric result into runtime guarantees, many algorithms require additional smoothness

conditions. We say a function f (x) is l-smooth if for all x, y,

(cid:107)∇f (y) − ∇f (x)(cid:107) ≤ l(cid:107)y − x(cid:107).

This is a standard assumption in optimization.
Lipschitz if for all x, y

In order to avoid saddle points, say a function f (x) is ρ-Hessian

(cid:107)∇2f (y) − ∇2f (x)(cid:107) ≤ ρ(cid:107)y − x(cid:107).

We call an optimization algorithm saddle-avoiding if the algorithm is able to ﬁnd a point with small gradient and
almost positive semideﬁnite Hessian.
Deﬁnition 8. A local search algorithm is called saddle-avoiding, if for a function f : Rd → R that is l-smooth and
ρ-Lipschitz Hessian, given a point x such that either (cid:107)∇f (x)(cid:107) ≥  or λmin(∇2f (x)) ≤ −√
ρ, can ﬁnd a point y in
poly(1/, d, l, ρ) iterations such that f (y) ≤ f (x) − δ where δ = poly(d, l, ρ, ).

11

−√

As a immediate corollary, we know such algorithms can ﬁnd a point x such that (cid:107)∇f (x)(cid:107) ≤  and λmin(∇2f (x)) ≥
ρ in poly(1/, d, l, ρ) iterations.
Existing results show many algorithms are saddle-avoiding, including cubic regularization [Nesterov and Polyak,
2006], stochastic gradient descent [Ge et al., 2015], trust-region algorithms [Sun et al., 2015b]. The most recent
algorithms [Jin et al., 2017, Carmon et al., 2016, Agarwal et al., 2016] are more efﬁcient: in particular the number of
iterations only depend poly-logarithmic on dimension d. Now we are ready to formally state Corollary 2.

Corollary 17. Let R be the Frobenius norm of the initial points U0, V0, a saddle-avoiding local search algorithm can
ﬁnd a point -close to global optimal for matrix sensing (10)(3), matrix completion (11)(4) in poly(R, 1/, d, σ(cid:63)
1, 1/σ(cid:63)
r )
iterations. For robust PCA (12)(6), alternating between a saddle-avoiding local search algorithm and computing
optimal S ∈ Sγα will ﬁnd a point -close to global optimal in poly(R, 1/, d, σ(cid:63)

r ) iterations.

1, 1/σ(cid:63)

This corollary states the existence of simple local search algorithms which can efﬁciently optimizing non-convex
objectives of matrix sensing, matrix completion and robust PCA in polynomial time. The proof essentially follows
from the guarantees of the saddle-avoiding algorithm and the strict-saddle properties we prove. We will sketch the
proof in Section D.

Towards faster convergence For many low-rank matrices problems, in the neighborhood of local minima, objective
function satisﬁes conditions similar to strong convexity [Zheng and Lafferty, 2016, Bhojanapalli et al., 2016] (more
precisely, the (α, β)-regularity condition as Assumption A3.b in [Jin et al., 2017]). Jin et al. [2017] showed a principle
way of how to combine these strong local structures with saddle-avoiding algorithm to give global linear convergence.
Therefore, it is likely that some saddle-avoiding algorithms (such as perturbed gradient descent) can achieve linear
convergence for these problems.

7 Conclusions

In this paper we give a framework that explains the recent success in understanding optimization landscape for low
rank matrix problems. Our framework connects and simpliﬁes the existing proofs, and generalizes to new settings
such as asymmetric matrix completion and robust PCA. The key observation is when the Hessian operator preserves
the norm of certain matrices, one can use the same directions of improvement to prove similar optimization landscape.
F is exactly what it requires to maintain this norm preserving property
We show the regularizer 1
in the asymmetric case.Our analysis also allows the interaction between regularizer and Hessian to handle difﬁcult
settings such as.

4(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2

For low rank matrix problems, there are generalizations such as weighted matrix factorization[Li et al., 2016] and
1-bit matrix sensing[Davenport et al., 2014] where the Hessian operator may behave differently as the settings we can
analyze. How to characterize the optimization landscape in these settings is still an open problem.

In order to get general ways of understanding optimization landscapes for more generally, there are still many open
problems. In particular, how can we decide whether two problems are similar enough to share the same optimization
landscape? A minimum requirement is that the non-convex problem should have the same symmetry structure – the set
of equivalent global optimum should be the same. In this work, we show if the problems come from convex objective
functions with similar Hessian properties, then they have the same optimization landscape. We hope this serves as a
ﬁrst step towards general tools for understanding optimization landscape for groups of problems.

12

References

Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma. Finding approximate local minima

for nonconvex optimization in linear time. arXiv preprint arXiv:1611.01146, 2016.

Mikhail Belkin, Luis Rademacher, and James Voss. Basis learning as an algorithmic primitive. arXiv preprint

arXiv:1411.1420, 2014.

Yoshua Bengio. Learning deep architectures for AI. Foundations and trends R(cid:13) in Machine Learning, 2(1):1–127,

2009.

Srinadh Bhojanapalli, Anastasios Kyrillidis, and Sujay Sanghavi. Dropping convexity for faster semi-deﬁnite opti-

mization. arXiv:1509.03917, 2015.

Srinadh Bhojanapalli, Behnam Neyshabur, and Nathan Srebro. Global optimality of local search for low rank matrix

recovery. arXiv preprint arXiv:1605.07221, 2016.

Samuel Burer and Renato DC Monteiro. A nonlinear programming algorithm for solving semideﬁnite programs via

low-rank factorization. Mathematical Programming, 95(2):329–357, 2003.

Emmanuel Candes and Benjamin Recht. Exact matrix completion via convex optimization. Communications of the

ACM, 55(6):111–119, 2012.

Emmanuel J Candes and Yaniv Plan. Tight oracle inequalities for low-rank matrix recovery from a minimal number

of noisy random measurements. IEEE Transactions on Information Theory, 57(4):2342–2359, 2011.

Emmanuel J Cand`es and Benjamin Recht. Exact matrix completion via convex optimization. Foundations of Compu-

tational mathematics, 9(6):717–772, 2009.

Emmanuel J Candes, Justin K Romberg, and Terence Tao. Stable signal recovery from incomplete and inaccurate

measurements. Communications on pure and applied mathematics, 59(8):1207–1223, 2006.

Emmanuel J Cand`es, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis? Journal of the

ACM (JACM), 58(3):11, 2011.

Yair Carmon, John C Duchi, Oliver Hinder, and Aaron Sidford. Accelerated methods for non-convex optimization.

arXiv preprint arXiv:1611.00756, 2016.

Yudong Chen and Martin J Wainwright. Fast low-rank estimation by projected gradient descent: General statistical

and algorithmic guarantees. arXiv preprint arXiv:1509.03025, 2015.

Mark A Davenport and Justin Romberg. An overview of low-rank matrix recovery from incomplete observations.

IEEE Journal of Selected Topics in Signal Processing, 10(4):608–622, 2016.

Mark A Davenport, Yaniv Plan, Ewout van den Berg, and Mary Wootters. 1-bit matrix completion. Information and

Inference, 3(3):189–223, 2014.

Maryam Fazel. Matrix rank minimization with applications. PhD thesis, PhD thesis, Stanford University, 2002.

Rong Ge, Furong Huang, Chi Jin, and Yang Yuan. Escaping from saddle points—online stochastic gradient for tensor

decomposition. arXiv:1503.02101, 2015.

Rong Ge, Jason D Lee, and Tengyu Ma. Matrix completion has no spurious local minimum. In Advances in Neural

Information Processing Systems, pages 2973–2981, 2016.

Moritz Hardt. Understanding alternating minimization for matrix completion. In FOCS 2014. IEEE, 2014.

Moritz Hardt and Mary Wootters. Fast matrix completion without the condition number.

638–678, 2014.

In COLT 2014, pages

13

Harold Hotelling. Analysis of a complex of statistical variables into principal components. Journal of educational

psychology, 24(6):417, 1933.

Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi. Low-rank matrix completion using alternating minimization.

In Proceedings of the forty-ﬁfth annual ACM symposium on Theory of computing, pages 665–674, 2013.

Chi Jin, Rong Ge, Praneeth Netrapalli, Sham M Kakade, and Michael I Jordan. How to escape saddle points efﬁciently.

arXiv preprint arXiv:1703.00887, 2017.

Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from a few entries. Information

Theory, IEEE Transactions on, 56(6):2980–2998, 2010a.

Raghunandan H Keshavan, Andrea Montanari, and Sewoong Oh. Matrix completion from noisy entries. The Journal

of Machine Learning Research, 11:2057–2078, 2010b.

Yehuda Koren. The bellkor solution to the netﬂix grand prize. Netﬂix prize documentation, 81, 2009.

Rafał Latała. Some estimates of norms of random matrices. Proceedings of the American Mathematical Society, 133

(5):1273–1282, 2005.

Qiuwei Li and Gongguo Tang. The nonconvex geometry of low-rank matrix optimizations with general objective

functions. arXiv preprint arXiv:1611.03060, 2016.

Yuanzhi Li, Yingyu Liang, and Andrej Risteski. Recovery guarantee of weighted low-rank approximation via alter-

nating minimization. arXiv preprint arXiv:1602.02262, 2016.

Zhouchen Lin, Minming Chen, and Yi Ma. The augmented lagrange multiplier method for exact recovery of corrupted

low-rank matrices. arXiv preprint arXiv:1009.5055, 2010.

Yurii Nesterov and Boris T Polyak. Cubic regularization of newton method and its global performance. Mathematical

Programming, 108(1):177–205, 2006.

Praneeth Netrapalli, UN Niranjan, Sujay Sanghavi, Animashree Anandkumar, and Prateek Jain. Non-convex robust

pca. In Advances in Neural Information Processing Systems, pages 1107–1115, 2014.

Dohyung Park, Anastasios Kyrillidis, Constantine Caramanis, and Sujay Sanghavi. Non-square matrix sensing without

spurious local minima via the burer-monteiro approach. arXiv preprint arXiv:1609.03240, 2016.

Benjamin Recht, Maryam Fazel, and Pablo A Parrilo. Guaranteed minimum-rank solutions of linear matrix equations

via nuclear norm minimization. SIAM review, 52(3):471–501, 2010.

Jasson DM Rennie and Nathan Srebro. Fast maximum margin matrix factorization for collaborative prediction. In

Proceedings of the 22nd international conference on Machine learning, pages 713–719. ACM, 2005.

Ju Sun, Qing Qu, and John Wright. Complete dictionary recovery over the sphere I: Overview and the geometric

picture. arXiv:1511.03607, 2015a.

Ju Sun, Qing Qu, and John Wright. When are nonconvex problems not scary? arXiv preprint arXiv:1510.06096,

2015b.

Ruoyu Sun and Zhi-Quan Luo. Guaranteed matrix completion via nonconvex factorization. In Foundations of Com-

puter Science (FOCS), 2015 IEEE 56th Annual Symposium on, pages 270–289. IEEE, 2015.

Stephen Tu, Ross Boczar, Mahdi Soltanolkotabi, and Benjamin Recht. Low-rank solutions of linear matrix equations

via procrustes ﬂow. arXiv preprint arXiv:1507.03566, 2015.

Xinyang Yi, Dohyung Park, Yudong Chen, and Constantine Caramanis. Fast algorithms for robust pca via gradient

descent. In Advances in neural information processing systems, pages 4152–4160, 2016.

14

Xiao Zhang, Lingxiao Wang, and Quanquan Gu. A nonconvex free lunch for low-rank plus sparse matrix recovery.

arXiv preprint arXiv:1702.06525, 2017.

Tuo Zhao, Zhaoran Wang, and Han Liu. A nonconvex optimization framework for low rank matrix estimation. In

Advances in Neural Information Processing Systems, pages 559–567, 2015.

Qinqing Zheng and John Lafferty. Convergence analysis for rectangular matrix completion using burer-monteiro

factorization and gradient descent. arXiv preprint arXiv:1605.07051, 2016.

15

A Proofs for Symmetric Positive Deﬁnite Problems

In this section we provide the missing proofs for the symmetric matrix problems. First we prove Lemma 6 which
connects difference in the matrix U and the difference in the matrix M.
Lemma 6. Given matrices U, U(cid:63) ∈ Rd×r, let M = UU(cid:62) and M(cid:63) = U(cid:63)(U(cid:63))(cid:62), and let ∆ be deﬁned as in
Deﬁnition 6, then we have (cid:107)∆∆(cid:62)(cid:107)2
F ≤ 2(cid:107)M − M(cid:63)(cid:107)2
Proof. Recall in Deﬁnition 6, ∆ = U − U(cid:63)RU where

(cid:107)M − M(cid:63)(cid:107)2
F .

F , and σ(cid:63)

√
1
2−1)

r(cid:107)∆(cid:107)2

F ≤

2(

RU = argmin

R(cid:62)R=RR(cid:62)=I

(cid:107)U − U(cid:63)R(cid:107)2
F.

We ﬁrst prove following claim, which will used in many places across this proof:

This because by expanding the Frobenius norm, and letting the SVD of U(cid:63)(cid:62)U be ADB(cid:62), we have:

U(cid:62)U(cid:63)RU is a symmetric PSD matrix.

(17)

argmin

R:RR(cid:62)=R(cid:62)R=I

=

argmin

R:RR(cid:62)=R(cid:62)R=I

(cid:107)U − U(cid:63)R(cid:107)2
F =
−tr(U(cid:62)U(cid:63)R) =

argmin

R:RR(cid:62)=R(cid:62)R=I

argmin

R:RR(cid:62)=R(cid:62)R=I

−(cid:104)U, U(cid:63)R(cid:105)
−tr(DA(cid:62)RB)

Since A, B, R ∈ Rr×r are all orthonormal matrix, we know A(cid:62)RB is also orthonormal matrix. Moreover for any
orthonormal matrix T, we have:

(cid:88)

DiiTii ≤(cid:88)

Dii

i

tr(DT) =

i

The last inequality is because Dii is singular value thus non-negative, and T is orthonormal, thus Tii ≤ 1. This means
the maximum of tr(DT) is achieved when T = I, i.e., the minimum of −tr(DA(cid:62)RB) is achieved when R = AB(cid:62).
Therefore, U(cid:62)U(cid:63)RU = BDA(cid:62)AB(cid:62) = BDB(cid:62) is symmetric PSD matrix.

With Eq.(17), the remaining of proof directly follows from the results by substituting (U, Y) in Lemma 40 and 41

with (U(cid:63)RU, U).

Now we are ready to prove the main lemma.

Lemma 7 (Main). For the objective (8), let ∆ be deﬁned as in Deﬁnition 6 and M = UU(cid:62). Then, for any U ∈ Rd×r,
we have

∆ : ∇2f (U) : ∆ =∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63))

+ 4(cid:104)∇f (U), ∆(cid:105) + [∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)]

Proof. Recall the objective function is:

f (U) =

1
2

(UU(cid:62) − M(cid:63)) : H : (UU(cid:62) − M(cid:63)) + Q(U)

and let M = UU(cid:62). Calculating gradient and Hessian, we have for any Z ∈ Rd×r:

(cid:104)∇f (U), Z(cid:105) =(M − M(cid:63)) : H : (UZ(cid:62) + ZU(cid:62)) + (cid:104)∇Q(U), Z(cid:105)

Z : ∇2f (U) : Z =(UZ(cid:62) + ZU(cid:62)) : H : (UZ(cid:62) + ZU(cid:62)) + 2(M − M(cid:63)) : H : ZZ(cid:62) + Z : ∇2Q(U) : Z

(18)

16

Let Z = ∆ = U − U(cid:63)R as in Deﬁnition 6 and note M − M(cid:63) + ∆∆(cid:62) = U∆(cid:62) + ∆U(cid:62), then
∆ : ∇2f (U) : ∆ =(U∆(cid:62) + ∆U(cid:62)) : H : (U∆(cid:62) + ∆U(cid:62)) + 2(M − M(cid:63)) : H : ∆∆(cid:62) + ∆ : ∇2Q(U) : ∆

=(M − M(cid:63) + ∆∆(cid:62)) : H : (M − M(cid:63) + ∆∆(cid:62)) + 2(M − M(cid:63)) : H : ∆∆(cid:62) + ∆ : ∇2Q(U) : ∆
=∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63)) + 4(M − M(cid:63)) : H : (M − M(cid:63) + ∆∆(cid:62))

+ ∆ : ∇2Q(U) : ∆

=∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63)) + 4(cid:104)∇f (U), ∆(cid:105)

+ [∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)]

where in last line, we use the calculation of gradient ∇f (U) in Eq.18. This ﬁnishes the proof.

In the subsequent subsections we will prove the guarantees for matrix completion and robust PCA. The proof of

matrix sensing is already given in Section 4.

A.1 Matrix Completion
For matrix completion, the crucial component of the proof is the interaction between regularizer and the Hessian. We
ﬁrst state the properties (gradient and Hessian) of the regularizer Q here:

Lemma 18. The gradient and the hessian of regularization Q(U) = λ(cid:80)d

i=1((cid:107)e(cid:62)

i U(cid:107) − α)4

+ is:

d(cid:88)
d(cid:88)

i=1

(cid:104)∇Q(U), Z(cid:105) =4λ

Z : ∇2Q(U) : Z =4λ

((cid:107)e(cid:62)

i U(cid:107) − α)3

+

e(cid:62)
i UZ(cid:62)ei
(cid:107)eiU(cid:107)
(cid:107)e(cid:62)
i U(cid:107)2(cid:107)e(cid:62)

((cid:107)e(cid:62)

i U(cid:107) − α)3
d(cid:88)

+

((cid:107)e(cid:62)

i U(cid:107) − α)2

+

i=1

+ 12λ

i=1

i UZ(cid:62)ei)

(cid:18) e(cid:62)

i Z(cid:107)2 − (e(cid:62)
(cid:19)2
i U(cid:107)3
(cid:107)e(cid:62)
i UZ(cid:62)ei
(cid:107)eiU(cid:107)

(19)

(20)

Proof. This Lemma follows from direct calculation using linear algebra and calculus.

In the ﬁrst step of our framework, we hope to show that the regularizer forces the matrix U to not have large rows.
This is formalized and proved below (the Lemma is similar to Lemma 4.7 in Ge et al. [2016], but we get a stronger
guarantee here):
Lemma 9. There exists an absolute constant c, when sample rate p ≥ Ω( µr
d ) and λ = Θ( d
we have for any points U with (cid:107)∇f (U)(cid:107)F ≤  for polynomially small , with probability at least 1 − 1/poly(d):

d log d), α2 = Θ( µrσ(cid:63)

µrκ(cid:63) ),

1

(cid:18) (µr)1.5κ(cid:63)σ(cid:63)

1

(cid:19)

d

(cid:107)e(cid:62)

i U(cid:107)2 ≤ O

max

i

Proof. Recall the calculation of gradient:

∇f (U) =

2
p

(M − M(cid:63))ΩU + ∇Q(U)

where by Lemma 18, the gradient of regularizer is:

∇Q(U) = 4λ

d(cid:88)

((cid:107)e(cid:62)

i U(cid:107) − α)3

+

i=1

17

eie(cid:62)
i U
(cid:107)eiU(cid:107)2

Let i(cid:63) = argmaxi (cid:107)e(cid:62)

Lemma 9 we immediately prove the lemma. In case of (cid:107)e(cid:62)
Since (cid:107)∇f (U)(cid:107)F ≤ , we have (cid:104)∇f (U), ei(cid:63) e(cid:62)
probability, following holds:

i U(cid:107) be the row index with maximum 2-norm. If (cid:107)e(cid:62)
i(cid:63) U(cid:105) ≤ (cid:107)e(cid:62)

i(cid:63)∇f (U), e(cid:62)

i(cid:63) U(cid:105) = (cid:104)e(cid:62)

i(cid:63) U(cid:107) ≥ 2α, consider gradient along ei(cid:63) e(cid:62)

i(cid:63) U(cid:107) < 2α, by the choice of α in
i(cid:63) U direction.
i(cid:63) U(cid:107). Therefore, with 1− 1/poly(d)

(cid:107)e(cid:62)

i(cid:63) U(cid:107) ≥(cid:104)e(cid:62)

i(cid:63) U(cid:105)

≥4λ((cid:107)e(cid:62)

(cid:107)e(cid:62)

i(cid:63) (UU(cid:62))Ω(cid:105)

i(cid:63) U(cid:105) = (cid:104)e(cid:62)
i(cid:63) [

i(cid:63)∇f (U), e(cid:62)
(UU(cid:62) − M(cid:63))ΩU + ∇Q(U)], e(cid:62)
2
p
i(cid:63) U(cid:107) − 2
+(cid:107)e(cid:62)
i(cid:63) U(cid:107) − α)3
(cid:104)e(cid:62)
i(cid:63) (M(cid:63))Ω, e(cid:62)
p
i(cid:63) U(cid:107)4 − 2
i(cid:63) (M(cid:63))Ω(cid:107) · 1√
(cid:107)e(cid:62)
1√
p
p
√
i(cid:63) U(cid:107)4 − 2
1 + 0.01(cid:107)e(cid:62)
√
1)(cid:107)e(cid:62)
i(cid:63) U(cid:107)4 − O(
i(cid:63) U(cid:105) = (cid:107)e(cid:62)

i(cid:63) (UU(cid:62))Ω(cid:107)
(cid:107)e(cid:62)
√
i(cid:63)M(cid:63)(cid:107) · O(
d)(cid:107)UU(cid:62)(cid:107)∞
i(cid:63) U(cid:107)2

µrσ(cid:63)
i(cid:63) (UU(cid:62))ΩU, e(cid:62)

(cid:107)e(cid:62)
(cid:107)e(cid:62)

≥ λ
2
≥ λ
2
≥ λ
2

where second inequality use the fact (cid:104)e(cid:62)
Swartz; second last inequality is by Lemma 35 and our choice p ≥ Ω( µr
i(cid:63)M(cid:63)(cid:107), and by Lemma 39 we have 1√
p(cid:107)e(cid:62)
1√
inequality is because (cid:107)e(cid:62)

i(cid:63)M(cid:63)(cid:107) ≤(cid:112) µr

i(cid:63) (M(cid:63))Ω(cid:107) ≤ √

1 as M(cid:63) is µ-incoherent.

1 + 0.01(cid:107)e(cid:62)

d σ(cid:63)

Rearrange terms, we have:

√

i(cid:63) (UU(cid:62))Ω(cid:107)2 ≥ 0; third inequality is by Cauchy-
d log d) with large enough constant, we have
d)(cid:107)UU(cid:62)(cid:107)∞; the last
p(cid:107)e(cid:62)

√
i(cid:63) (UU(cid:62))Ω(cid:107) ≤ O(

By choosing  small enough to satisfy ( 

(cid:107)e(cid:62)

µrσ(cid:63)
1
λ

i(cid:63) U(cid:107)3 ≤ O(
3 ≤ √
µr·σ(cid:63)
λ ) 2
λ
i U(cid:107)2 ≤ c · max
(cid:107)e(cid:62)

1

(cid:26)

, this gives:

max

i

)(cid:107)e(cid:62)

i(cid:63) U(cid:107) +

2
λ

√

µr · σ(cid:63)
λ

1

α2,

(cid:27)

Finally, substituting our choice of α2 and λ, we ﬁnished the proof.

In the second step, we need to prove that the sum of Hessian H related terms in Equation (9) is signiﬁcantly

negative when U and U(cid:63) are not close.
Lemma 10. When sample rate p ≥ Ω( µ3r4(κ(cid:63))4 log d
at least 1 − 1/poly(d), for all U with (cid:107)∇f (U)(cid:107)F ≤  for polynomially small , we have
∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63)) ≤ −0.3σ(cid:63)

), by choosing α2 = Θ( µrσ(cid:63)

d

1

r(cid:107)∆(cid:107)2

F

d ) and λ = Θ( d

µrκ(cid:63) ) with probability

Proof. The key problem here is for the matrix ∆, it captures the difference between U and U(cid:63) and could have norms
concentrate in very few columns. Note that when ∆ is not incoherent, Hessian will still perserve norm for matrices like
∆U(cid:62) Lemma 35, but not necessarily perserve norm for matrices like ∆∆(cid:62). Therefore, we use different concentration
lemmas in different regimes (divided according to whether ∆ is small or large).
First by our choice of α, λ and Lemma 9, we know with 1 − 1/poly(d) probabilty, the maximum 2-norm of any

row of U will be small:

(cid:18) (µr)1.5κ(cid:63)σ(cid:63)

1

(cid:19)

d

(cid:107)e(cid:62)

i U(cid:107)2 ≤ O

max

i

Case 1: (cid:107)∆(cid:107)2

F ≤ σ(cid:63)

r /4.

In this case, ∆ is small, and ∆∆(cid:62) is even smaller. Although H does not perserve norm for ∆∆(cid:62) very well, it will
only contribute a very small factor to overall summation. Speciﬁcally, by our choice of p, we will have Lemma 35
holds with small constant δ and thus:

(cid:107)U(cid:63)∆(cid:62)(cid:107)2

Ω ≥ (1 − δ)(cid:107)U(cid:63)∆(cid:62)(cid:107)2

F ≥ (1 − δ)σ(cid:63)

r(cid:107)∆(cid:107)2

F

1
p

18

On the other hand, by Lemma 37, we know:

(cid:115)

(cid:107)∆∆(cid:62)(cid:107)2

Ω ≤(cid:107)∆(cid:107)4

F + O(

1
p

d
p

· (µr)1.5κ(cid:63)σ(cid:63)

1

d

)(cid:107)∆(cid:107)2

F ≤ (cid:107)∆(cid:107)4

F +

σ(cid:63)
r
4

(cid:107)∆(cid:107)2

F ≤ σ(cid:63)

r
2

(cid:107)∆(cid:107)2

F

this gives the summation:

∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63))
=∆∆(cid:62) : H : ∆∆(cid:62) − 3(U(cid:63)∆(cid:62) + ∆U(cid:63)(cid:62) + ∆∆(cid:62)) : H : (U(cid:63)∆(cid:62) + ∆U(cid:63)(cid:62) + ∆∆(cid:62))
≤ − 12(U(cid:63)∆(cid:62) : H : ∆∆(cid:62) + U(cid:63)∆(cid:62) : H : U(cid:63)∆(cid:62))
≤ − 12
(cid:107)U(cid:63)∆(cid:62)(cid:107)Ω((cid:107)U(cid:63)∆(cid:62)(cid:107)Ω − (cid:107)∆∆(cid:62)(cid:107)Ω)
((cid:107)U(cid:63)∆(cid:62)(cid:107)2
Ω − (cid:107)U(cid:63)∆(cid:62)(cid:107)Ω(cid:107)∆∆(cid:62)(cid:107)Ω) = − 12
√
√
p
p
r(cid:107)∆(cid:107)2
F ≤ −1.2σ(cid:63)
1 − δ(
≤ − 12

1 − δ −(cid:112)2/3)σ(cid:63)

F

r(cid:107)∆(cid:107)2
The last inequality is by choosing p ≥ Ω( µ3r4(κ(cid:63))4 log d
Case 2: (cid:107)∆(cid:107)2

F ≥ σ(cid:63)

r /4.

d

In this case ∆ is large, by Lemma 38 with high probability, our choice of p gives:

) with large enough constant factor, we have small δ.

(cid:107)∆∆(cid:62)(cid:107)2

Ω ≤(cid:107)∆∆(cid:62)(cid:107)2

F + O

1
p

≤(cid:107)∆∆(cid:62)(cid:107)2

F + O

dr log d

p

(cid:107)∆∆(cid:62)(cid:107)2∞ +

dr log d

(cid:107)∆∆(cid:62)(cid:107)F(cid:107)∆∆(cid:62)(cid:107)∞

· (µr)3(κ(cid:63)σ(cid:63)
1)2

d2

+

dr log d

· (µr)3(κ(cid:63)σ(cid:63)
1)2

d2

p
r(cid:107)∆(cid:107)2

F

F + 0.1σ(cid:63)

(cid:115)

p

(cid:115)

(cid:33)

(cid:33)

(cid:107)∆(cid:107)2

F

(cid:32)
(cid:32)

(cid:32)
(cid:32)

dr log d

p

+

σ(cid:63)
r
20

dr log d

p

dr log d

p
− σ(cid:63)
r
20

r )2
(σ(cid:63)
80
Again by Lemma 38 with high probability

≤(cid:107)∆∆(cid:62)(cid:107)2

F +

(cid:107)M − M(cid:63)(cid:107)2

Ω ≥(cid:107)M − M(cid:63)(cid:107)2

F − O

1
p

≥(cid:107)M − M(cid:63)(cid:107)2

F − O

≥(cid:107)M − M(cid:63)(cid:107)2

r )2
F − (σ(cid:63)
80

This gives:

(cid:107)∆(cid:107)2

F ≤ (cid:107)∆∆(cid:62)(cid:107)2
(cid:115)
(cid:115)

(cid:107)M − M(cid:63)(cid:107)2∞ +

1)2
· (µr)3(κ(cid:63)σ(cid:63)

+

d2

(cid:33)

(cid:107)M − M(cid:63)(cid:107)F(cid:107)M − M(cid:63)(cid:107)∞

dr log d

p

(cid:33)

dr log d

p

· (µr)3(κ(cid:63)σ(cid:63)
1)2

(cid:107)M − M(cid:63)(cid:107)F

d2
F − 0.1σ(cid:63)

r(cid:107)∆(cid:107)2

F

(cid:107)M − M(cid:63)(cid:107)F ≥ 0.95(cid:107)M − M(cid:63)(cid:107)2

∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63))
≤(cid:107)∆∆(cid:62)(cid:107)2
≤ − 0.85(cid:107)M − M(cid:63)(cid:107)2

F − 3(0.95(cid:107)M − M(cid:63)(cid:107)2
F ≤ −0.3σ(cid:63)

F − 0.1σ(cid:63)
r(cid:107)∆(cid:107)2

F + 0.4σ(cid:63)

F + 0.1σ(cid:63)

r(cid:107)∆(cid:107)2

r(cid:107)∆(cid:107)2

F

r(cid:107)∆(cid:107)2
F)

where the last step is by Lemma 6. This ﬁnishes the proof.

Finally, as in step 3 of our framework, we need to bound the contribution from the regularizer to Equation (9).

Lemma 11. By choosing α2 = Θ( µrσ(cid:63)

1

d ) and λα2 ≤ O(σ(cid:63)

r ), we have:

[∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)] ≤ 0.1σ(cid:63)

r(cid:107)∆(cid:107)2

F

1
4

19

Proof. By Lemma 18, the contribution from the regularizer to Equation (9) can be calculated as follows:

[∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)] =λ

1
4

d(cid:88)

i=1

+ 3λ

− 4λ

+

((cid:107)e(cid:62)

i U(cid:107) − α)3
d(cid:88)
d(cid:88)

((cid:107)e(cid:62)

i=1

i U(cid:107) − α)2

+

((cid:107)e(cid:62)

i U(cid:107) − α)3

+

i=1

(cid:107)e(cid:62)

i U(cid:107)2(cid:107)e(cid:62)

i U∆(cid:62)ei)2

(cid:18) e(cid:62)

i ∆(cid:107)2 − (e(cid:62)
(cid:19)2
i U(cid:107)3
(cid:107)e(cid:62)
i U∆(cid:62)ei
i U(cid:107)
(cid:107)e(cid:62)
i U∆(cid:62)ei
e(cid:62)
i U(cid:107)
(cid:107)e(cid:62)
i U(cid:63)R(cid:107) = (cid:107)e(cid:62)

i U(cid:63)(cid:107) ≤ (cid:112)µ r

Denote three terms in RHS to be A1, A2, A3. Since (cid:107)e(cid:62)

i U − e(cid:62)

i ∆(cid:107) = (cid:107)e(cid:62)

1 for some large constant C. Thus, when (cid:107)e(cid:62)

i U(cid:107) − α > 0, we have e(cid:62)

i U ≈ e(cid:62)

d σ(cid:63)

d σ(cid:63)

1. By
i ∆, strictly

e(cid:62)
i U∆(cid:62)ei = e(cid:62)

i U(U − U(cid:63)R)(cid:62)ei ≥ (cid:107)e(cid:62)

i U(cid:107)2 − (cid:107)e(cid:62)

i U(cid:107)(cid:107)e(cid:62)

i U(cid:63)(cid:107) ≥ (1 − 1
C

)(cid:107)e(cid:62)

i U(cid:107)2

choosing α > C(cid:112)µ r

speaking:

and

i ∆(cid:107) ≤ (cid:107)e(cid:62)

i U(cid:107)((cid:107)e(cid:62)

i U(cid:63)(cid:107)) ≤ (1 +

)(cid:107)e(cid:62)

i U(cid:107)2

1
C

(cid:107)e(cid:62)

i U(cid:107)(cid:107)e(cid:62)
d(cid:88)

i=1

i U(cid:107) + (cid:107)e(cid:62)
(cid:18)

(1 +

(cid:19)

Now we bound the summation A1 + A2 + A3 by seperately bounding A1 + 0.1A3 and A2 + 0.9A3. First, we have:

A1 + 0.1A3 ≤ λ

((cid:107)e(cid:62)

i U(cid:107) − α)3

+(cid:107)e(cid:62)

i U(cid:107)

1
C

)2 − (1 − 1
C

)2 − 0.4(1 − 1
C

)

< 0

+

i=1

((cid:107)e(cid:62)

d(cid:88)

(cid:20) e(cid:62)

A2 + 0.9A3 = 3λ

i U(cid:107) − α)2

Then, the remaining part is:

i U∆(cid:62)ei
e(cid:62)
(cid:107)e(cid:62)
i U(cid:107)

i U∆(cid:62)ei
i U(cid:107) − 1.2((cid:107)e(cid:62)
(cid:107)e(cid:62)

(cid:21)
3 , and decompose this term as A2 + 0.9A3 =(cid:80)d
(cid:20)
i U(cid:107) ≥ 9α, and C ≥ 100, we have:
e(cid:62)
i U∆(cid:62)ei
3 ≤ 3λ((cid:107)e(cid:62)
(cid:107)e(cid:62)
i U(cid:107)
i U(cid:107) − α)2
This is because in the above product, we have ((cid:107)e(cid:62)

We further denote i-th summand in RHS as A(i)
0.9A(i)
3 ).
Case 1: for i such that (cid:107)e(cid:62)

1
C
+ > 0, e(cid:62)

i U(cid:107) − 1.2((cid:107)e(cid:62)

i U(cid:107) − α)+

i U(cid:107) − α)+

i U(cid:107) − α)2

2 + 0.9A(i)

2 + 0.9A(i)

i U∆(cid:62)ei
i U(cid:107) ≥ (1 − 1
(cid:107)e(cid:62)

C )(cid:107)e(cid:62)

)(cid:107)e(cid:62)

≤ 0

A(i)

(cid:21)

(1 +

+

(cid:2)(1 + 1

C )(cid:107)e(cid:62)

i U(cid:107) − 1.2((cid:107)e(cid:62)

i U(cid:107) − α)+

Case 2: for i such that α < (cid:107)e(cid:62)

i U(cid:107) < 9α, we call this set I = {i | α < (cid:107)e(cid:62)
3 ≤ 3 · 104 × λ|I|α4

2 + 0.9A(i)

A(i)

i U(cid:107) < 9α}:

(cid:3) ≤ 0.
(cid:88)

i∈I

i=1(A(i)

2 +

i U(cid:107) ≥ 0 and

In sum, this proves there exists some large constant c2 so the regularization term:
[∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)] ≤ c2λ|I|α4

1
4

20

Finally, by the property of set I:

Therefore, as long as λα2 ≤ σ(cid:63)
have

r(cid:107)∆(cid:107)2
σ(cid:63)

F ≥ σ(cid:63)

r

(cid:88)

i∈I

(cid:107)e(cid:62)

i ∆(cid:107)2 ≥ σ(cid:63)

r|I|α2

r /c3 for some large absolute constant c3 (which is satisﬁed by our choice of λ), we
[∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)] ≤ 0.1σ(cid:63)

r(cid:107)∆(cid:107)2

F

Combining these lemmas, we are now ready to prove the main theorem for symmetric matrix completion.

Theorem 12. When sample rate p ≥ Ω( µ3r4(κ(cid:63))4 log d
µrκ(cid:63) ). Then with
probability at least 1−1/poly(d), for matrix completion objective (11) we have 1) all local minima satisfy UU(cid:62) = M(cid:63)
2) the function is (, Ω(σ(cid:63)

))-strict saddle for polynomially small .

), by choosing α2 = Θ( µrσ(cid:63)

d ) and λ = Θ( d

d

1

r ), O( 
σ(cid:63)
r

Proof. By Lemma 10, we know

∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63)) ≤ −0.3σ(cid:63)

r(cid:107)∆(cid:107)2

F

On the other hand, by Lemma 11, we have the regularization term:

[∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)] ≤ 0.1σ(cid:63)

r(cid:107)∆(cid:107)2

F

This means for point U with small gradient satisfying (cid:107)∇f (U)(cid:107)F ≤ :

∆ : ∇2f (U) : ∆ ≤ −0.2σ(cid:63)
if (cid:107)∆(cid:107)F ≥ 40

r(cid:107)∆(cid:107)2

F + 4(cid:107)∆(cid:107)F

That is, if U is close to U(cid:63) (i.e.
(, 0.1σ(cid:63)
This means all local minima are global minima (satisfying UU(cid:62) = M(cid:63)), which ﬁnishes the proof.

F. This proves
)-strict saddle property. Take  = 0, we know all stationary points with (cid:107)∆(cid:107)F (cid:54)= 0 are saddle points.

), we have ∆ : ∇2f (U) : ∆ ≤ −0.1σ(cid:63)

r(cid:107)∆(cid:107)2

r , 40
σ(cid:63)
r

σ(cid:63)
r

A.2 Robust PCA
For robust PCA, the ﬁrst crucial step is to analyze the matrix factorization problem when target matrix is not necessarily
low rank (that happens if we ﬁx S).
Lemma 13. Let A ∈ Rd×d be a symmetric PSD matrix, and matrix factorization objective to be:

f (U) = (cid:107)UU(cid:62) − A(cid:107)2

F

then 1) all local minima satisﬁes UU(cid:62) = Pr(A) (best rank-r approximation), 2)

where σr(A) ≥ 15σr+1(A).
objective is (, Ω(σ(cid:63)
Proof. Denote M(cid:63) = Pr(A) to be the top r part and S = A − M(cid:63) to be the remaining part. In our framework, we
can also view this remaining part as regularization term. That is:

))-strict saddle.

r ), O( 
σ(cid:63)
r

M : H : M = (cid:107)M(cid:107)2

F

and Q(U) = (cid:104)M(cid:63) − UU(cid:62), S(cid:105) +

(cid:107)S(cid:107)2

F

1
2

Moreover, since the eigenspace of M(cid:63) is perpendicular to S, we have:

[∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)] = −2(cid:104)∆∆(cid:62), S(cid:105) + 4(cid:104)U∆ + ∆U, S(cid:105) = 6(cid:104)UU(cid:62), S(cid:105) ≤ 6(cid:107)S(cid:107)(cid:107)∆(cid:107)2

F

The last step is because suppose XDX(cid:62) is the SVD of S, then (cid:104)UU(cid:62), S(cid:105) ≤ (cid:107)D(cid:107)(cid:107)X(cid:62)U(cid:107)2
(cid:107)S(cid:107)(cid:107)∆(cid:107)2
F.

F = (cid:107)D(cid:107)(cid:107)X(cid:62)(U − U(cid:63))(cid:107)2

F ≤

21

Therefore, for point U with small gradient satisfying (cid:107)∇f (U)(cid:107)F ≤ :

∆ : ∇2f (U) : ∆ =∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63)) + 4(cid:104)∇f (U), ∆(cid:105)

+ [∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)]

≤(cid:107)∆∆(cid:62)(cid:107)2
≤ − (cid:107)M − M(cid:63)(cid:107)2

F − 3(cid:107)M − M(cid:63)(cid:107)2

F + 4(cid:107)∆(cid:107)F + 6(cid:107)S(cid:107)(cid:107)∆(cid:107)2
F ≤ −0.4σ(cid:63)

F + 4(cid:107)∆(cid:107)F + 6(cid:107)S(cid:107)(cid:107)∆(cid:107)2

F

r(cid:107)∆(cid:107)2

F + 4(cid:107)∆(cid:107)F

The second last inequality is due to Lemma 40 that (cid:107)∆∆(cid:62)(cid:107)2
F, and last inequality is due to Lemma
41 and (cid:107)S(cid:107) = λr+1(A) ≤ λr(A)/15. This means if U is close to U(cid:63), that is, if (cid:107)∆(cid:107)F ≥ 20
, we have ∆ : ∇2f (U) :
∆ ≤ −0.2σ(cid:63)
)-strict saddle property. Take  = 0, we know all stationary points with
(cid:107)∆(cid:107)F (cid:54)= 0 are saddle points. This means all local minima are global minima (satisfying UU(cid:62) = M(cid:63)), which ﬁnishes
the proof.

F ≤ 2(cid:107)M − M(cid:63)(cid:107)2

F. This proves (, 0.2σ(cid:63)

r(cid:107)∆(cid:107)2

r , 20
σ(cid:63)
r

σ(cid:63)
r

Next we need to show that if U is close to the best rank-r approximation of M(cid:63) + S(cid:63) − S, then it also must be
close to the true U(cid:63). The proofs of this lemma for symmetric robust PCA is almost directly followed by the arguments
for asymmetric versions. Therefore we do not repeat the proofs here.
Lemma 14. There is an absolute constant c, assume γ > c, and γα· µr· (κ(cid:63))5 ≤ 1
c . Let U†(U†)(cid:62) be the best rank r-
approximation of M(cid:63) + S(cid:63) − S, where S is the minimizer as in Eq.(12). Assume minR(cid:62)R=RR(cid:62)=I (cid:107)U − U†R(cid:107)F ≤ .
Let ∆ be deﬁned as in Deﬁnition 6, then (cid:107)∆(cid:107)F ≤ O(
Proof. The proof follows from the same argument as the proof of Lemma 24.

κ(cid:63)) for polynomially small .

√

Combining these two lemmas, it is not hard to show to main result for Robust PCA.

Theorem 15. There is an absolute constant c, if γ > c, and γα · µr · (κ(cid:63))5 ≤ 1
we have 1) all local minima satisﬁes UU(cid:62) = M(cid:63); 2) objective function is (, Ω(σ(cid:63)
for polynomially small .

c holds, for objective function Eq.(12)
))-pseudo strict saddle

√
r ), O( 
σ(cid:63)
r

κ(cid:63)

Proof. Recall objective function:

f (U) =

1
2

min
S∈Sγα

(cid:107)UU(cid:62) + S − M(cid:63) − S(cid:63)(cid:107)2

F

Consider point U with small gradient satisfying (cid:107)∇f (U)(cid:107)F ≤ . Let

SU = argmin
S∈Sγα

(cid:107)UU(cid:62) + S − M(cid:63) − S(cid:63)(cid:107)2

F

and function fU( ˜U) = (cid:107) ˜U ˜U(cid:62) + SU − M(cid:63) − S(cid:63)(cid:107)2
fU(U) = f (U). Since fU( ˜U) is matrix factorization objective where by Lemma 28:

F , then, we know for all ˜U, we have fU( ˜U) ≥ f ( ˜U) and

µrσ(cid:63)
(cid:107)S(cid:63) − SU(cid:107) ≤ 2γα · 2
1
d
σr(M(cid:63) + S(cid:63) − SU) ≥ σ(cid:63)
r − (cid:107)S(cid:63) − SU(cid:107) ≥ 0.99σ(cid:63)
σr+1(M(cid:63) + S(cid:63) − SU) ≤ (cid:107)S(cid:63) − SU(cid:107) ≤ 0.01σ(cid:63)

≤ 0.01σ(cid:63)

r

r

r

This gives σr(M(cid:63) + S(cid:63) − SU) ≥ 15σr+1(M(cid:63) + S(cid:63) − SU). Given (cid:107)∇fU(U)(cid:107)F = (cid:107)∇f (U)(cid:107)F ≤ , by Lemma
23, we know either λmin(∇2fU(U)) ≤ −0.2σ(cid:63)
where U†(U†)(cid:62) is the
√
best rank r-approximation of M(cid:63) + S(cid:63) − SU. By Lemma 24, we immediately have (cid:107)∆(cid:107)F ≤ 103
, which proves
σ(cid:63)
r
(, Ω(σ(cid:63)

))-pseudo strict saddle. By taking  = 0, we proved all local minima satisﬁes UU(cid:62) = M(cid:63).

r or minR(cid:62)R=RR(cid:62)=I (cid:107)U − U†R(cid:107)F ≤ 20

σ(cid:63)
r

κ(cid:63)

κ(cid:63)

r ), O( 

√
σ(cid:63)
r

22

B Proofs for Asymmetric Problems

In this section we give proofs for the asymmetric settings. In particular, we ﬁrst prove the main lemma, which gives
the crucial reduction from asymmetric case to symmetric case.
Lemma 16. For the objective (15), let ∆, N be deﬁned as in Deﬁnition 7. Then, for any W ∈ R(d1+d2)×r, we have

∆ : ∇2f (W) : ∆ ≤∆∆(cid:62) : H : ∆∆(cid:62) − 3(N − N(cid:63)) : H : (N − N(cid:63))

+ 4(cid:104)∇f (W), ∆(cid:105) + [∆ : ∇2Q(W) : ∆ − 4(cid:104)∇Q(W), ∆(cid:105)]

Further, if H0 satisﬁes M : H0 : M ∈ (1 ± δ)(cid:107)M(cid:107)2
(14), then N : H : N ∈ (1 ± 2δ)(cid:107)N(cid:107)2
F .
Proof. Recall the objective function is (N = WW(cid:62)):

F for some matrix M = UV(cid:62), let W and N be deﬁned as in

f (W) =

1
2

[(N − N(cid:63)) : 4H1 : (N − N(cid:63)) + N : G : N] + Q(W)

Calculating gradient and Hessian, we have for any Z ∈ Rd×r:

(cid:104)∇f (W), Z(cid:105) =(N − N(cid:63)) : 4H1 : (WZ(cid:62) + ZW(cid:62)) + N : G : (WZ(cid:62) + ZW(cid:62)) + (cid:104)∇Q(W), Z(cid:105)

Z : ∇2f (W) : Z =(WZ(cid:62) + ZW(cid:62)) : (4H1 + G) : (WZ(cid:62) + ZW(cid:62)) + 2(N − N(cid:63)) : 4H1 : ZZ(cid:62)

+ 2N : G : ∆∆(cid:62) + Z : ∇2Q(W) : Z

Let Z = ∆ = W − W(cid:63)R as in Deﬁnition 7, and note N − N(cid:63) + ∆∆(cid:62) = W∆(cid:62) + ∆W(cid:62) and N(cid:63) : G : N(cid:63) = N(cid:63) :
G : W(cid:63)W(cid:62) = 0 due to U(cid:63)(cid:62)U(cid:63) = V(cid:63)(cid:62)V(cid:63). Let H = 4H1 + G, then

(cid:104)∇f (W), ∆(cid:105) =(N − N(cid:63)) : H : (W∆(cid:62) + ∆W(cid:62)) + N(cid:63) : G : (W∆(cid:62) + ∆W(cid:62)) + (cid:104)∇Q(W), ∆(cid:105)

=(N − N(cid:63)) : H : (W∆(cid:62) + ∆W(cid:62)) + 2N(cid:63) : G : N + (cid:104)∇Q(W), ∆(cid:105)

(21)

Where the last equality is use the fact N(cid:63) : G : W(cid:63)W(cid:62) = N(cid:63) : G : WW(cid:63)(cid:62) = 0. For Hessian along ∆ direction:

∆ : ∇2f (W) : ∆ =(W∆(cid:62) + ∆W(cid:62)) : H : (W∆(cid:62) + ∆W(cid:62)) + 2(N − N(cid:63)) : 4H1 : ∆∆(cid:62)

(22)
For ﬁrst term of Eq.(22): since ∆∆(cid:62) + (N− N(cid:63)) = W∆(cid:62) + ∆W(cid:62) and (a + b)2 = a2 + 2b(a + b)− b2 and Eq.(21),
we have:

+ 2N : G : ∆∆(cid:62) + ∆ : ∇2Q(W) : ∆

(W∆(cid:62) + ∆W(cid:62)) : H : (W∆(cid:62) + ∆W(cid:62))
=∆∆(cid:62) : H : ∆∆(cid:62) + 2(N − N(cid:63)) : H : (W∆(cid:62) + ∆W(cid:62)) − (N − N(cid:63)) : H : (N − N(cid:63))
=∆∆(cid:62) : H : ∆∆(cid:62) − (N − N(cid:63)) : H : (N − N(cid:63)) + 2(cid:104)∇f (W), ∆(cid:105) − 4N(cid:63) : G : N − 2(cid:104)∇Q(W), ∆(cid:105)

For the sum of second and third terms of Eq.(22):

2(N − N(cid:63)) : 4H1 : ∆∆(cid:62) + 2N : G : ∆∆(cid:62)
=2(N − N(cid:63)) : H : ∆∆(cid:62) + 2N(cid:63) : G : N
= − 2(N − N(cid:63)) : H : (N − N(cid:63)) + 2(N − N(cid:63)) : H : (W∆(cid:62) + ∆W(cid:62)) + 2N(cid:63) : G : N
= − 2(N − N(cid:63)) : H : (N − N(cid:63)) + 2(cid:104)∇f (W), ∆(cid:105) − 2N(cid:63) : G : N − 2(cid:104)∇Q(W), ∆(cid:105)

In sum, we have:

∆ : ∇2f (W) : ∆ =∆∆(cid:62) : H : ∆∆(cid:62) − 3(N − N(cid:63)) : H : (N − N(cid:63)) − 6N(cid:63) : G : N

+ 4(cid:104)∇f (W), ∆(cid:105) + [∆ : ∇2Q(W) : ∆ − 4(cid:104)∇Q(W), ∆(cid:105)]

23

cleary the third term is always non-positive. this gives:

∆ : ∇2f (W) : ∆ ≤∆∆(cid:62) : H : ∆∆(cid:62) − 3(N − N(cid:63)) : H : (N − N(cid:63)) + 4(cid:104)∇f (W), ∆(cid:105)

+ [∆ : ∇2Q(W) : ∆ − 4(cid:104)∇Q(W), ∆(cid:105)]

(cid:18) A0 Ac

(cid:19)

A(cid:62)

c A1

The remaining claims directly follows from Lemma 19.

Lemma 19. Let A =

∈ R(d1+d2)×(d1+d2) be a symmetric matrix, if for H0 we have:

(1 − δ)(cid:107)Ac(cid:107)2

F ≤ Ac : H0 : Ac ≤ (1 + δ)(cid:107)Ac(cid:107)2

F

Then, we have:

Proof. By calculation,

(1 − 2δ)(cid:107)A(cid:107)2

F ≤ A : H : A ≤ (1 + 2δ)(cid:107)A(cid:107)2

F

A : H : A = 4Ac : H0 : Ac +(cid:0)(cid:107)A1(cid:107)2

F + (cid:107)A2(cid:107)2

F − 2(cid:107)Ac(cid:107)2

F )(cid:1)

The lemma easily follows.

In the remainder of this section we prove the main theorems for matrix completion and robust PCA.

B.1 Matrix Completion
Across this section, we denote

Q1(U) = λ1

d1(cid:88)

i=1

((cid:107)e(cid:62)

i U(cid:107) − α1)4

+ and Q2(V) = λ2

d2(cid:88)

((cid:107)e(cid:62)

j V(cid:107) − α2)4

+

j=1

and clearly Q(W) = Q1(U) + Q2(V). We always denote d = max{d1, d2}

We proceed in three steps analogous to the symmetric setting. First we show the regularizer again implies rows of

U, V cannot be too large (similar to Lemma 9).
Lemma 20. Let d = max{d1, d2}, there is an absolute constant c, when sample rate p ≥ Ω( µr log d
1 = Θ( µrσ(cid:63)
µrκ(cid:63) ), λ2 = Θ( d2
α2
polynomially small . with probability at least 1 − 1/poly(d):

min{d1,d2} ), and
µrκ(cid:63) ), we have for any points W with (cid:107)∇f (W)(cid:107)F ≤  with

2 = Θ( µrσ(cid:63)

), λ1 = Θ( d1

), α2

d1

d2

1

1

(cid:18) µ2r2.5(κ(cid:63))2σ(cid:63)

(cid:19)

1

d1

(cid:18)(M − M(cid:63))ΩV

(M − M(cid:63))(cid:62)

Ω U

(cid:19)

4
p

+

and max

(cid:107)e(cid:62)

j V(cid:107)2 ≤ O

j

√

d1 maxi (cid:107)e(cid:62)

(cid:18)U(U(cid:62)U − V(cid:62)V)

V(V(cid:62)V − U(cid:62)U)

i U(cid:107) ≥ √
(cid:19)

+ ∇Q(W)

(cid:18) µ2r2.5(κ(cid:63))2σ(cid:63)

(cid:19)

1

d2

d2 maxj (cid:107)e(cid:62)

j V(cid:107). We know

(cid:107)e(cid:62)

i U(cid:107)2 ≤ O

max

i

Proof. In this proof, by symmetry, W.L.O.G, we can assume
gradient can be calculated as:

∇f (W) =

Where:

∇Q(W) = 4λ1

d1(cid:88)

i=1

d2(cid:88)

i=d1+1

((cid:107)e(cid:62)

i W(cid:107) − α2)3

+

eie(cid:62)
i W
(cid:107)eiW(cid:107)2

((cid:107)e(cid:62)

i W(cid:107) − α1)3

+

i W

eie(cid:62)
(cid:107)eiW(cid:107)2 + 4λ2

24

Clearly, we have (cid:104)∇Q(W), W(cid:105) ≥ 0, therefore, for any points W with small gradient (cid:107)∇f (W)(cid:107)F ≤ , we have:

(cid:107)W(cid:107)F ≥(cid:104)∇f (W), W(cid:105)

(cid:104)(M − M(cid:63))Ω, M(cid:105) + (cid:104)∇Q(W), W(cid:105)
4
F +
p
(cid:104)(M(cid:63))Ω, (M)Ω(cid:105)
F − 4
p
(cid:112)
(cid:107)M(cid:107)Ω
(cid:107)M(cid:63)(cid:107)Ω · 1√
F − 4 · 1√
p
p
d1d2)(cid:107)M(cid:63)(cid:107)F(cid:107)M(cid:107)∞
F − O(
i U(cid:107), and j(cid:63) = argmaxj (cid:107)e(cid:62)
where last inequality is by Lemma 35 and Lemma 39. Let i(cid:63) = argmaxi (cid:107)e(cid:62)
j(cid:63) V(cid:107) and due to (cid:107)M(cid:63)(cid:107)F ≤ √
1 and (cid:107)M(cid:107)∞ ≤ (cid:107)e(cid:62)
i(cid:63) U(cid:107)(cid:107)e(cid:62)
assumption, we know
rσ(cid:63)
gives:
F ≤ O(d1σ(cid:63)
i(cid:63)W(cid:105):

=(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2
≥(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2
≥(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2
≥(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2
i(cid:63) U(cid:107) ≥ √
d2(cid:107)e(cid:62)
(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2

i(cid:63) U(cid:107)2 + O(d)(cid:107)e(cid:62)

i(cid:63)∇f (W), e(cid:62)

d1(cid:107)e(cid:62)

r)(cid:107)e(cid:62)

i(cid:63) U(cid:107)

√

√

1

j V(cid:107). By
j(cid:63) V(cid:107), this

(23)

In case (cid:107)e(cid:62)
(cid:107)e(cid:62)

i(cid:63) U(cid:107) ≥ 2αi, consider (cid:104)e(cid:62)
i(cid:63) U(cid:107) ≤(cid:104)e(cid:62)
i(cid:63)W(cid:105)
=(cid:104)e(cid:62)

i(cid:63)∇f (W), e(cid:62)

(cid:20) 4

(cid:21)

i(cid:63)

, e(cid:62)

i(cid:63) U(cid:105)

+(cid:107)e(cid:62)
(cid:107)e(cid:62)

(M − M(cid:63))ΩV + U(U(cid:62)U − V(cid:62)V) + ∇Q1(U)
p
i(cid:63) U(cid:107) − α1)3
≥4λ1((cid:107)e(cid:62)
i(cid:63) U(cid:107)4 − 4
(cid:107)e(cid:62)
≥ λ1
1√
2
p
i(cid:63) U(cid:107)4 − O(1)(cid:107)e(cid:62)
(cid:107)e(cid:62)
≥ λ1
2
i(cid:63) U(cid:107)4 − √
(cid:107)e(cid:62)
≥ λ1
1(cid:107)e(cid:62)
2

i(cid:63) U(cid:107) − 4
(cid:104)e(cid:62)
i(cid:63) (M(cid:63))Ω, e(cid:62)
p
i(cid:63) (M(cid:63))Ω(cid:107) · 1√
p

i(cid:63) U(cid:107)2 − (cid:107)U(cid:62)U − V(cid:62)V(cid:107)F(cid:107)e(cid:62)

i(cid:63)M(cid:63)(cid:107) ·(cid:112)

µrσ(cid:63)

i(cid:63) (M)Ω(cid:105) − (cid:107)U(cid:62)U − V(cid:62)V(cid:107)F(cid:107)e(cid:62)
i(cid:63) U(cid:107)2

(cid:107)e(cid:62)

i(cid:63) (M)Ω(cid:107) − (cid:107)U(cid:62)U − V(cid:62)V(cid:107)F(cid:107)e(cid:62)
i(cid:63) U(cid:107)2

d2(cid:107)M(cid:107)∞ − (cid:107)U(cid:62)U − V(cid:62)V(cid:107)F(cid:107)e(cid:62)

i(cid:63) U(cid:107)2

i(cid:63) U(cid:107)2

i(cid:63) U(cid:107) + O((cid:112)dσ(cid:63)

where second last inequality is by Lemma 35 and Lemma 39. Substitute in Eq.(23), we have:

λ1(cid:107)e(cid:62)

√
i(cid:63) U(cid:107)3 ≤ O(

µrσ(cid:63)

1)(cid:107)e(cid:62)

by choosing  to be polynomially small, we have:
j V(cid:107) ≤ max

(cid:107)e(cid:62)

max

(cid:114) d2

d1

j

1 · r

1

4 )(cid:107)e(cid:62)

√
i(cid:63) U(cid:107)2 +  + O(

d)(cid:107)e(cid:62)

(cid:26)

√

µr · σ(cid:63)
λ1

1

,

√

d1σ(cid:63)
1
λ2
1

r

α2
1,

i(cid:63) U(cid:107)1.5
(cid:27)

(cid:107)e(cid:62)

i U(cid:107)2 ≤ c max

i

Finally, substituting our choice of α2 and λ, we ﬁnished the proof.

Next we show the Hessian H related terms in Eq.(16) is negative when W (cid:54)= W(cid:63). This is analogous to Lemma 10.

Lemma 21. Let d = max{d1, d2}, when sample rate p ≥ Ω( µ4r6(κ(cid:63))6 log d
Θ( µrσ(cid:63)
d2
(cid:107)∇f (W)(cid:107)F ≤  for polynomially small :

2 =
µrκ(cid:63) ). Then with probability at least 1 − 1/poly(d), for all W with

min{d1,d2} ), by choosing α2

µrκ(cid:63) ), λ2 = Θ( d2

) and λ1 = Θ( d1

1 = Θ( µrσ(cid:63)

), α2

d1

1

1

∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63)) ≤ −0.3σ(cid:63)

r(cid:107)∆(cid:107)2

F

Proof. Again the idea is similar, we divide into cases according to the norm of ∆ and use different concentration
inequalities. By our choice of α, λ and Lemma 20, we known when  is polynomially small, with high probability:

(cid:18) µ2r2.5(κ(cid:63))2σ(cid:63)

(cid:19)

1

d1

(cid:107)e(cid:62)

i U(cid:107)2 ≤ O

max

i

(cid:18) µ2r2.5(κ(cid:63))2σ(cid:63)

(cid:19)

1

d2

and max

j

(cid:107)e(cid:62)

j V(cid:107)2 ≤ O

25

In this proof, we denote ∆ = (∆(cid:62)
Case 1: (cid:107)∆(cid:107)2

F ≤ σ(cid:63)

r /40. By Lemma 35 and Lemma 19, we know:

U, ∆(cid:62)

V)(cid:62), clearly, we have (cid:107)∆U(cid:107)F ≤ (cid:107)∆(cid:107)F and (cid:107)∆V(cid:107)F ≤ (cid:107)∆(cid:107)F.

W(cid:63)∆(cid:62) : H : W(cid:63)∆(cid:62) ≥ (1 − 2δ)(cid:107)W(cid:63)∆(cid:62)(cid:107)2

F ≥ (1 − 2δ)σ(cid:63)

r(cid:107)∆(cid:107)2

F

On the other hand, by Lemma 37 and our choice of p, we have:

(cid:115)

(cid:107)∆U∆(cid:62)

V(cid:107)2

Ω ≤(1 + δ)(cid:107)∆U(cid:107)2

F(cid:107)∆V(cid:107)2

F + O(

1
p

d
p

· µ2r2.5(κ(cid:63))2σ(cid:63)

√

1

)(cid:107)∆U(cid:107)F(cid:107)∆V(cid:107)F

Thus by (cid:107)∆U(cid:107)2

≤(1 + δ)(cid:107)∆(cid:107)4
(cid:107)∆(cid:107)2
F ≤ σ(cid:63)
2
F +
r (
9
F ≤ σ(cid:63)
F ≤ (cid:107)∆(cid:107)2
r /40 and (cid:107)∆V(cid:107)2
F ≤ σ(cid:63)
r /40,
∆∆(cid:62) : H : ∆∆(cid:62) =
U(cid:107)2
F + (cid:107)∆V∆(cid:62)
4
p

Ω +(cid:0)(cid:107)∆U∆(cid:62)

F ≤ (cid:107)∆(cid:107)2

(cid:107)∆U∆(cid:62)

V(cid:107)2

σ(cid:63)
r
4

+

δ
60

d1d2
)(cid:107)∆(cid:107)2

F ≤ σ(cid:63)

r
20

(cid:107)∆(cid:107)2

F

V(cid:107)2

F − 2(cid:107)∆U∆(cid:62)

V(cid:107)2

r(cid:107)∆(cid:107)2
σ(cid:63)

F

F )(cid:1) ≤ 1

4

This gives:

∆∆(cid:62) : H : ∆∆(cid:62) − 3(N − N(cid:63)) : H : (N − N(cid:63))
=∆∆(cid:62) : H : ∆∆(cid:62) − 3(W(cid:63)∆(cid:62) + ∆W(cid:63)(cid:62) + ∆∆(cid:62)) : H : (W(cid:63)∆(cid:62) + ∆W(cid:63)(cid:62) + ∆∆(cid:62))
≤ − 12(W(cid:63)∆(cid:62) : H : ∆∆(cid:62) + W(cid:63)∆(cid:62) : H : W(cid:63)∆(cid:62))
√
≤ − 12
√
p
≤ − 12

√
W(cid:63)∆(cid:62) : H : W(cid:63)∆(cid:62)(
√
1 − 2δ(

1 − 2δ −(cid:112)1/4)σ(cid:63)

∆∆(cid:62) : H : ∆∆(cid:62))

√

The last inequality is by choosing large enough p, we have small δ.
Case 2: (cid:107)∆(cid:107)2

F ≥ σ(cid:63)
(cid:107)∆U∆(cid:62)
V(cid:107)2

1
p

r /40, by Lemma 38 with high probability, our choice of p gives:
Ω ≤(cid:107)∆U∆(cid:62)

(cid:107)∆U∆(cid:62)

V(cid:107)2∞ +

V(cid:107)2

dr log d

dr log d

F + O

(cid:107)∆U∆(cid:62)

V(cid:107)F(cid:107)∆U∆(cid:62)

V(cid:107)∞

r(cid:107)∆(cid:107)2

W(cid:63)∆(cid:62) : H : W(cid:63)∆(cid:62) −
F ≤ −1.2σ(cid:63)
r(cid:107)∆(cid:107)2
(cid:115)

F

d1d2
F ≤ (cid:107)∆U∆(cid:62)
(cid:115)

p

(cid:115)

(cid:115)

p

p

p

+

(cid:32)
(cid:32)

(cid:32)
(cid:32)

(cid:33)

(cid:33)

(cid:33)

(cid:33)

≤(cid:107)∆U∆(cid:62)

V(cid:107)2

F + O

dr log d

1)2
· µ4r5(κ(cid:63))4(σ(cid:63)

+

dr log d

p

· µ4r5(κ(cid:63))4(σ(cid:63)
1)2

d1d2

(cid:107)∆(cid:107)2

F

≤(cid:107)∆U∆(cid:62)

V(cid:107)2

F +

r )2
(σ(cid:63)
1000

σ(cid:63)
r
1000

(cid:107)∆(cid:107)2

V(cid:107)2

F + 0.01σ(cid:63)

r(cid:107)∆(cid:107)2

F

Again by Lemma 38 with high probability

(cid:107)M − M(cid:63)(cid:107)2

Ω ≥(cid:107)M − M(cid:63)(cid:107)2

F − O

1
p

≥(cid:107)M − M(cid:63)(cid:107)2

F − O

dr log d

p

dr log d

r )2
F − (σ(cid:63)
1000
Then by simple calculation, this gives:

≥(cid:107)M − M(cid:63)(cid:107)2

(cid:107)M − M(cid:63)(cid:107)2∞ +

dr log d

(cid:107)M − M(cid:63)(cid:107)F(cid:107)M − M(cid:63)(cid:107)∞

1)2
· µ4r5(κ(cid:63))4(σ(cid:63)

+

d1d2

dr log d

p

· µ4r5(κ(cid:63))4(σ(cid:63)
1)2

(cid:107)M − M(cid:63)(cid:107)F

(cid:107)M − M(cid:63)(cid:107)F ≥ 0.99(cid:107)M − M(cid:63)(cid:107)2

d1d2
F − 0.01σ(cid:63)

r(cid:107)∆(cid:107)2

F

p
− σ(cid:63)
1000

r

r(cid:107)∆(cid:107)2
F + 0.12σ(cid:63)
where the last step is by Lemma 6. This ﬁnishes the proof.

∆∆(cid:62) : H : ∆∆(cid:62) − 3(N − N(cid:63)) : H : (N − N(cid:63))
≤(cid:107)∆∆(cid:62)(cid:107)2
F + 0.04σ(cid:63)
≤ − 0.94(cid:107)N − N(cid:63)(cid:107)2

F − 3(0.98(cid:107)N − N(cid:63)(cid:107)2
F ≤ −0.3σ(cid:63)

r(cid:107)∆(cid:107)2

r(cid:107)∆(cid:107)2
F)

F − 0.04σ(cid:63)
r(cid:107)∆(cid:107)2

F

26

Finally we bound the contribution from regularizer (analogous to Lemma 11). In fact since our regularizers are

very similar we can directly use the same calculation.

Lemma 22. By choosing α2

1 = Θ( µrσ(cid:63)

1

d1

), α2

1 = Θ( µrσ(cid:63)

1

d1

), λ1α2

1 ≤ O(σ(cid:63)

2 ≤ O(σ(cid:63)

r ), we have:

[∆ : ∇2Q(W) : ∆ − 4(cid:104)∇Q(W), ∆(cid:105)] ≤ 0.1σ(cid:63)

1
4

r ) and λ2α2
r(cid:107)∆(cid:107)2

F

Proof. By same calculation as the proof of Lemma 11, we can show:

[∆U : ∇2Q1(U) : ∆U − 4(cid:104)∇Q1(U), ∆U(cid:105)] ≤ 0.1σ(cid:63)
[∆V : ∇2Q2(V) : ∆V − 4(cid:104)∇Q2(V), ∆V(cid:105)] ≤ 0.1σ(cid:63)

F

r(cid:107)∆U(cid:107)2
r(cid:107)∆V(cid:107)2

F

1
4
1
4

Given Q(W) = Q1(U) + Q2(V), the lemma follows.

Combining three lemmas, our main result for asymmetric matrix completion easily follows.

Theorem 4. Let d = max{d1, d2}, when sample rate p ≥ Ω( µ4r6(κ(cid:63))6 log d
and λ1 = Θ( d1
local minima satisfy UV(cid:62) = M(cid:63) 2) The objective is (, Ω(σ(cid:63)

2 = Θ( µrσ(cid:63)
µrκ(cid:63) ). With probability at least 1 − 1/poly(d), for Objective Function (4) we have 1) all

))-strict saddle for polynomially small .

min{d1,d2} ), choose α2

µrκ(cid:63) ), λ2 = Θ( d2

1 = Θ( µrσ(cid:63)

), α2

d1

d2

)

1

1

r ), O( 
σ(cid:63)
r

Proof. Same argument as the proof Theorem 12 by combining Lemma 20, 21 and 22.

B.2 Robust PCA
For robust PCA, again a crucial step is to analyze the matrix factorization problem. We prove the following Lemma
(analogous to Lemma 13).
Lemma 23. Let matrix factorization objective to be (U ∈ Rd1×r, V ∈ Rd2×r):

f (W) = 2(cid:107)UV(cid:62) − A(cid:107)2

F +

(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2

F

1
2

r , 20
σ(cid:63)
r

then 1) all local minima satisﬁes UV(cid:62) is the top-r SVD of matrix A; 2) objective is

and σr(A) ≥ 30σr+1(A).
)-strict saddle
(, 0.2σ(cid:63)
Proof. Denote M(cid:63) = Pr(A) to be the top-r SVD of A, and S = A− M(cid:63) to be the remaining part. In our framework,
we can also view this remaining part as regularization term. That is:

M : H0 : M = (cid:107)M(cid:107)2

F

and Q(W) = 4(cid:104)M(cid:63) − UV(cid:62), S(cid:105) + 2(cid:107)S(cid:107)2

F

Moreover, since the eigenspace of M(cid:63) is perpendicular to S, we have:

[∆ : ∇2Q(W) : ∆ − 4(cid:104)∇Q(W), ∆(cid:105)] = − 8(cid:104)∆U∆(cid:62)

V, S(cid:105) + 16(cid:104)U∆(cid:62)

V + ∆UV(cid:62), S(cid:105)

=24(cid:104)UV(cid:62), S(cid:105) ≤ 24(cid:107)S(cid:107)(cid:107)∆U(cid:107)F(cid:107)∆V(cid:107)F ≤ 12(cid:107)S(cid:107)(cid:107)∆(cid:107)2

F

The last step is because suppose XDY(cid:62) is the SVD of S, then

(cid:104)UV(cid:62), S(cid:105) ≤ (cid:107)D(cid:107)(cid:107)X(cid:62)U(cid:107)F(cid:107)Y(cid:62)V(cid:107)F = (cid:107)D(cid:107)(cid:107)X(cid:62)(U − U(cid:63))(cid:107)F(cid:107)Y(cid:62)(V − V(cid:63))(cid:107)F ≤ (cid:107)S(cid:107)(cid:107)∆U(cid:107)F(cid:107)∆V(cid:107)F

Using Lemma 19, the remaining argument is the same as Lemma 13.

Next we prove when W is close to the optimal solution of the matrix factorization problem, it must also be close
to the true W(cid:63). The proof of this lemma uses several crucial properties in choice of S and the sparse set Sγα. This will
require several supporting lemmas which we prove after the main theorem. Our proof is inspired by Yi et al. [2016].

27

c . Let X†D†Y†(cid:62) be the best rank
F . Let U† = X†(D†) 1
2 ,
2 . Assume minR(cid:62)R=RR(cid:62)=I (cid:107)W − W†R(cid:107)F ≤ . Let ∆ be deﬁned as in Deﬁnition 7, then (cid:107)∆(cid:107)F ≤

Lemma 24. There is an absolute constant c, assume γ > c, and γα · µr · (κ(cid:63))5 ≤ 1
r-approximation of M(cid:63) + S(cid:63) − SW, where SW = argminS∈Sγα (cid:107)UV(cid:62) + S − M(cid:63) − S(cid:63)(cid:107)2
V† = Y†(D†) 1
O(
Proof. By assumption, we have (cid:107)W − W†(cid:107)F ≤ , we also have W in the neighborhood of W(cid:63).

κ(cid:63)) for polynomially small .

√

First, we know

(cid:107)N† − N(cid:107)F =(cid:107)W†W†(cid:62) − WW(cid:62)(cid:107)F ≤ ((cid:107)W†(cid:107) + (cid:107)W(cid:107))(cid:107)W − W†(cid:107)F

≤(2(cid:107)W†(cid:107) + (cid:107)W − W†(cid:107)F)(cid:107)W − W†(cid:107)F ≤ 3(cid:112)σ(cid:63)

Where the last step is due to (cid:107)U†(cid:107) = (cid:107)V†(cid:107) ≤ (cid:112)(cid:107)M(cid:63) + S(cid:63) − SW(cid:107) ≤ (cid:112)(cid:107)M(cid:63)(cid:107) + (cid:107)S(cid:63) − SW(cid:107) ≤ (cid:112)1.01σ(cid:63)

1

1,
1 by our choice of . Then by Lemma 25 and Lemma 42

≤ 0.5(cid:112)σ(cid:63)

(cid:107)W†(cid:107) ≤ (cid:107)U†(cid:107) + (cid:107)V†(cid:107) and (cid:107)U − U†(cid:107)F ≤ 20
we have:

σ(cid:63)
r

By triangle inequality, this gives:

(cid:107)N† − N(cid:63)(cid:107)F ≤ 2(cid:107)M† − M(cid:63)(cid:107)F ≤ 4(cid:107)SW − S(cid:63)(cid:107)F

(cid:107)N − N(cid:63)(cid:107)F ≤ (cid:107)N† − N(cid:107)F + (cid:107)N† − N(cid:63)(cid:107)F ≤ 4(cid:107)SW − S(cid:63)(cid:107)F + 3(cid:112)σ(cid:63)

1

On the other hand, by Lemma 27, we know matrix M† is 4µ(κ(cid:63))4-incoherent. Thus for any i ∈ [d1]:

(cid:107)e(cid:62)

i U(cid:107) ≤ (cid:107)e(cid:62)

i (U − U†)(cid:107) + (cid:107)e(cid:62)

i U†(cid:107) ≤ (cid:107)W − W†(cid:107)F + 2(κ(cid:63))2

≤ 3(κ(cid:63))2

(24)

(cid:114) 1.01µrσ(cid:63)

1

d1

(cid:114) µrσ(cid:63)

1

d1

By symmetry, we also have for any j ∈ [d2], (cid:107)e(cid:62)

. Then, by Lemma 30:

j V(cid:107) ≤ 3(κ(cid:63))2
F ≤ 2(cid:107)M − M(cid:63)(cid:107)2

(cid:107)SW − S(cid:63)(cid:107)2

Ω(cid:63)∪Ω +

8

γ − 1

(cid:107)M − M(cid:63)(cid:107)2

F

(cid:113) µrσ(cid:63)

1

d1

by Lemma 26:

(cid:107)M − M(cid:63)(cid:107)2
Similarly, we also have (cid:107)M − M(cid:63)(cid:107)2
we also have σ(cid:63)

Ω ≤ 36γαµ(κ(cid:63))4r(cid:107)M(cid:63)(cid:107)((cid:107)∆U(cid:107)2
Ω(cid:63) ≤ 0.04σ(cid:63)
(cid:107)N − N(cid:63)(cid:107)2

r(cid:107)∆(cid:107)2

r(cid:107)∆(cid:107)2

F. Given our choice of γ, this gives:

F + (cid:107)∆V(cid:107)2

F) ≤ 0.04σ(cid:63)

r(cid:107)∆(cid:107)2

F

F Clearly, we have (cid:107)M − M(cid:63)(cid:107)F ≤ (cid:107)N − N(cid:63)(cid:107)F. By Lemma 41,

F ≤ 2(cid:107)M − M(cid:63)(cid:107)2

Ω(cid:63)∪Ω +

8

γ − 1

(cid:107)M − M(cid:63)(cid:107)2

F ≤ 1
25

(cid:107)N − N(cid:63)(cid:107)2

F

(25)

2(

F ≤
√
1
2−1)
(cid:107)SW − S(cid:63)(cid:107)2

Finally, combineing Eq.(24) and Eq.(25), we have:

By Lemma 41, we know:

(cid:107)∆(cid:107)F ≤ 1√
σ(cid:63)
r

(cid:115)

(cid:107)N(cid:63) − N(cid:107)F − 60

≤ 4
5

(cid:107)N(cid:63) − N(cid:107)F

√
κ(cid:63)√
σ(cid:63)
r

(cid:115)

√
2(

1
2 − 1)

(cid:107)N(cid:63) − N(cid:107)F ≤

√

· 5 · 3

κ(cid:63) ≤ 20

√

κ(cid:63)

√
2(

1
2 − 1)

This ﬁnishes the proof.

Now we are ready to prove the main theorem:

Theorem 5. There is an absolute constant c, if γ > c, and γα · µr · (κ(cid:63))5 ≤ 1
have 1) all local minima satisﬁes UV(cid:62) = M(cid:63); 2) objective function is (, Ω(σ(cid:63)
polynomially small .

c holds, for objective function Eq.(6) we
))-pseudo strict saddle for

r ), O( 

κ(cid:63)

√
σ(cid:63)
r

28

Proof. Recall scaled version (multiplied by 4) of objective function Eq.(6) is:

F +
Consider point W with small gradient satisfying (cid:107)∇f (W)(cid:107)F ≤ . Let

f (W) = 2 min
S∈Sγα

(cid:107)UV(cid:62) + S − M(cid:63) − S(cid:63)(cid:107)2

(cid:107)U(cid:62)U − V(cid:62)V(cid:107)2

F

1
2

SW = argmin
S∈Sγα

(cid:107)UV(cid:62) + S − M(cid:63) − S(cid:63)(cid:107)2

F

and function fW( ˜W) = 2(cid:107) ˜U ˜V(cid:62) + SW − M(cid:63) − S(cid:63)(cid:107)2
fW( ˜W) ≥ f ( ˜W) and fW(W) = f (W). Since fW( ˜W) is matrix factorization objective where by Lemma 28:

2(cid:107) ˜U(cid:62) ˜U − ˜V(cid:62) ˜V(cid:107)2

F, then, we know for all ˜W, we have

F + 1

µrσ(cid:63)
(cid:107)S(cid:63) − SW(cid:107) ≤ 2γα · 2
1
d
σr(M(cid:63) + S(cid:63) − SW) ≥ σ(cid:63)
r − (cid:107)S(cid:63) − SW(cid:107) ≥ 0.99σ(cid:63)
σr+1(M(cid:63) + S(cid:63) − SW) ≤ (cid:107)S(cid:63) − SW(cid:107) ≤ 0.01σ(cid:63)

≤ 0.01σ(cid:63)

r

r

r

This gives σr(M(cid:63) + S(cid:63) − SW) ≥ 15σr+1(M(cid:63) + S(cid:63) − SW). Given (cid:107)∇fW(W)(cid:107)F = (cid:107)∇f (W)(cid:107)F ≤ , by Lemma 23,
we know either λmin(∇2fW(W)) ≤ −0.2σ(cid:63)
where U† = X†(D†) 1
2 ,
2 and X†D†Y†(cid:62) is the best rank r-approximation of M(cid:63) + S(cid:63) − SW. By Lemma 24, we immediately
V† = Y†(D†) 1
√
have (cid:107)∆(cid:107)F ≤ 103
σ(cid:63)
r

r or minR(cid:62)R=RR(cid:62)=I (cid:107)W − W†R(cid:107)F ≤ 20

, which ﬁnishes the proof.

σ(cid:63)
r

κ(cid:63)

B.3 Supporting Lemmas for robust PCA
In the proof of Lemma 24, we used several supporting lemmas. We now prove them one by one. The ﬁrst is a classical
result from matrix perturbations.
Lemma 25. Let M† be the top-r SVD of matrix M(cid:63) + S ∈ Rd1×d2 where M(cid:63) is rank r. Then we have:

Proof. By triangle inequality:

(cid:107)M† − M(cid:63)(cid:107)F ≤ 2(cid:107)S(cid:107)F

(cid:107)M(cid:63) − M†(cid:107)F ≤(cid:107)M(cid:63) + S − M†(cid:107)F + (cid:107)S(cid:107)F

For the second term, by the fact M(cid:63) is rank r, the deﬁnition of M† and Weyl’s inequality:

d(cid:88)

d(cid:88)

i (M(cid:63) + S) ≤ d(cid:88)

σ2

i=r+1

i=r+1

i=r+1

(σr+1(M(cid:63)) + σi−r(S))2 =

i−r(S) ≤ (cid:107)S(cid:107)2
σ2

F

(cid:107)M(cid:63) + S − M†(cid:107)2

F =

This ﬁnishes the proof.

Next we show how to bound the norm of matrix M − M(cid:63) restricted to a sparse set Ω.

Lemma 26. Let M(cid:63) = U(cid:63)V(cid:63)(cid:62) are both µ-incoherent matrix, M = UV(cid:62) satisﬁes maxi (cid:107)e(cid:62)
maxj (cid:107)e(cid:62)

and Ω has at most α fraction of non-zero entries in each row/column. Then:

j V(cid:107)2 ≤ µr(cid:107)M(cid:63)(cid:107)

i U(cid:107)2 ≤ µr(cid:107)M(cid:63)(cid:107)

d1

d2

and

(cid:107)M − M(cid:63)(cid:107)2

Ω ≤ 4αµr(cid:107)M(cid:63)(cid:107)((cid:107)∆U(cid:107)2

F + (cid:107)∆V(cid:107)2
F)

where ∆U = U − U(cid:63), ∆V = V − V(cid:63).

29

Proof. Then for any (i, j), since M, M(cid:63) are both µ-incoherent, we have:
j ∆V(cid:107) + (cid:107)e(cid:62)
i U(cid:107)(cid:107)e(cid:62)
(cid:107)e(cid:62)
i ∆U(cid:107)

V + ∆UV(cid:63)(cid:62))(i,j)| ≤ (cid:107)e(cid:62)

|(M − M(cid:63))(i,j)| =|(U∆(cid:62)

j ∆V(cid:107) +

(cid:114)

(cid:114)

(cid:107)e(cid:62)

≤

(cid:107)M(cid:63)(cid:107) µr
d2

i ∆U(cid:107)(cid:107)e(cid:62)

j V(cid:63)(cid:107)

Therefore,

(cid:107)M(cid:63)(cid:107) µr
d1

(cid:88)

(cid:107)M(cid:63) − M(cid:107)2

(cid:107)M(cid:63)(cid:107) µr
d1

Ω ≤2
≤4αµr(cid:107)M(cid:63)(cid:107)((cid:107)∆U(cid:107)2

(i,j)∈Ω

(cid:107)e(cid:62)

j ∆V(cid:107)2 + (cid:107)M(cid:63)(cid:107) µr
d2
F + (cid:107)∆V(cid:107)2
F)

(cid:107)e(cid:62)

i ∆U(cid:107)2

Next, we show for any ﬁxed sparse estimator S, if M(cid:63) is incoherent, the top-r SVD of M(cid:63) + S(cid:63) − S will also be

incoherent. Thus, sparse matrix will not interfere incoherence in this sense.
Lemma 27. For any S ∈ Sγα, let M† be the top-r SVD of M(cid:63) + S(cid:63) − S, and the SVD of M† to be XDY(cid:62), if
γα · µr · κ(cid:63) ≤ 1

1000 , then we have:

max

i

(cid:107)e(cid:62)

i X(cid:107)2 ≤ 4

µr(κ(cid:63))4

d1

and max

j

(cid:107)e(cid:62)

j Y(cid:107)2 ≤ 4

µr(κ(cid:63))4

d2

where condition number κ(cid:63) = σ1(M(cid:63))/σr(M(cid:63)).
Proof. Since XDY(cid:62) is the top r SVD of M(cid:63) + S(cid:63) − S, we have:

(M(cid:63) + S(cid:63) − S)(M(cid:63) + S(cid:63) − S)(cid:62)X = XD2

Therefore, for any i ∈ [d], because S, S(cid:63) has at most γα fraction non-zero entries in each row, and (cid:107)S(cid:63)(cid:107)∞ ≤ 2 µrσ(cid:63)
1√
and S ∈ Sγα, we have:
d1d2
r (D)(cid:107)e(cid:62)
σ2

i X(cid:107) ≤(cid:107)e(cid:62)
≤((cid:107)e(cid:62)
≤

i XD2(cid:107) = (cid:107)e(cid:62)
i M(cid:63)(cid:107) + (cid:107)e(cid:62)

i (M(cid:63) + S(cid:63) − S)(M(cid:63) + S(cid:63) − S)(cid:62)X(cid:107)
(cid:21)
i (S(cid:63) − S)(cid:107))(cid:107)M(cid:63) + S(cid:63) − S(cid:107)
((cid:107)S(cid:63)(cid:107)∞ + (cid:107)S(cid:107)∞)(cid:107)D(cid:107)

1 +(cid:112)γαd2

σ(cid:63)

(cid:20)(cid:114) µr
(cid:114) µr

d1

d1

≤

σ(cid:63)
1 (1 + 5

γα · µr)(cid:107)D(cid:107) ≤ 1.2

1 · (cid:107)D(cid:107)
σ(cid:63)

(cid:114) µr

d1

On the other hand, by Lemma 28 and Weyl’s inequality, we also have

√

(cid:112)

Therefore, in sum, we have:

(cid:107)S(cid:63) − S(cid:107) ≤(cid:107)S(cid:63)(cid:107) + (cid:107)S(cid:107) ≤ γα

d1d2((cid:107)S(cid:63)(cid:107)∞ + (cid:107)S(cid:107)∞) ≤ 0.1σ(cid:63)

r

(cid:107)D(cid:107) ≤(cid:107)M(cid:107) + (cid:107)S(cid:63) − S(cid:107) ≤ 1.1σ(cid:63)
σr(D) ≥σr(M(cid:63)) − (cid:107)S(cid:63) − S(cid:107) ≥ 0.9σ(cid:63)

1

r

(cid:114) µr

(cid:107)e(cid:62)

i X(cid:107) ≤ 1.2

1 · (cid:107)D(cid:107)
σ(cid:63)
σ2
r (D)
j Y(cid:107) with any j ∈ [d2].

d1

(cid:114) µr

d1

≤ 2

(κ(cid:63))2

By symmetry, we can also prove it for (cid:107)e(cid:62)

We also need to upper bound the spectral norm of sparse matrix S.

30

Proof. Let Ω be the support of matrix S, and β =

, we have:

(cid:112)

(cid:107)S(cid:107) ≤ α

(cid:113) d1

d2

d1d2(cid:107)S(cid:107)∞

Lemma 28. For any sparse matrix S that can only has at most α fraction non-zero entries in each row/column, we
have:

(cid:107)S(cid:107) =

sup

(x,y):(cid:107)x(cid:107)=1,(cid:107)y(cid:107)=1

x(cid:62)Sy =

xi(S)ijyj ≤ 1
2

(cid:88)

(i,j)∈Ω

(cid:88)

(i,j)∈Ω

(cid:107)S(cid:107)∞(βx2

i +

j ) ≤ α
y2

1
β

(cid:112)

d1d2(cid:107)S(cid:107)∞

Then, we show a crucial property of the optimal S ∈ Sγα:

Lemma 29. For any matrix A, let Sopt = argminS∈Sγα (cid:107)A − S(cid:107)2
[d1] × [d2] − Ω, we have:

|A(i,j)| ≤ |A(γαd1)

(i,·)

| + |A(γαd2)

(·,j)

|

F, and Ω be the support of S, then for any (i, j) ∈

(i,·) is the k-th largest element (in terms of absolute value) in i-th row of A and A(k)

where A(k)
(in terms of absolute value) in j-th column of A.
Proof. Assume the contradiction that in optimal solution S there is a pair (i, j) ∈ [d1] × [d2] − Ω such that

(·,j) the k-th largest element

|A(i,j)| > |A(γαd1)

(i,·)

| + |A(γαd2)

(·,j)

|

(i,·)

If row i has exactly γαd1 elements in Ω, let e1 = (i, j(cid:48)) be the smallest entry in row i (j(cid:48) = arg minz:(i,z)∈Ω |A(i,z)|),
clearly A(i,j(cid:48)) ≤ A(γαd1)
. If row i has fewer elements we just let e1 be empty. Similarly, if column j has exactly
γαd2 elements in Ω, let e2 = (i(cid:48), j) be the smallest entry in the column (i(cid:48) = arg minz:(z,j)∈Ω |A(z,j)|), we also have
A(i(cid:48),j) ≤ A(γαd2)
the support constraint. Let q(x) = (x − max{0, x − 2 µrσ(cid:63)
1√
q(x) + q(y) ≤ q(x + y). Now the difference we get from changing S to S(cid:48) is
d1d2

Now we can add (i, j) to Ω, and remove e1 and e2. Call the resulting matrix S(cid:48). This clearly does not violate
})2, this function is monotone for x > 0 and satisﬁes

(·,j)

.

(cid:107)A − S(cid:48)(cid:107)2

F = (cid:107)A − S(cid:107)2

F − q(|A(i,j)|) + q(|Ae1|) + q(|Ae2|) < (cid:107)A − S(cid:107)2
F .

This contradicts with the fact that S was the optimal. Therefore there cannot be such an entry (i, j).

Finally, by using above lemmas, we can show how to bound the difference between optimal S and true sparse

matrix S(cid:63), which is a key step in Lemma 24.
Lemma 30. For any matrix A ∈ Rd1×d2, let S(cid:63) ∈ Sα and
Sopt = argmin
S∈Sγα

(cid:107)S − S(cid:63) + A(cid:107)2

F

Let Ω be the support of Sopt and Ω(cid:63) be the support of S(cid:63), we will have:

Proof. Clearly, the support of Sopt − S(cid:63) must be a subset of Ω ∪ Ω(cid:63). Therefore, we have:

(cid:107)Sopt − S(cid:63)(cid:107)2

F ≤ 2(cid:107)A(cid:107)2

Ω(cid:63)∪Ω +

8

γ − 1

(cid:107)A(cid:107)2

F

(cid:107)Sopt − S(cid:63)(cid:107)2

F = (cid:107)Sopt − S(cid:63)(cid:107)2

Ω + (cid:107)Sopt − S(cid:63)(cid:107)2

Ω(cid:63)−Ω

31

For the ﬁrst term, since Sopt is deﬁned as minimizer over Sγα, we know for (i, j) ∈ Ω:

(cid:26)

(cid:26)

(cid:27)

(cid:27)

(Sopt)(i,j) = max

min

(S(cid:63) − A)(i,j), 2

µrσ(cid:63)
1√
d1d2

, − 2

µrσ(cid:63)
1√
d1d2

By assumption we know S(cid:63) ∈ Sα thus (cid:107)S(cid:63)(cid:107)∞ ≤ 2 µrσ(cid:63)
1√
d1d2

, this gives |(Sopt − S(cid:63))(i,j)| ≤ |(A)(i,j)| thus:
Ω ≤ (cid:107)A(cid:107)2

Ω

(cid:107)Sopt − S(cid:63)(cid:107)2

For the second term, by triangle inequality, we have:

(cid:107)Sopt − S(cid:63)(cid:107)Ω(cid:63)−Ω = (cid:107)S(cid:63)(cid:107)Ω(cid:63)−Ω ≤ (cid:107)S(cid:63) − A(cid:107)Ω(cid:63)−Ω + (cid:107)A(cid:107)Ω(cid:63)−Ω

By Lemma 29, we have for any (i, j) ∈ Ω(cid:63) − Ω:

where the second inequality used the fact that S(cid:63) has at most α non-zero entries each row/column. Then, we have:

|(S(cid:63) − A)(i,j)|2 ≤2|(S(cid:63) − A)(γαd1)

|2 + 2|(S(cid:63) − A)(γαd2)

|2

(i,·)
≤2|(A)((γ−1)α)
≤

(γ − 1)α

(i,·)

2

|2 + 2|(A)((γ−1)α)

i A(cid:107)2 + (cid:107)Aej(cid:107)2(cid:1)
(cid:0)(cid:107)e(cid:62)

(·,j)

(·,j)
|2

Ω(cid:63)−Ω ≤ (cid:88)

(cid:107)S(cid:63) − A(cid:107)2

(cid:0)(cid:107)e(cid:62)
i A(cid:107)2 + (cid:107)Aej(cid:107)2(cid:1)

2

(γ − 1)α

(i,j)∈Ω(cid:63)−Ω
(cid:107)A(cid:107)2

≤ 4

γ − 1

F

Therefore, in conclusion, we have:

(cid:107)Sopt − S(cid:63)(cid:107)2

F ≤(cid:107)Sopt − S(cid:63)(cid:107)2
≤(cid:107)A(cid:107)2
≤2(cid:107)A(cid:107)2

Ω + 2(cid:107)A(cid:107)2
Ω(cid:63)∪Ω +

Ω + (cid:107)Sopt − S(cid:63)(cid:107)2

Ω(cid:63)−Ω
Ω(cid:63)−Ω + 2(cid:107)S(cid:63) − A(cid:107)2
8
γ − 1

(cid:107)A(cid:107)2

F

Ω(cid:63)−Ω

C Matrix Sensing with Noise

In this section we demonstrate how to handle noise using our framework. The key idea here is to consider the noise as
a perturbation to the original objective function and use Q(U) (originally the regularizer) to also capture the noise.

C.1 Symmetric case
Here, we assume in each observation, instead of observing the exact value bi = (cid:104)Ai, M(cid:63)(cid:105) we observe bi = (cid:104)Ai, M(cid:63)(cid:105)+
ni. Here ni is i.i.d N (0, σ2). Recall the objective function in Section 4, we now have:

m(cid:88)

i=1

f (M) =

1
m

((cid:104)M − M(cid:63), Ai(cid:105) + ni)2

(26)

Deﬁne Q(U) = f (UU(cid:62)) − 1

2 (UU(cid:62) − M(cid:63)) : H : (UU(cid:62) − M(cid:63)) to be the perturbation, we can write out the

non-convex objective

min

U∈Rd×r

1
2

(UU(cid:62) − M(cid:63)) : H : (UU(cid:62) − M(cid:63)) + Q(U).

(27)

Now we can use the same framework. Again, since for matrix sensing we have the RIP property, we do not need

the ﬁrst step to restrict to special low rank matrices. Using our approach we can get

32

Theorem 31. For objective Equation (27), suppose the sensing matrices {Ai}’s satisfy (2r, 1/10)-RIP, with high
probability all points satisfy ﬁrst and second order optimality condition must satisfy

(cid:114)

Proof. Using the same proof as Theorem 8, we know

(cid:107)UU(cid:62) − M(cid:63)(cid:107)F ≤ O(σ

dr log m

m

).

∆∆(cid:62) : H : ∆∆(cid:62) − 3(N − N(cid:63)) : H : (N − N(cid:63)) ≥ −0.5(cid:107)N − N(cid:63)(cid:107)2
F .

We then bound the contribution from Q.

Q(U) = − 2
m

(cid:104)∇Q(U), ∆(cid:105) = − 2
m

∆ : ∇2Q(U) : ∆ = − 4
m

i=1

m(cid:88)
m(cid:88)
m(cid:88)

i=1

i=1

Therefore,

m(cid:88)

i=1

(ni)2

((cid:104)M − M(cid:63), Ai(cid:105)ni) +

1
m
((cid:104)U∆(cid:62) + ∆U(cid:62), Ai(cid:105)ni)

((cid:104)∆∆(cid:62), Ai(cid:105)ni)

m(cid:88)
m(cid:88)

[∆ : ∇2Q(U) : ∆ − 4(cid:104)∇Q(U), ∆(cid:105)]
≤ − 4
m

((cid:104)∆∆(cid:62), Ai(cid:105)ni) +

8
m
((cid:104)M − M(cid:63), Ai(cid:105) · ni) +

m(cid:88)
m(cid:88)

i=1

i=1

=

4
m

i=1

4
m

i=1

((cid:104)U∆(cid:62) + ∆U(cid:62), Ai(cid:105)ni)

((cid:104)U∆(cid:62) + ∆U(cid:62), Ai(cid:105) · ni).

(cid:114)
(cid:114)

(cid:114)
(cid:114)

Here the last step follows from M − M(cid:63) + ∆∆(cid:62) = U∆(cid:62) + ∆U(cid:62). Intuitively, ni is random and should not have
large correlation with any ﬁxed vector. We formalize this in Lemma 34. Using this lemma, we know

((cid:104)M − M(cid:63), Ai(cid:105)ni)| ≤ 4σ

dr log m

m

(cid:107)M − M(cid:63)(cid:107)F

m(cid:88)

i=1

| 4
m

m(cid:88)

| 4
m

√

2)(cid:107)M − M(cid:63)(cid:107)F (by

((cid:104)U∆(cid:62) + ∆U(cid:62), Ai(cid:105)ni| ≤ 4σ

dr log m

(cid:107)U∆(cid:62) + ∆U(cid:62)(cid:107)F

i=1

≤ 4(1 +

(cid:114)

√

m

2)σ

(cid:107)M − M(cid:63)(cid:107)F

dr
m

Here the last inequality follows from (cid:107)U∆(cid:62) + ∆U(cid:62)(cid:107)F ≤ (cid:107)M − M(cid:63)(cid:107)F + (cid:107)∆∆(cid:62)(cid:107)F ≤ (1 +
Lemma 40). Now using the main Lemma 7, we know
∆ : ∇2f (U) : ∆ ≤ − 1
2

(cid:107)M − M(cid:63)(cid:107)2

F

√
+ (8 + 4

2)σ

dr log m

m

(cid:107)M − M(cid:63)(cid:107)F .

If the current point satisfy the second order optimality condition we must have

(cid:107)M − M(cid:63)(cid:107)F ≤ O(σ

dr log m

).

m

Note that this bound matches the intuitive bound from the VC-dimension of rank-r matrices.

33

C.2 Asymmetric case
For the asymmetric case, the proof is again almost identical. We use the same noise model where the observation
bi = (cid:104)Ai, M(cid:63)(cid:105) + ni where ni ∼ N (0, σ2). We also use the same notations as in Deﬁnition 7. Let Q(W) =
Q(U, V) = 2
m

(cid:80)m
i=1[((cid:104)M − M(cid:63), Ai(cid:105) + ni)2 − ((cid:104)M − M(cid:63), Ai(cid:105))2], we have the objective function

f (U, V) =

1
2

(N − N(cid:63)) : H : (N − N(cid:63)) + Q(W).

(28)

Again by bounding the gradient and Hessian for Q(W) we get the following

Theorem 32. For objective Equation (28), suppose the sensing matrices {Ai}’s satisfy (2r, 1/20)-RIP, let d = d1+d2,
with high probability all points satisfy ﬁrst and second order optimality condition must satisfy

(cid:114)

(cid:107)UV(cid:62) − M(cid:63)(cid:107)F ≤ O(σ

dr log m

m

).

Proof. Again using the same proof as Theorem 3, we know

∆∆(cid:62) : H : ∆∆(cid:62) − 3(M − M(cid:63)) : H : (M − M(cid:63)) ≥ −0.5(cid:107)M − M(cid:63)(cid:107)2
F .

We then bound the contribution from Q.

Q(W) = − 8
m

(cid:104)∇Q(W), ∆(cid:105) = − 8
m

∆ : ∇2Q(W) : ∆ = − 16
m

i=1

m(cid:88)
m(cid:88)
m(cid:88)

i=1

i=1

((cid:104)M − M(cid:63), Ai(cid:105)ni) +

4
m
V + ∆U V(cid:62), Ai(cid:105)ni)

((cid:104)U∆(cid:62)

((cid:104)∆U ∆(cid:62)

V , Ai(cid:105)ni)

m(cid:88)

i=1

(ni)2

Let Bi be the (d1 + d2) × (d1 + d2) matrix whose diagonal blocks are 0, and off diagonal blocks are equal to Ai

and A(cid:62)

i respectively, we have

[∆ : ∇2Q(W) : ∆ − 4(cid:104)∇Q(W), ∆(cid:105)]
≤ − 16
m

((cid:104)∆∆(cid:62), Bi(cid:105)ni) +

32
m

((cid:104)W∆(cid:62) + ∆W(cid:62), Bi(cid:105)ni)

=

16
m

((cid:104)N − N(cid:63), Bi(cid:105)ni) +

((cid:104)W∆(cid:62) + ∆W(cid:62), Bi(cid:105)ni).

16
m

Now we can use Lemma 34 again to bounding the noise terms:

m(cid:88)
m(cid:88)

i=1

i=1

m(cid:88)

i=1

| 8
m

m(cid:88)

i=1

| 8
m

i=1

i=1

m(cid:88)
m(cid:88)
(cid:114)
(cid:114)
(cid:114)

((cid:104)N − N(cid:63), Bi(cid:105)ni)| ≤ 8σ

((cid:104)W∆(cid:62) + ∆W(cid:62), Bi(cid:105)ni| ≤ 8σ

dr log m

m

(cid:107)M − M(cid:63)(cid:107)F

dr log m

m

(cid:107)U∆(cid:62)

V + ∆U V(cid:62)(cid:107)F

≤ 8σ

dr log m

(cid:107)W∆(cid:62) + ∆W(cid:62)(cid:107)F

(cid:114)

√

m

2)σ

(cid:107)N − N(cid:63)(cid:107)).

dr
m

≤ 8(1 +

34

Therefore the Hessian at ∆ direction is equal to:

∆ : ∇2f (W) : ∆ ≤ − 1
2

(cid:107)N − N(cid:63)(cid:107)2

√
F + (16 + 8

2)σ

(cid:114)

dr log m

m

(cid:107)N − N(cid:63)(cid:107)F .

(cid:113) dr log m

m ).

When the point satisﬁes the second order optimality condition we have

(cid:114)

(cid:107)N − N(cid:63)(cid:107)F ≤ O(σ

dr log m

m

).

In particular, M − M(cid:63) is a submatrix of N − N(cid:63), therefore (cid:107)M − M(cid:63)(cid:107)F ≤ O(σ

D Proof Sketch for Running Time

In this section we sketch the proof for Corollary 17.

Corollary 17. Let R be the Frobenius norm of the initial points U0, V0, a saddle-avoiding local search algorithm can
ﬁnd a point -close to global optimal for matrix sensing (10)(3), matrix completion (11)(4) in poly(R, 1/, d, σ(cid:63)
1, 1/σ(cid:63)
r )
iterations. For robust PCA (12)(6), alternating between a saddle-avoiding local search algorithm and computing
optimal S ∈ Sγα will ﬁnd a point -close to global optimal in poly(R, 1/, d, σ(cid:63)

r ) iterations.

1, 1/σ(cid:63)

on the detailed proofs of the guarantees, so we only give a proof sketch here.

The full proof require some additional analysis depending on the particular algorithm used, and is highly dependent
Our geometric results show that for small enough (cid:48), the objective functions are ((cid:48), γ, C(cid:48)) strict-saddle where γ
r ). Choose (cid:48) = /C, we know for each point, either it has a gradient at
and C may depend polynomially on (σ(cid:63)
least (cid:48), or the Hessian has an eigenvalue smaller than −γ, or (cid:107)∆(cid:107)F ≤ . In the ﬁrst two cases, by Deﬁnition 8 we
know saddle-avoiding algorithm can decrease the function value by an inverse polynomial factor in polynomial time.
By the radius of the initial solution, the difference in function value between the original solution and optimal solution
is bounded by poly(R, σ(cid:63)
1, d), so after a polynomial number of iterations we can no longer decrease function value and
must be in the third case (where (cid:107)∆(cid:107)F ≤ ).

1, σ(cid:63)

Smoothness and Hessian Lipschitz The objective funcitons we work with are mostly polynomials thus both smooth
and Hessian Lipschitz. The regularizers we add also tried to make sure at least both smoothness and Hessians Lipschitz
are satisﬁed. However, the objective functions are still not very smooth or Hessian-Lipschitz especially in the region
when then the norm of (U, V) is very large. This is because the polynomials are of degree more than 2 and in general
the smoothness and Hessian-Lipschitzness parameters (l, ρ) depend on the norm of the current point (U, V). It is not
hard to show that when the solution is constrained into a ball of radius R, the parameters l, ρ are all poly(R). Therefore
to complete the proof we need to show that the intermediate steps of the algorithms cannot escape from a large ball.
In fact, for all the known algorithms, on our objective functions the following is true

Lemma 33. For current saddle avoiding algorithms (including cubic regularization [Nesterov and Polyak, 2006],
perturbed gradient descent [Jin et al., 2017] ) There exists a radius R that is polynomial in problem parameters, such
that if initially (cid:107)U(cid:107)F + (cid:107)V(cid:107)F = R0 ≤ R, then with high probability all the iterations will have (cid:107)U(cid:107)F + (cid:107)V(cid:107)F ≤ 2R.
The proof of this lemma is mostly calculations (and observing the fact that when U, V are both very large, the
gradient will essentially point to 0), as an example this is done for matrix factorization in [Jin et al., 2017]. We omit
the proof in this paper.

Note that our geometric results for matrix sensing does not depend on the dimension. If we can prove a bound on
R that is independent of the dimension d, by recent result in [Jin et al., 2017], we can get algorithms whose number of
iterations depend only on log d for matrix sensing.

35

Handling Robust-PCA For robust PCA, the objective function is only pseudo strict-saddle (see Deﬁnition 5).

In order to turn the geometric property to an algorithm, the ﬁrst observation is that the optimal S for U, V can be
found in polynomial time: The problem of ﬁnding the optimal S can be formulated as a weighted bipartite matching
problem where one part corresponds to the rows, the other part corresponds to the columns, and the value corresponds
to the improvement in objective function when we add (i, j) into the support. According to the deﬁnition of S, each
row/column can be matched a limited number of times. This problem can be solved by converting it to max-ﬂow, and
standard analysis shows that there exists an optimal integral solution.
Next we view the robust PCA objective function of form f (U, V) = minS∈Sγα g(U, V; S). We show that
alternating between saddle-avoiding local search and optimizing S over Sγα will allow us to get the desired guarantee.
For a point U, V, if it is not close enough to the global optimal solution, we can ﬁx the optimal S for U, V and
study g(U, V; S). First, we know for this optimal choice of S, the gradient of g(U, V; S) over (U, V) is the same
as gradient of f (U, V). Then, by Theorem 15 / Theorem 5, we know either the gradient of g(U, V; S) is large
or the Hessian of g(U, V; S) has an eigenvalue at most −Ω(σ(cid:63)
r ). By the guarantee of saddle-avoiding algorithms in
polynomial number of steps we can ﬁnd U(cid:48), V(cid:48) such that the objective function g(U(cid:48), V(cid:48); S) will decrease by a inverse
polynomial. After that, replacing S with S(cid:48) (optimal for U(cid:48), V(cid:48)) cannot increase function value, so in polynomial time
we found a new point such that f (U(cid:48), V(cid:48)) ≤ f (U, V) − δ where δ is at least an inverse polynomial. This procedure
cannot be repeated by more than polynomial number of times (because the function value cannot decrease below the
optimal value), so the algorithm ﬁnds an approximate optimal point in polynomial time.

E Concentrations

In this section we summarize the concentration inequalities we use for different problems.

E.1 Matrix Sensing
Deﬁnition 9 (Restrict Isometry Property). Measurement A ({Ai}) satisﬁes (r, δr)-Restrict Isometry Property (RIP)
if for any matrix X with rank r, we have:

(1 − δr)(cid:107)X(cid:107)2

F ≤ 1
m

(cid:104)Ai, X(cid:105)2 ≤ (1 + δr)(cid:107)X(cid:107)2

F

m(cid:88)

i=1

In the case of Gaussian measurement, standard analysis shows when m = O( dr
condition with probability at least 1 − eΩ(d). (Candes and Plan [2011], Theorem 2.3)

We need the follow inequality for handling noise.

δ2 ), we have A satisfying (r, δ)-RIP

Lemma 34. Suppose the set of sensing matrices A1, A2, ..., Am satisfy the (2r, δ)-RIP condition, let n1, n2, ..., nm
be iid. Gaussian N (0, σ2), then with high probability for any matrix M of rank at most r, we have

ni(cid:104)Ai, M(cid:105)| ≤ O

σ

dr log m

m

(cid:107)M(cid:107)F

Proof. Since the LHS is linear in M we focus on matrices with (cid:107)M(cid:107)F = 1.

Let X be an -net for rank-r matrices with Frobenius norm 1. By standard constructions we know log |X| ≤
(cid:80)m
dr log(dr/). We will set  = 1/m so log(dr/) = O(log m) (m is at least dr for RIP condition). Now, for any
i=1 ni(cid:104)Ai, M is just a Gaussian random variable with variance at most σ2(1 + δ)/m.
matrix M ∈ X , we know 1
is at most exp(−C(cid:48)dr log m). When C is a large enough
Therefore, the probability that it is larger than σ
constant we can apply union bound, and we know for every M ∈ X ,

(cid:113) dr log m

m

m

m(cid:88)

i=1

| 1
m

m(cid:88)

| 1
m

(cid:32)

(cid:114)

(cid:32)

(cid:114)

(cid:33)

.

(cid:33)

.

ni(cid:104)Ai, M(cid:105)| ≤ O

σ

dr log m

m

(cid:107)M(cid:107)F

i=1

36

On the other hand, with high probability the norm of the vector n ∈ Rm is O(σ
M(cid:48) be the closest matrix in X , let zi = (cid:104)Ai, M − M(cid:48)(cid:105), then we know the norm of zi is at most 1+δ
property). Now we know

m). Suppose M is not in X , let
m (again by RIP

m(cid:88)

i=1

m(cid:88)

i=1

| 1
m

ni(cid:104)Ai, M(cid:105)| ≤ | 1
m

ni(cid:104)Ai, M(cid:48)(cid:105)| + (cid:104)z, n(cid:105) ≤ O

σ

dr log m

m

(cid:107)M(cid:107)F

√

(cid:32)

(cid:114)

(cid:33)

.

E.2 Matrix Completion
For matrix completion, we need different concentration inequalities for different kinds of matrices. The ﬁrst kind of
matrix lies in a tangent space and is proved in Candes and Recht [2012].
Lemma 35. Candes and Recht [2012] Let subspace

T = {M ∈ Rd1×d2|M = U(cid:63)X(cid:62) + YV(cid:63)(cid:62), for some X ∈ Rd1×r, Y ∈ Rd2×r}.

for any δ > 0, as long as sample rate p ≥ Ω( µr

δ2d log d), we will have:
PT PΩPT − PT (cid:107) ≤ δ
(cid:107) 1
p

For arbitrary low rank matrix, we use the following lemma which comes from graph theory.

Lemma 36. Suppose Ω ⊂ [d1] × [d2] is the set of edges of a random bipartite graph with (d1, d2) nodes, where any
pair of nodes on different side is connected with probability p. Let d = max d1, d2,then there exists universal constant
c1, c2, for any δ > 0 so that if p ≥ c1
min{d1,d2} , then with probability at least 1 − d−4, we have for any x, y ∈ Rd:
(cid:88)

(cid:115)

log d

1
p

(i,j)∈Ω

xiyj ≤ (cid:107)x(cid:107)1(cid:107)y(cid:107)1 + c2

(cid:107)x(cid:107)2(cid:107)y(cid:107)2

d
p

Proof. Let A be the adjacency matrix of the graph. Clearly E[A] = pJ where J is the all 1’s matrix. Let Z =
A − E[A]. The matrix Z has independent entries with expectation 0 and variance p(1 − p). By random matrix theory,
we know when p ≥ c1
pd [Latała,
2005]1. Now for any vectors x, y simultaneously, we have

min{d1,d2}, with probability at least 1 − d−4, we have (cid:107)Z(cid:107) = (cid:107)A − E[A](cid:107) ≤ c2
(cid:88)

√

log d

1
p

(i,j)∈Ω

xiyj =

1
p

x(cid:62)Ay =

x(cid:62)(pJ + Z)y

1
p

(cid:115)

≤(cid:104)x, 1(cid:105)(cid:104)y, 1(cid:105) + c2

Above lemma immediately implies following:

(cid:107)x(cid:107)2(cid:107)y(cid:107)2 ≤ (cid:107)x(cid:107)1(cid:107)y(cid:107)1 + c2

d
p

(cid:107)x(cid:107)2(cid:107)y(cid:107)2.

d
p

(cid:115)

(cid:115)

Lemma 37. Let d = max{d1, d2}. There exists universal constant c1, c2, for any δ > 0 so that if p ≥ c1
then with probability at least 1 − 1

2 d−4, we have for any matrices X, Y ∈ Rd×r:

log d

min{d1,d2} ,

(cid:107)XY(cid:62)(cid:107)2

¯Ω ≤ (cid:107)X(cid:107)2

F(cid:107)Y(cid:107)2

F + c2

1
p

d
p

(cid:107)X(cid:107)F(cid:107)Y(cid:107)F · max

i

(cid:107)e(cid:62)

i X(cid:107) · max

j

(cid:107)e(cid:62)

j Y(cid:107)

1The high probability result follows directly from Talagrand’s inequality.

37

Proof.

(cid:107)XY(cid:62)(cid:107)2

Ω =

1
p

1
p

(cid:88)

(i,j)∈Ω

The remaining follows from Lemma 36.

(cid:107)e(cid:62)

i X(cid:107)2(cid:107)e(cid:62)

j Y(cid:107)2

On the other hand, for all low-rank matrices we also have the following (which is tighter for incoherent matrices).
Lemma 38. Ge et al. [2016] Let d = max{d1, d2}, then with at least probability 1 − eΩ(d) over random choice of Ω,
we have for any rank 2r matrices A ∈ Rd1×d2:

(cid:115)

(cid:12)(cid:12)(cid:12)(cid:12) 1

p

(cid:12)(cid:12)(cid:12)(cid:12) ≤ O(

(cid:107)PΩ(A)(cid:107)2

Ω − (cid:107)A(cid:107)2

F

dr log d

p

(cid:107)A(cid:107)2∞ +

dr log d

p

(cid:107)A(cid:107)F(cid:107)A(cid:107)∞)

Although Ge et al. [2016] stated the symmetric version, and we need the asymmetric version here, the proof in Ge

et al. [2016] works directly. In fact, they ﬁrst proved the asymmetric case in the proof.

Finally, for a matrix with each entry randomly sampled independently with small probability p, next lemma says

with high probablity, no row can have too many non-zero entries.
Lemma 39. Let Ωi denote the support of Ω on i-th row, let d = max{d1, d2}. Assume pd2 ≥ log(2d), then with at
least probability 1 − 1/poly(d) over random choice of Ω, we have for all i ∈ [d1] simultaneously:

|Ωi| ≤ O(pd2)
Proof. This follows directly from Chernoff bound and union bound.

F Auxiliary Inequalities

In this section, we provide some frequently used lemmas regarding matrices. Our ﬁrst two lemmas lower bound
(cid:107)UU(cid:62) − YY(cid:62)(cid:107)2
Lemma 40. Let U and Y be two d × r matrices. Further let U(cid:62)Y = Y(cid:62)U be a PSD matrix. Then,

F by (cid:107)(U − Y)(U − Y)(cid:62)(cid:107)2

F and (cid:107)U − Y(cid:107)2
F.

Proof. To prove this, we let ∆ = U − Y, and expand:

(cid:107)(U − Y)(U − Y)(cid:62)(cid:107)2

F ≤ 2(cid:107)UU(cid:62) − YY(cid:62)(cid:107)2

F

(cid:107)UU(cid:62) − YY(cid:62)(cid:107)2

F =(cid:107)U∆(cid:62) + ∆U(cid:62) − ∆∆(cid:62)(cid:107)2
=tr(2U(cid:62)U∆(cid:62)∆ + (∆(cid:62)∆)2 + 2(U(cid:62)∆)2 − 4U(cid:62)∆∆(cid:62)∆)
=tr(2U(cid:62)(U − ∆)∆(cid:62)∆ + (

2U(cid:62)∆)2 +

∆(cid:62)∆ −

√

F

1√
2

(∆(cid:62)∆)2)

1
2

≥tr(2U(cid:62)Y∆(cid:62)∆ +

1
2

(∆(cid:62)∆)2) ≥ 1
2

(cid:107)∆∆(cid:62)(cid:107)2

F

The last inequality is due to U(cid:62)Y is a PSD matrix.
Lemma 41. Let U and Y be two d × r matrices. Further let U(cid:62)Y = Y(cid:62)U be a PSD matrix. Then,

σmin(U(cid:62)U)(cid:107)U − Y(cid:107)2

F ≤ (cid:107)(U − Y)U(cid:62)(cid:107)2

F ≤

√
2(

1
2 − 1)

(cid:107)UU(cid:62) − YY(cid:62)(cid:107)2

F

38

Proof. The left inequality is basic, we only need to prove right inequality. To prove this, we let ∆ = U − Y, and
expand:

(cid:107)UU(cid:62) − YY(cid:62)(cid:107)2

F =(cid:107)U∆(cid:62) + ∆U(cid:62) − ∆∆(cid:62)(cid:107)2
=tr(2U(cid:62)U∆(cid:62)∆ + (∆(cid:62)∆)2 + 2(U(cid:62)∆)2 − 4U(cid:62)∆∆(cid:62)∆)
√
=tr((4 − 2
√
√
≥tr((4 − 2

2)U(cid:62)(U − ∆)∆(cid:62)∆ + (∆(cid:62)∆ −
2)U(cid:62)Y∆(cid:62)∆ + 2(

2 − 1)U(cid:62)U∆(cid:62)∆) ≥ 2(

2U(cid:62)∆)2 + 2(

√

√

F

√

2 − 1)U(cid:62)U∆(cid:62)∆)

2 − 1)(cid:107)U∆(cid:62)(cid:107)2

F

The last inequality is due to U(cid:62)Y is a PSD matrix.

Next we show the difference between matrices formed by swapping sigular spaces of M1 and M2 can be upper

bounded by the difference between M1 and M2.
Lemma 42. Let M1, M2 ∈ Rd1×d2 be two arbitrary matrices whose SVDs are U1D1V(cid:62)
have:

(cid:107)U1D1U(cid:62)

1 − U2D2U(cid:62)
2 (cid:107)2

F + (cid:107)V1D1V(cid:62)

1 − V2D2V(cid:62)
2 (cid:107)2

F ≤ 2(cid:107)M1 − M2(cid:107)2

F

1 and U2D2V(cid:62)

2 . Then we

Proof. Expand the Frobenius Norm out, we have LHS:
2 (cid:107)2
1 − U2D2U(cid:62)
F + (cid:107)V1D1V(cid:62)
2 − U1D1U(cid:62)
1 U2D2U(cid:62)

(cid:107)U1D1U(cid:62)
=2tr(D2
1 + D2

1 − V2D2V(cid:62)
2 (cid:107)2

F

2 − V1D1V(cid:62)

1 V2D2V(cid:62)
2 )

On the other hand, we also have RHS:

2(cid:107)M1 − M2(cid:107)2
=2tr(D2
1 + D2

F = 2(cid:107)U1D1V(cid:62)
2 − U1D1V(cid:62)
1 V(cid:62)

1 V2D

1
2

1 − U2D2V(cid:62)
2 (cid:107)2

F

1 V2D2U(cid:62)

2 − U2D2V(cid:62)

2 V1D1U(cid:62)
1 )

Let A = D

1
2

1 U(cid:62)

1 U2D

2 and B = D

1
2

1
2

2 . We know to prove the lemma, we only need to show tr(AA(cid:62) +

BB(cid:62)) ≥ tr(AB(cid:62) + BA(cid:62)). This is true because (cid:107)A − B(cid:107)2

F ≥ 0, which ﬁnishes the proof.

39

