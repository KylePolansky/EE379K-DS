Fast Bayesian Permanental Processes

A. Supplementary Material
Accompanying the submission Fast Bayesian Intensity Estimation for the Permanental Process.

A.1. Exact Expected Log Loss
We evaluate our estimated ˆλ using the expectation under the true PP(λ) of the log likelihood under PP(ˆλ), where PP is
the Poisson process. Adams et al. (2009) approximate this quantity using Monte Carlo, employing numerical integration
for (1). It turns out that for the computational cost of one such numerical integration, we may compute the expected loss
using standard results for L´evy processes (Cont & Tankov, 2004). An elementary self contained argument runs as follows:

(cid:104)

(cid:105)

= Ecard(X)
= Ecard(X)

EX∼PP(λ)

log pX∼PP(ˆλ)(X)

(cid:16)
(cid:16)
where Ω is the sampling domain, H(λ, ˆλ) :=(cid:82)
functions proportional to λ and ˆλ and we recall Λ(S) :=(cid:82)

= Γ(Ω)

(cid:90)

x∈Ω

=

(cid:104)EX∼PP(λ)| card(X)
(cid:104)

(cid:16)

log ˆΛ(Ω) + H(λ, ˆλ)
λ(x) log ˆλ(x) − ˆλ(x)

(cid:105)(cid:105)
(cid:17) − ˆΛ(Ω)

(cid:105)

log pX∼PP(ˆλ)(X)

(cid:104)
(cid:17) − ˆΛ(Ω)
(cid:17)

dx,

card(X)

log ˆΛ(Ω) + H(λ, ˆλ)

x∈Ω

dx is the cross-entropy between the probability density
x∈S λ(x) dx. The ﬁrst line is the tower law of expectation. To
see the second line, note that we may sample X ∼ PP(λ) by ﬁrst sampling card(X) ∼ Poisson(Λ(Ω)), and then drawing
each element of X according to the probability density proportional to λ. The third line uses the Poisson expectation
Ecard(X) [card(X)] = Γ(Ω) and the fourth some simple algebra.
As an aside, we may therefore write the Kullback-Leibler divergence in a form resembling that for probability distributions:

λ(x)
Λ(Ω) log

ˆλ(x)
ˆΛ(Ω)

(cid:0)PP(f )(cid:13)(cid:13) PP(g)(cid:1) = EX∼PP(λ)
(cid:18)

(cid:90)

DKL

(cid:104)
(cid:105)
log pX∼PP(λ)(X) − log pX∼PP(ˆλ)(X)

(cid:19)

=

x∈Ω

f (x) log

f (x)
g(x)

+ g(x) − f (x)

dx.

A.2. Bayesian Decision Theory for the Expected Log Loss

To determine the intensity function which maximises the expected log likelihood we deﬁne the loss
(cid:96)(λ, λ(cid:48)) := Eni∼N (Bi),i=1,2,...,m|λ log p (N (Bi) = ni, i = 1, 2, . . . , m|λ(cid:48))

where N (Bi) is the random variable representing the number of points in the set Bi ⊆ Ω, Ω is the domain of the process

and we recall Λ(S) :=(cid:82)

x∈S λ(x) dx. It is well known that (Baddeley, 2007)

p(N (Bi) = ni, i = 1, 2, . . . , m|λ) =

Λ(Bi)ni

ni!

exp(−Λ(Ω)).

Bayesian decision theory considers the expected loss

i

(cid:89)

where the expectation is with respect to the posterior predictive distribution given the data D. Combining these expressions

and assuming without loss of generality that Ω =(cid:83)

(cid:34)

L(λ(cid:48)) = Eλ|D

Eni∼N (Bi),i=1,2,...,m|λ

(ni log Λ(cid:48)(Bi) − log(ni!) − Λ(cid:48)(Bi))

The optimal choice is Λ∗ := argmaxλ(cid:48) L(λ(cid:48)), so by stationarity

L(λ(cid:48)) := Eλ|D [(cid:96)(λ, λ(cid:48))] ,

i Bi yields

(cid:104)(cid:88)
(cid:2) Eni∼N (Bi)|λ [ni](cid:3)

i

λ∗(Bi) = Eλ|D

= Eλ|D [Λ(Bi)] ,

(cid:105)(cid:35)

.

and so λ∗ = Eλ|D[λ], the expectation of the posterior predictive distribution.

Fast Bayesian Permanental Processes

(a) λ0

(b) λ1

(c) λ2

(d) λ3

(e) λ4

Figure 6. Predictive distributions for the test problems of subsection 6.2.

0.00.51.01.52.02.53.0inputdomainΩ=[0,π]02040Poissonintensityλdatalocationsxitrueintensityλ(x)predictivemedian[0.1,0.9]pred.intervalpredictivesamples0.00.51.01.52.02.53.00500.00.51.01.52.02.53.0025500.00.51.01.52.02.53.0020400.00.51.01.52.02.53.0050A.3. Standard Laplace Approximations for the GP

Fast Bayesian Permanental Processes

Following e.g. (Rasmussen & Williams, 2006), assume that we are given an independent and identically distributed sample
{(xi, yi)}1≤i≤m, and the goal is to estimate p(y|x). Let the true joint in f = (f (xi))i, y = (y(xi))i be

log p(y, f|X, k) = log p(y|f ) + log p(f|X, k)

= log p(y|f ) − 1
2

f(cid:62)K−1f − 1
2

log |K| − m
2

log 2π,

where K = (k(xi), xj)ij and X = (x1, x2, . . . , xm). The Laplace approximation ﬁts a normal to the posterior,

log p(f|y, X) ≈ log N (f| ˆf , Q)

(f − ˆf )(cid:62)Q−1(f − ˆf ) − 1
2

= − 1
2
:= log q(f|y, X).

log |Q| − m
2

log 2π

ˆf and Q come from a second order approximation of the log posterior at its mode, i.e.

ˆf = argmax

f

= argmax

f

p(y|f , X)
p(y, f|X)

Q−1 = − ∂2

∂f ∂f(cid:62) log p(y, f|X)

(cid:12)(cid:12)(cid:12)(cid:12)f = ˆf

(cid:12)(cid:12)(cid:12)(cid:12)fi= ˆfi

= K−1 + W

Wii = − ∂2
∂f 2
i

log p(yi|fi)

Taylor expanding log p(y, f|X) at f = ˆf,

log p(y, f|X) ≈ log p(y, ˆf|X) − 1
2
= log p(y|f = ˆf ) − 1
2
:= log q(y, f|X)

(f − ˆf )(cid:62)Q−1(f − ˆf )
ˆf(cid:62)K−1 ˆf − 1
2

log |K| − m
2

log 2π − 1
2

(f − ˆf )(cid:62)Q−1(f − ˆf )

Now

log

So we get the approximate marginal likelihood
log Z := log p(y|X)

(cid:90)

(cid:90)

exp(− 1
2

x(cid:62)H−1x)dx =

m
2

log 2π +

log |H|

1
2

(17)

(18)

(19)

≈ log
q(y, f|X)df
= log p(y|f = ˆf ) − 1
2
= log p(y|f = ˆf ) − 1
2

ˆf(cid:62)K−1 ˆf − 1
2
ˆf(cid:62)K−1 ˆf − 1
2

log |K−1 + W|

log |K| − 1
2
log |I + KW|

This is a standard textbook approach (Rasmussen & Williams, 2006), but we can get the same approximation via

log p(y|X) ≈ log q(y, ˆf|X) − log q( ˆf|y, X),

since the right hand side is true for all f, not just ˆf. Hence we need only subtract the approximate log likelihoods as above.
By evaluating at ˆf, the second r.h.s. term in (17), vanishes immediately.

