Supplementary Material for Multilevel Clustering via Wasserstein Means

Nhat Ho 1 XuanLong Nguyen 1 Mikhail Yurochkin 1 Hung Hai Bui 2 Viet Huynh 3 Dinh Phung 3

Θ
on Θ.

k(cid:48)(cid:80)

p(cid:48)
iδθ(cid:48)

(cid:104)T, MG,G(cid:48)(cid:105)

(1)

P N,λ =

λkT 1
k

#P1.

Appendix A

In this appendix, we collect relevant information on the
Wasserstein metric and Wasserstein barycenter problem,
which were introduced in Section 2 in the paper. For any
Borel map g : Θ → Θ and probability measure G on
Θ, the push-forward measure of G through g, denoted by
´
g#G, is deﬁned by the condition that
f (y)d(g#G)(y) =

f (g(x))dG(x) for every continuous bounded function f

´

Θ

Wasserstein metric When G =

k(cid:80)

i=1

piδθi and G(cid:48) =

i

i=1

are discrete measures with ﬁnite support, i.e., k
and k(cid:48) are ﬁnite, the Wasserstein distance of order r be-
tween G and G(cid:48) can be represented as

: T 1k(cid:48) = p, T 1k = p(cid:48)(cid:111)

T∈Π(G,G(cid:48))

W r

where we have
Π(G, G(cid:48)) =

r (G, G(cid:48)) = min
(cid:110)
MG,G(cid:48) = (cid:8)(cid:107)θi − θ(cid:48)
j(cid:107)(cid:9)

T ∈ Rk×k(cid:48)

+

+

i,j

∈ Rk×k(cid:48)

1, . . . , p(cid:48)

such that p = (p1, . . . , pk)T and p(cid:48) = (p(cid:48)

k(cid:48))T ,
is the cost matrix, i.e.
matrix of pairwise distances of elements between G and G(cid:48),
and (cid:104)A, B(cid:105) = tr(AT B) is the Frobenius dot-product of ma-
trices. The optimal T ∈ Π(G, G(cid:48)) in optimization problem
(1) is called the optimal coupling of G and G(cid:48), representing
the optimal transport between these two measures. When
k = k(cid:48), the complexity of best algorithms for ﬁnding the
optimal transport is O(k3 log k). Currently, (Cuturi, 2013)
proposed a regularized version of (1) based on Sinkhorn
distance where the complexity of ﬁnding an approximation
of the optimal transport is O(k2). Due to its favorably fast

1Department of Statistics, University of Michigan, Ann Ar-
bor, USA. 2Adobe Research. 3Center for Pattern Recognition and
Data Analytics (PRaDA), Deakin University, Australia. Corre-
spondence to: Nhat Ho <minhnhat@umich.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

computation, throughout the paper we shall utilize Cuturi’s
algorithm to compute the Wasserstein distance between G
and G(cid:48) as well as their optimal transport in (1).

Wasserstein barycenter As introduced in Section 2.2 in
the paper, for any probability measures P1, P2, . . . , PN ∈
P2(Θ), their Wasserstein barycenter P N,λ is such that

P N,λ = arg min
P∈P2(Θ)

λiW 2

2 (P, Pi)

where λ ∈ ∆N denote weights associated with
P1, . . . , PN . According to (Agueh and Carlier, 2011),
PN,λ can be obtained as a solution to so-called multi-
marginal optimal transporation problem. In fact, if we de-
k as the measure preseving map from P1 to Pk, i.e.,
note T 1
k #P1, for any 1 ≤ k ≤ N, then
Pk = T 1

N(cid:88)

i=1

(cid:19)

(cid:18) N(cid:88)

k=1

Unfortunately, the forms of the maps T 1
k are analytically in-
tractable, especially if no special constraints on P1, . . . , PN
are imposed.
Recently, (Anderes et al., 2015) studied the Wasserstein
barycenters P N,λ when P1, P2, . . . , PN are ﬁnite discrete
measures and λ =
. They demonstrate
the following sharp result (cf. Theorem 2 in (Anderes et al.,
2015)) regarding the number of atoms of P N,λ
Theorem A.1. There exists a Wasserstein barycenter P N,λ

1/N, . . . , 1/N

(cid:18)

(cid:19)

such that supp(P N,λ) ≤ N(cid:80)

si − N + 1.

i=1

Therefore, when P1, . . . , PN are indeed ﬁnite discrete mea-
sures and the weights are uniform, the problem of ﬁnd-
ing Wasserstein barycenter P N,λ over the (computationally
large) space P2(Θ) is reduced to a search over a smaller
space Ol(Θ) where l =

si − N + 1.

N(cid:80)
Appendix B

i=1

In this appendix, we provide proofs for the remaining re-
sults in the paper. We start by giving a proof for the tran-

Supplementary Material for Multilevel Clustering via Wasserstein Means
S. Now, from the deﬁnition of πS
ˆ
ˆ

2 (X, πS (X))) =

E(W 2

2 (P, πS (P ))dQ(P )
W 2
(P,S)dQ(P )
(X,S))

d2
W2

(4)

=

= E(d2

W2

where the integrations in the above equations range over
P2(Θ). By combining (3) and (4), we would obtain that

EX∼Q(d2

W2

(X,S)) ≥ W 2

2 (Q, πS #Q).

(5)

is

(5),

and

straightforward

that
Therefore, we

From (2)
it
2 (Q, πS#Q).
EX∼Q(d(X, S)2) = W 2
achieve the conclusion of the lemma.
Lemma B.2. For any closed subset S ⊂ P2(Θ) and µ ∈
P2(P2(Θ)) with supp(µ) ⊆ S, there holds W 2
2 (Q, µ) ≥
2 (Q, πS #Q) for any Q ∈ P2(P2(Θ)).
W 2

Proof. Since supp(µ) ⊆ S, it is clear that W 2

2 (Q, µ) =

ˆ

W 2

2 (P, G)dπ(P, G).

inf

π∈Π(Q,µ)

P2(Θ)×S

Additionally, we have

ˆ

2 (P, G)dπ(P, G) ≥
W 2

=

ˆ
ˆ

(P,S)dπ(P, G)

(P,S)dQ(P )

d2
W2

d2
W2

= EX∼Q(d2
= W 2

2 (Q, πS #Q)

W2

(X, S))

where the last inequality is due to Lemma B.1 and the inte-
grations in the ﬁrst two terms range over P2(Θ) × S while
that in the ﬁnal term ranges over P2(Θ). Therefore, we
achieve the conclusion of the lemma.

G∈S W 2

2 (Q, P ) = min

sition from multilevel Wasserstein means objective func-
tion to objective function (4) in Section 3.1 in the paper.
All the notations in this appendix are similar to those in
the main text. For each closed subset S ⊂ P2(Θ), denote
the Voronoi region generated by S on the space P2(Θ) by
the collection of subsets {VP}P∈S, where VP := {Q ∈
P2(Θ) : W 2
2 (Q, G)}. We deﬁne the
projection mapping πS as: πS : P2(Θ) → S where
πS (Q) = P as Q ∈ VP . Note that, for any P1, P2 ∈ S
such that VP1 and VP2 share the boundary, the values of πS
at the elements in that boundary can be chosen to be either
P1 or P2. Now, we start with the following useful lemmas.
Lemma B.1. For any closed subset S on P2(Θ), if Q ∈
P2(P2(Θ)), then EX∼Q(d2
2 (Q, πS #Q)
where d2
P∈S W 2

(X,S)) = W 2

(X,S) = inf

W2
2 (X, P ).

W2

Proof. For any element π ∈ Π(Q, πS #Q):

ˆ

2 (P, G)dπ(P, G) ≥
W 2

=

ˆ
ˆ

d2
W2

d2
W2

(P,S)dπ(P, G)

(P,S)dQ(P )
(X,S))

W2

= EX∼Q(d2

ˆ

where the integrations in the ﬁrst two terms range over
P2(Θ) × S while that in the ﬁnal term ranges over P2(Θ).
Therefore, we obtain

2 (Q, πS #Q) = inf
W 2

W 2

2 (P, G)dπ(P, G)

P2(Θ)×S
≥ EX∼Q(d2

(X,S))

W2

(2)
where the inﬁmum in the ﬁrst equality ranges over all π ∈
Π(Q, πS #Q).
On the other hand, let g : P2(Θ) → P2(Θ) × S such that
g(P ) = (P, πS (P )) for all P ∈ P2(Θ). Additionally, let
µπS = g#Q, the push-forward measure of Q under map-
ping g. It is clear that µπS is a coupling between Q and
πS #Q. Under this construction, we obtain for any X ∼ Q
that

E(cid:0)W 2

2 (X, πS (X))(cid:1) =

ˆ

2 (P, G)dµπS (P, G)

W 2
ˆ

W 2

2 (P, G)dπ(P, G)

≥ inf

= W 2

2 (Q, πS #Q)

(3)

Equipped with Lemma B.1 and Lemma B.2, we are ready
to establish the equivalence between multilevel Wasserstein
means objective function (5) and objective function (4) in
Section 3.1 in the main text.
Lemma B.3. For any given positive integers m and M, we
have

A :=

inf

H∈EM (P2(Θ))

2 (H,
W 2

1
m

δGj )

m(cid:88)

j=1

m(cid:88)

j=1

where the inﬁmum in the second inequality ranges over all
π ∈ Π(Q, πS #Q) and the integrations range over P2(Θ)×

=

1
m

inf

H=(H1,...,HM )

d2
W2

(Gj, H) := B.

PROOF OF THEOREM 3.1 The proof of this theorem
is straightforward from the formulation of Algorithm 1. In
fact, for any Gj ∈ Ekj (Θ) and H = (H1, . . . , HM ), we
denote the function

f (G, H) =

W 2

2 (Gj, P j

n) +

d2
W2

(Gj, H)
m

m(cid:88)

j=1

where G = (G1, . . . , Gm). To obtain the conclusion of
this theorem, it is sufﬁcient to demonstrate for any t ≥ 0
that

f (G(t+1), H (t+1)) ≤ f (G(t), H (t)).

This inequality comes directly from f (G(t+1), H (t)) ≤
f (G(t), H (t)), which is due to the Wasserstein barycen-
for 1 ≤ j ≤ m, and
ter problems to obtain G(t+1)
f (G(t+1), H (t+1)) ≤ f (G(t+1), H (t)), which is due
to the optimization steps to achieve elements H (t+1)
of
H (t+1) as 1 ≤ u ≤ M. As a consequence, we achieve
the conclusion of the theorem.

u

j

PROOF OF THEOREM 4.1 To simplify notation, write

Ln =

inf

Gj∈Okj (Θ),
H∈EM (P2(Θ))

L0 =

inf

Gj∈Okj (Θ),
H∈EM (P2(Θ))

fn(G,H),

f (G,H).

For any  > 0, from the deﬁnition of L0, we can ﬁnd Gj ∈
Okj (Θ) and H ∈ EM (P(Θ)) such that

f (G,H)1/2 ≤ L1/2

0 + .

Therefore, we would have
n − L1/2
≤ L1/2
L1/2
≤ fn(G,H)1/2 − f (G,H)1/2 + 

n − f (G,H)1/2 + 

0

fn(G,H) − f (G,H)

fn(G,H)1/2 + f (G,H)1/2

+ 
2 (Gj, P j)|
|W 2
2 (Gj, P j
nj
W2(Gj, P j
nj ) + W2(Gj, P j)

) − W 2

+ 

W2(P j
nj

, P j) + .

=

j=1

≤ m(cid:88)
≤ m(cid:88)
0 ≥ m(cid:80)

j=1

where the second equality in the above display is due to
Lemma B.1 while the last inequality is from the fact that
πH #Q is a discrete probability measure in P2(P2(Θ))
with exactly M support points. Since the inequality in the
above display holds for any , it implies that B ≥ A. On
the other hand, from the formation of A, for any  > 0, we
also can ﬁnd H(cid:48) ∈ EM (P2(Θ)) such that

A ≥ W 2
≥ W 2
1
m

2 (H(cid:48),Q) − 
m(cid:88)
2 (Q, πH(cid:48)#Q) − 

(Gj, H(cid:48)) − 

d2
W2

=
≥ B − 

j=1

where H(cid:48) = supp(H(cid:48)), the second inequality is due to
Lemma B.2, and the third equality is due to Lemma B.1.
Therefore, it means that A ≥ B. We achieve the conclu-
sion of the lemma.
Proposition B.4. For any positive integer numbers m, M
and kj as 1 ≤ j ≤ m, we denote

m(cid:88)

i=1

m(cid:88)

j=1

W 2

2 (Gj, P j
nj

)

W 2

2 (Gj, P j
nj

)

C :=

inf

Gj∈Okj (Θ) ∀1≤j≤m,

m(cid:88)

H∈EM (P2(Θ))
2 (H,

1
m

i=1

δGi)

+ W 2

D :=

inf

Gj∈Okj (Θ) ∀1≤j≤m,

H=(H1,...,HM )

d2
W2

(Gj, H)
m
Then, we have C = D.

+

.

Supplementary Material for Multilevel Clustering via Wasserstein Means

Proof. Write Q =

1
m

δGj . From the deﬁnition of B,

for any  > 0, we can ﬁnd H such that

m(cid:80)
m(cid:88)

j=1

j=1

B ≥ 1
m

d2
W2

(Gj, H) − 
(X, H)) − 

= EX∼Q(d2
= W 2
≥ A − 

W2

2 (Q, πH #Q) − 

Proof. The proof of this proposition is a straightfor-
ward application of Lemma B.3.
Indeed, for each ﬁxed
(G1, . . . , Gm) the inﬁmum w.r.t to H in C leads to the
same inﬁmum w.r.t to H in D, according to Lemma B.3.
Now, by taking the inﬁmum w.r.t to (G1, . . . , Gm) on both
sides, we achieve the conclusion of the proposition.

In the remainder of the Supplement, we present the proofs
for all remaining theorems stated in the main text.

j=1

0 − m(cid:80)

, P j) − . Hence, |L1/2

By reversing the direction, we also obtain the inequality
n −
n − L1/2
L1/2
W2(P j
nj
, P j)| ≤  for any  > 0. Since P j ∈
L1/2
, P j) →
P2(Θ) for all 1 ≤ j ≤ m, we obtain that W2(P j
0 almost surely as nj → ∞ (see for example Theorem
6.9 in (Villani, 2009)). As a consequence, we obtain the
conclusion of the theorem.

W2(P j
nj

j=1

nj

Supplementary Material for Multilevel Clustering via Wasserstein Means

PROOF OF THEOREM 4.2 For any  > 0, we denote

(cid:26)

A() =

(cid:27)
Gi ∈ Oki(Θ),H ∈ EM (P(Θ)) :
d(G,H,F) ≥ 

.

Since Θ is a compact set, we also have Okj (Θ) and
EM (P2(Θ)) are compact for any 1 ≤ i ≤ m. As a con-
sequence, A() is also a compact set. For any (G,H) ∈
A(), by the deﬁnition of F we would have f (G,H) >
f (G0,H0) for any (G0,H0) ∈ F. Since A() is compact,
it leads to

inf

(G,H)∈A()

f (G,H) > f (G0,H0).

n

n

for any (G0,H0) ∈ F. From the formulation of fn
as in the proof of Theorem 4.1, we can verify that
lim
n → ∞. Combining this result with that of Theorem 4.1,
(G0,H0) ∈ F. Therefore, for any  > 0, as n is large

n→∞ f ((cid:98)G
, (cid:98)Hn) = lim
, (cid:98)Hn) almost surely as
, (cid:98)Hn) → f (G0,H0) as n → ∞ for any
, (cid:98)Hn,F) < . As a consequence,

n→∞ fn((cid:98)G
we obtain f ((cid:98)G
enough, we have d((cid:98)G

we achieve the conclusion regarding the consistency of the
mixing measures.

n

n

Appendix C

In this appendix, we provide details on the algorithm for
the Multilevel Wasserstein means with sharing (MWMS)
formulation (Algorithm 2). Recall the MWMS objective
function as follows

inf

SK ,Gj ,H∈BM,SK

W 2

2 (Gj, P j
nj

) +

d2
W2

(Gj, H)
m

where BM,SK =
supp(Gj) ⊆ SK ∀1 ≤ j ≤ m

(cid:27)

.

Gj ∈ OK(Θ), H = (H1, . . . , HM ) :

We make the following remarks regarding the initializa-
tions and updates of Algorithm 2:

(cid:26)

(cid:27)

(i) An efﬁcient way to initialize global set S(0)

K =
∈ Rd×K is to perform K-means on
1 , . . . , a(0)
a(0)
the whole data set Xj,i for 1 ≤ j ≤ m, 1 ≤ i ≤ nj;
are indeed the solutions of the fol-

(ii) The updates a(t+1)

K

j

lowing optimization problems

m(cid:88)
(cid:26)

j=1

(cid:26) m(cid:88)

l=1

inf
a(t)
j

m(cid:80)

(cid:27)

,

, H (t)
il

)

W 2

2 (G(t)

l

m

W 2

2 (G(t)

l

, P l

n) +

l=1

Algorithm 2 Multilevel Wasserstein Means with Sharing
(MWMS)

(cid:110)

Input: Data Xj,i, K, M.
Output: global set SK, local measures Gj, and elements
Hi of H.
Initialize S(0)
K =
H (0), and t = 0.
while S(t)
K , G(t)

have not converged do

, elements H (0)

a(0)
1 , . . . , a(0)

j , H (t)

(cid:111)

of

K

i

i

j , P j

n (cf. Appendix

j , H (t)
ij

.

i with h(t)

i,v as v-th column.

T u
i,v +

i,v.
U u

m(cid:80)

u=1

(cid:80)

v(cid:54)=i

T u
i,vXu,v+

1. Update global set S(t)
K :
for j = 1 to m do
ij ← arg min
j , H (t)
u ).
1≤u≤M
T j ← optimal coupling of G(t)
A).
U j ← optimal coupling of G(t)

2 (G(t)

W 2

end for
for i = 1 to M do

m(cid:80)

i ← atoms of H (t)
h(t)
ni(cid:80)
end for
for i = 1 to K do
m(cid:80)
(cid:19)

mD ← m
i ←
m(cid:80)
(cid:80)
a(t+1)

ni(cid:80)

(cid:18)

u=1

u=1

v=1

m

i,vh(t)
U u
ju,v

v=1
/mD.

v
u=1
end for
2. Update G(t)
j
for j = 1 to m do
j ←
G(t+1)

for 1 ≤ j ≤ m:

arg min

Gj :supp(Gj )≡S (t+1)

K

W 2

2 (Gj, P j
nj

)

)/m.

2 (Gj, H (t)
ij

+W 2
end for
3. Update H (t)
4. t ← t + 1.

i

end while

for 1 ≤ i ≤ M as Algorithm 1.

nj(cid:88)
m(cid:88)
(cid:88)
m(cid:88)

u=1

v=1

m

+

j,v(cid:107)a(t)
T u

j − Xu,v(cid:107)2

j,v(cid:107)a(t)
U u

j − h(t)

ij ,v||2.

u=1

v

where T j is an optimal coupling of G(t)
j , P j
n and U j
is an optimal coupling of G(t)
. By taking the
ﬁrst order derivative of the above function with respect
to a(t)
as the closed form
minimum of that function;

j , we quickly achieve a(t+1)

j , H (t)
ij

j

(iii) Updating the local weights of G(t+1)

j

updating G(t+1)
stem from S(t+1)

as the atoms of G(t+1)
.

j

j

K

is equivalent to
are known to

Now, similar to Theorem 3.1 in the main text, we also have
the following theoretical guarantee regarding the behavior
of Algorithm 2 as follows

Theorem C.1. Algorithm 2 monotonically decreases the
objective function of the MWMS formulation.

j,v

(cid:88)
E ≥ m(cid:88)
m(cid:88)
j,v(cid:107)a(t+1)
m(cid:88)

u=1
+ U u
≥ m

j=1

j

≥ m

j

mT u

j,v(cid:107)a(t+1)
− h(t)
iu,v(cid:107)2
2 (G(t)(cid:48)
W 2

, P j

j

n) +

2 (G(t)(cid:48)
W 2

j

, P j

n) +

j=1

= mf (G(cid:48)(t), H (t))

− Xu,v(cid:107)2

m(cid:88)
m(cid:88)

j=1

j=1

2 (G(t)(cid:48)
W 2

j

, H (t)
ij

)

(G(t)(cid:48)

j

d2
W2

, H (t))

1

m ), G(t)(cid:48)

, . . . , G(t)(cid:48)
j by the elements of S(t+1)

where G(cid:48)(t) = (G(t)(cid:48)
are formed by re-
placing the atoms of G(t)
, noting
that supp(G(t)(cid:48)
as 1 ≤ j ≤ m, and the second
) ⊆ S (t+1)
inequality comes directly from the deﬁnition of Wasser-
stein distance. Hence, we obtain

K

K

j

j

f (G(t), H (t)) ≥ f (G(cid:48)(t), H (t)).

(6)

From the formation of G(t+1)

as 1 ≤ j ≤ m, we get

m(cid:88)

j

, H (t)) ≤ m(cid:88)

d2
W2

(G(t+1)

j

(G(t)(cid:48)

j

d2
W2

, H (t)).

j=1

j=1

Thus, it leads to

f (G(cid:48)(t), H (t)) ≥ f (G(t+1), H (t)).

(7)

Finally, from the deﬁnition of H (t+1)
have

1

, . . . , H (t+1)

M , we

f (G(t+1), H (t)) ≥ f (G(t+1), H (t+1)).

(8)

Supplementary Material for Multilevel Clustering via Wasserstein Means

which is equivalent to ﬁnd a(t)
j

to optimize

Therefore, the update of a(t+1)

from Algorithm 2 leads to

i

Proof. The proof is quite similar to the proof of Theorem
3.1. In fact, recall from the proof of Theorem 3.1 that for
any Gj ∈ Ekj (Θ) and H = (H1, . . . , HM ) we denote the
function

m(cid:88)

j=1

f (G, H) =

W 2

2 (Gj, P j

n) +

d2
W2

(Gj, H)
m

By combining (6), (7), and (8), we arrive at the conclusion
of the theorem.

Appendix D

where G = (G1, . . . , Gm). Now it is sufﬁcient to demon-
strate for any t ≥ 0 that

f (G(t+1), H (t+1)) ≤ f (G(t), H (t)).

where the formulation of f is similar as in the proof of
Theorem 3.1. Indeed, by the deﬁnition of Wasserstein dis-
tances, we have

In this appendix, we offer details on the data generation
processes utilized in the simulation studies presented in
Section 5 in the main text. The notions of m, n, d, M are
given in the main text. Let Ki be the number of supporting
atoms of Hi and kj the number of atoms of Gj. For any
d ≥ 1, we denote 1d to be d dimensional vector with all
components to be 1. Furthermore, Id is an identity matrix
with d dimensions.

m(cid:88)

(cid:88)

u=1

j,v

mT u

j,v(cid:107)a(t)

j − Xu,v(cid:107)2 + U u

E = mf (G(t), H (t)) =
iu,v(cid:107)2.

j − h(t)

j,v(cid:107)a(t)

m(cid:88)

j=1

W :=

1
m

W2( ˆGj, Gj) + dM( ˆH, H)

Comparison metric (Wasserstein distance to truth)

Supplementary Material for Multilevel Clustering via Wasserstein Means

For each group j = 1, . . . , m generate local measures and
data as follows:
pick cluster label ˜zj ∼ Unif({1, . . . , M}).
select shared atoms sj = {k : zk = ˜zj}.
weights of atoms psj

∼ Dir(1|sj|); Gj :=

(cid:88)

piδθi.

i∈sj

data mean µi ∼ Gj, i = 1, . . . , nj.
observation Xj,i ∼ N (µi,Id).
For the case of non-constrained variances, the variance to
generate atoms θi of Gj where i ∈ sj is set to be propor-
tional to global cluster label ˜zj assigned to Gj.
Three-stage K-means First, we estimate Gj for each
group 1 ≤ j ≤ m by using K-means algorithm with kj
clusters. Then, we cluster labels using K-means algorithm
with M clusters based on the collection of all atoms of Gjs.
Finally, we estimate the atoms of each Hi via K-means
algorithm with exactly L clusters for each group of local
atoms. Here, L is some given threshold being used in Algo-
rithm 1 in Section 3.1 in the main text to speed up the com-
putation (see ﬁnal remark regarding Algorithm 1 in Section
3.1). The three-stage K-means algorithm is summarized in
Algorithm 3.

Algorithm 3 Three-stage K-means

Input: Data Xj,i, kj, M, L.
Output: local measures Gj and global elements Hi of
H.
Stage 1
for j = 1 to m do

Gj ← kj clusters of group j with K-means (atoms as
centroids and weights as label frequencies).

end for
C ← collection of all atoms of Gj.
Stage 2
{D1, . . . , DM} ← M clusters from K-means on C.
Stage 3
for i = 1 to M do

Hi ← L clusters of Di with K-means (atoms as cen-
troids and weights as label frequencies).

end for

where ˆH := { ˆH1, . . . , ˆHM}, H := {H1, . . . , HM} and
dM( ˆH, H) is a minimum-matching distance (Tang et al.,
2014; Nguyen, 2015):

dM( ˆH, H) := max{d( ˆH, H), d(H, ˆH)}

where

d( ˆH, H) := max
1≤i≤M

min
1≤j≤M

W2(Hi, ˆHj).

Multilevel Wasserstein means setting The global clus-
ters are generated as follows:
means for atoms µi := 5(i − 1), i = 1, . . . , M.
atoms of Hi : φij ∼ N (µi1d,Id), j = 1, . . . , Ki.
weights of atoms: πi ∼ Dir(1Ki).

Ki(cid:88)

Let Hi :=

πijδφij .

j=1

For each group j = 1, . . . , m, generate local measures and
data as follows:

pick cluster label zj ∼ Unif({1, . . . , M}).
mean for atoms : τji ∼ Hzj , i = 1, . . . , kj.
atoms of Gj : θji ∼ N (τji,Id), i = 1, . . . , kj.
weights of atoms pj ∼ Dir(1kj ).

kj(cid:88)

Let Gj :=

pjiδθji.

i=1

data mean µi ∼ Gj, i = 1, . . . , nj.
observation Xj,i ∼ N (µi,Id).

For the case of non-constrained variances, the variance to
generate atoms θji of Gj is set to be proportional to global
cluster label zj assigned to Gj.
Multilevel Wasserstein means with sharing setting
The global clusters are generated as follows:
means for atoms µi := 5(i − 1), i = 1, . . . , M.
atoms of Hi : φij ∼ N (µi1d,Id), j = 1, . . . , Ki.
weights of atoms πi ∼ Dir(1Ki).

Ki(cid:88)

Let Hi :=

πijδφij .

j=1

For each shared atom k = 1, . . . , K:

pick cluster label zk ∼ Unif({1, . . . , M}).
mean for atoms : τk ∼ Hzk .
atoms of SK : θk ∼ N (τk,Id).

Supplementary Material for Multilevel Clustering via Wasserstein Means

References
M. Agueh and G. Carlier. Barycenters in the wasserstein
space. SIAM Journal on Mathematical Analysis, 43:
904–924, 2011.

E. Anderes, S. Borgwardt, and J. Miller. Discrete wasser-
stein barycenters: optimal transport for discrete data.
http://arxiv.org/abs/1507.07218, 2015.

M. Cuturi. Sinkhorn distances: lightspeed computation of
optimal transport. Advances in Neural Information Pro-
cessing Systems 26, 2013.

X. Nguyen. Posterior contraction of the population poly-

tope in ﬁnite admixture models. Bernoulli, 21:618–646,
2015.

Jian Tang, Zhaoshi Meng, Xuanlong Nguyen, Qiaozhu
Mei, and Ming Zhang. Understanding the limiting fac-
tors of topic modeling via posterior contraction analysis.
In Proceedings of The 31st International Conference on
Machine Learning, pages 190–198. ACM, 2014.

C. Villani.

Optimal Transport:

Old and New.
Grundlehren der Mathematischen Wissenschaften [Fun-
damental Principles of Mathemtical Sciences]. Springer,
Berlin, 2009.

