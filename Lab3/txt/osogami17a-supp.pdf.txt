Bidirectional Learning for Time-series Models with Hidden Units

A. Supplementary Materials for Bidirectional Learning for Time-series Models with Hidden

Units

Here, we derive speciﬁc learning rules suggested by (27)-(28) as well as those with approximation with (29). These learning
rule can be derived in a way similar to the learning rules (18)-(22) are derived from (17). We also provide some of the
details, which are omitted in the derivation of (18)-(22).
The learning rules for U and Z are derived from (27)-(28) as follows:

U[d] ← U[d] + η log pθ(x[t]|x[<t], h[<t])

α[s−1] (h[s] − (cid:104)H[s](cid:105)φ)(cid:62)

Z[d] ← Z[d] + η log pθ(x[t]|x[<t], h[<t])

β[s−1] (h[s] − (cid:104)H[s](cid:105)φ)(cid:62)

U[δ] ← U[δ] + η log pθ(x[t]|x[<t], h[<t])

x[s−δ] (h[s] − (cid:104)H[s](cid:105)φ)(cid:62)

Z[δ] ← Z[δ] + η log pθ(x[t]|x[<t], h[<t])

h[s−δ] (h[s] − (cid:104)H[s](cid:105)φ)(cid:62)

s=(cid:96)

t−1(cid:88)
t−1(cid:88)
t−1(cid:88)
t−1(cid:88)

s=(cid:96)

s=(cid:96)

for 1 ≤ δ < d, where (cid:104)H[s](cid:105)φ denotes the expected values of h[s] with respect to the conditional distribution given by the
following pφ:

s=(cid:96)

pφ(h[s]|x[<s], h[<s]) =

1

Z(cid:48) exp(−Eφ(h[s]|x[<s], h[<s]))

(36)

for any binary vectors h[s], where Z(cid:48) is a normalization factor for the probabilities to sum up to one, and

Eφ(h[s]|x[<s], h[<s]) = − d−1(cid:88)

(x[s−δ])(cid:62)U[δ] h[s] − d−1(cid:88)

δ=1

δ=1

(h[s−δ])(cid:62)Z[δ] h[s] − (α[s−1])(cid:62) U[d]h[s] − (β[s−1])(cid:62) Z[d]h[s].

(32)

(33)

(34)

(35)

(37)

(38)

(39)

(40)

(41)

(42)

(cid:88)

j∈H

(cid:89)

j∈H

The energy in (37) can be decomposed into the energy associated with each hidden unit j as follows:

Eφ(h[s]|x[<s], h[<s]) =

Eφ,j(h[s]

j |x[<s], h[<s])

where H denotes the set of hidden units, and

j |x[<s], h[<s]) = − d−1(cid:88)

Eφ,j(h[s]

j − d−1(cid:88)

δ=1

δ=1

(x[s−δ])(cid:62)U[δ]

:,j h[s]

(h[s−δ])(cid:62)Z[δ]

:,j h[s]

j − (α[s−1])(cid:62)U[d]

:,j h[s]

j − (β[s−1])(cid:62)Z[d]

:,j h[s]
j ,

where U:,j denotes a column vector corresponding to the j-th column of U, and Z:,j is deﬁned analogously.
Then (36) can be expressed as

pφ(h[s]|x[<s], h[<s]) =

pφ,j(h[s]

j |x[<s], h[<s]),

where

pφ,j(h[s]

j |x[<s], h[<s]) =

=

exp(−Eφ,j(h[s]

exp(−Eφ,j(h[s]

j |x[<s], h[<s]))
j = 0|x[<s], h[<s])) + exp(−Eφ,j(h[s]

j = 1|x[<s], h[<s]))

exp(−Eφ,j(h[s]
1 + exp(−Eφ,j(h[s]

j |x[<s], h[<s]))
j = 1|x[<s], h[<s]))

.

Bidirectional Learning for Time-series Models with Hidden Units

The j-th element of (cid:104)H[s](cid:105)φ is then given by

(cid:104)H [s]

j (cid:105)φ = pφ,j(h[s]

j = 1|x[<s], h[<s])

(43)

In (32)-(35), the value of (cid:104)H[s](cid:105)φ is computed with the latest values of φ. Let φ[t−1] be the value of φ immediately before
step t. With the recursive computation of (29), the learning rules of (32)-(35) are approximated with the following learning
rules:

U[d] ← U[d] + η (1 − γ) log pθ(x[t]|x[<t], h[<t])

γt−1−s α[s−1] (h[s] − (cid:104)H[s](cid:105)φ[s−1])(cid:62)

Z[d] ← Z[d] + η (1 − γ) log pθ(x[t]|x[<t], h[<t])

γt−1−s β[s−1] (h[s] − (cid:104)H[s](cid:105)φ[s−1])(cid:62)

U[δ] ← U[δ] + η (1 − γ) log pθ(x[t]|x[<t], h[<t])

γt−1−s x[s−δ] (h[s] − (cid:104)H[s](cid:105)φ[s−1])(cid:62)

Z[δ] ← Z[δ] + η (1 − γ) log pθ(x[t]|x[<t], h[<t])

γt−1−s h[s−δ] (h[s] − (cid:104)H[s](cid:105)φ[s−1])(cid:62)

s=(cid:96)

t−1(cid:88)
t−1(cid:88)
t−1(cid:88)
t−1(cid:88)

s=(cid:96)

s=(cid:96)

for 1 ≤ δ < d, where the quantity such as

t−1 ≡ t−1(cid:88)

G(cid:48)

s=(cid:96)

γt−1−s α[s−1] (h[s] − (cid:104)H[s](cid:105)φ[s−1])(cid:62)

can be computed recursively as

s=(cid:96)

t ← γ G(cid:48)
G(cid:48)

t−1 + (1 − γ) α[t−1] (h[t] − (cid:104)H[t](cid:105)φ[t−1])(cid:62).

One may consider real-valued units as well (Dasgupta & Osogami, 2017; Osogami, 2016). For example, each of x[t]
h[t]
j may have a Gaussian distribution with the following density:

i and

pθ,i(x[t]

i |x[<t], h[<t]) =

pφ,i(h[t]

j |x[<t], h[<t]) =

(cid:0)x[t]
(cid:0)h[t]

(cid:18)
(cid:18)

−

−

exp

exp

i − Eθ,i(x[t]

j − Eφ,j(h[t]

i = 1|x[<t], h[<t])(cid:1)2
(cid:19)
j = 1|x[<t], h[<t])(cid:1)2
(cid:19)

2 σ2
i

,

2 σ2
j

1(cid:112)2 π σ2
1(cid:113)

i

2 π σ2
j

where σ2

j are variance parameters, Eφ,j is given by (39), and Eθ,i(x[t]

i = 1|x[<t], h[<t]) is given by

i and σ2

i = 1|x[<s], h[<s]) = −bi − d−1(cid:88)

Eθ,i(x[s]

:,i − d−1(cid:88)

(x[s−δ])(cid:62)W[δ]

(h[s−δ])(cid:62)V[δ]

:,i − (α[s−1])(cid:62)W[d]

:,i − (β[s−1])(cid:62)V[d]
:,i .

δ=1

δ=1

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

