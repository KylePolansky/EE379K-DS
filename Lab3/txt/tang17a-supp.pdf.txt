Gradient Projection Iterative Sketch for large-scale constrained Least-squares

(Supplementary materials)

Junqi Tang 1 Mohammad Golbabaee 1 Mike E. Davies 1

(cid:63), we have:

LS solution xt
(cid:107)rt
i+1(cid:107)2 = sup
v∈Ct∩Bd
≤ sup
v∈Ct∩Bd

(cid:63) − η∇f (xi)) + ηvT∇f (xt
(cid:63)) − ηvT (∇f (xi) − ∇f (xt

(cid:63) − η∇f (xi))(cid:9)

(cid:8)vT (xi − xt
(cid:8)vT (xi − xt
(cid:8)vT (xi − xt
(cid:9)
(cid:8)vT (I − ηAT ST SA)rt
(cid:8)vT (I − ηAT ST SA)u(cid:9)(cid:107)rt
(cid:8)vT (I − ηAT ST SA)u(cid:9)(cid:107)rt

i(cid:107)2,

i

i(cid:107)2

(cid:63))(cid:9)
(cid:63)))(cid:9)

= sup

v∈Ct∩Bd

= sup
v∈Ct∩Bd
≤ sup
≤ sup
u,v∈Bd

u,v∈Ct∩Bd

1. Supplementary materials
1.1. the proof for Theorem 1

Proof. At ﬁrst we denote the underlying cost function of
GPIS as ft(x):
for t = 0, we have the cost function of the classical sketch
(CS):

ft(x) :=

(cid:107)Sy − SAx(cid:107)2
2,

1
2

(1)

for t = 1, 2, ..., N we have the the cost function of Iterative
Hessian Sketch (IHS):

ft(x) =

1
2

(cid:107)St+1A(x − xt)(cid:107)2

2 − mxT AT (y − Axt), (2)

and then we denote the optimal solution of ft constrained
to set K as xt

i+1(cid:107)2 = (cid:107)xt

(cid:63)(cid:107)2 have:

(cid:63) and (cid:107)rt

i+1 − xt

i+1 − xt

(cid:63)(cid:107)2 = (cid:107)PK(xt

i − η∇f (xi)) − xt

(cid:107)rt
i+1(cid:107)2 = (cid:107)xt
(cid:63)(cid:107)2
(3)
then we denote cone Ct to be the smallest close cone at xt
containing the set K − xt
(cid:63)
(cid:63), again because of the distance
preservation of translation by Lemma 6.3 of (Oymak et al.,
2015), we have:

(cid:107)rt
i+1(cid:107)2 = (cid:107)PK−xt
= sup

(cid:63)

v∈Ct∩Bd

i − η∇f (xi) − xt
(xt

(cid:8)vT (xi − xt

(cid:63) − µ∇f (xi))(cid:9) ,

(cid:63))(cid:107)2

(4)

then because of the optimality condition on the constrained

1Institute of Digital Communications,

Edinbuurgh, EH9 3JL, UK. Correspondence to:
<J.Tang@ed.ac.uk>.

the University of
Junqi Tang

We denote:

αt = sup
u,v∈Bd

vT (I − ηAT ST SA)u,

then by recursive subsitution we have:
0(cid:107)2,
t(cid:107)rt

(cid:107)rt
i+1(cid:107)2 ≤ αi

and suppose we run GPIHS inner loop kt time, we have:

kt+1(cid:107)2 ≤ {αt}kt (cid:107)rt
(cid:107)rt
(cid:115)
and we transfer it in terms of A-norm:

0(cid:107)2,

(cid:107)rt
kt+1(cid:107)A ≤ {αt}kt

(cid:107)rt
0(cid:107)A.

L
µ

From the main theorems of the Classical sketch (Pilanci &
Wainwright, 2015) and Iterative Hessian Sketch (Pilanci &
Wainwright, 2016) we have following relationships:

(cid:107)x0

(cid:63) − x(cid:63)(cid:107)A ≤ 2ρ0(cid:107)Ax(cid:63) − y(cid:107)2 = 2ρ0(cid:107)e(cid:107)2,

and,

(cid:63) − x(cid:63)(cid:107)A ≤ ρt(cid:107)xt
Then by triangle inequality we have:

(cid:107)xt

0 − x(cid:63)(cid:107)A.

(cid:107)x1

0 − x(cid:63)(cid:107)A ≤ (cid:107)x1

0 − x0

(cid:63)(cid:107)A + 2ρ0(cid:107)e(cid:107)2,

(5)

(6)

(7)

(8)

(9)

(10)

(11)

(12)

(13)

Supplementary materials. Proceedings of the 34 th Interna-
tional Conference on Machine Learning, Sydney, Australia, 2017.
JMLR: W&CP. Copyright 2017 by the author(s).

and,
(cid:107)xt+1

0 − x(cid:63)(cid:107)A ≤ (cid:107)xt+1

0 − xt

(cid:63)(cid:107)A + ρt(cid:107)xt

0 − x(cid:63)(cid:107)A.

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

(cid:107)x1

0 − x(cid:63)(cid:107)A ≤ (cid:107)x1

Then for t = 0 we can have:
(cid:115)
(cid:63)(cid:107)A + 2ρ0(cid:107)e(cid:107)2
0 − x0
0 − x0
L
µ

≤ {αt}kt

(cid:107)x0

(cid:63)(cid:107)A + 2ρ0(cid:107)e(cid:107)2,

for t = 1, 2, ..., N we have:

(14)

0 − x(cid:63)(cid:107)A
(cid:107)xt
(cid:115)
(cid:63) (cid:107)A + ρt(cid:107)xt−1
0 − xt−1
≤ (cid:107)xt
0 − x(cid:63)(cid:107)A
0 − xt−1
(cid:63) (cid:107)A
≤ {αt}kt
L
µ
(cid:115)
(cid:33)
(cid:32)
+ ρt(cid:107)xt−1
≤
{αt}kt

(cid:107)xt−1
0 − x(cid:63)(cid:107)A

(1 + ρt)

(cid:41)

(cid:40)

+ ρt

L
µ

(cid:107)xt−1

0 − x(cid:63)(cid:107)A,

The last inequality holds because:

(15)

(cid:107)xt−1

0 − x(cid:63)

fN−1

(cid:107)A ≤ (cid:107)xt−1

(cid:63) − x(cid:63)(cid:107)A

Then we denote:

t = {αt}kt
ρ(cid:63)

and do recursive substitution we can have:

(16)

≤ {1 + ρt}(cid:107)xt−1

0 − x(cid:63)(cid:107)A + (cid:107)xt−1
0 − x(cid:63)(cid:107)A,
(cid:115)
(cid:33)

(cid:32)
(cid:40) N(cid:89)

L
µ

(cid:41)

(1 + ρt)

+ ρt

(17)

then chain it. For all the sketched objective function ft(x) ,
t = 0, 1, ..., N, and any pair of vectors x, x(cid:48) ∈ K we have:
ft(x) − ft(x(cid:48))− < (cid:79)ft(x(cid:48)), x − x(cid:48) >= (cid:107)StA(x − x(cid:48))(cid:107)2
2
(23)
If we set x(cid:48) = xt
(cid:63), by ﬁrst order optimality condition we
immediately have:

(cid:26)

=

≥

ft(x) − ft(xt
(cid:63)) ≥ (cid:107)StA(x − xt
(cid:107)St A(x − xt
(cid:107)A(x − xt
(cid:63))
(cid:63))(cid:107)2
(cid:107)A(x − xt
(cid:107)Stv(cid:107)2

(cid:63))(cid:107)2
(cid:63))(cid:107)2(cid:107)2
(cid:63)(cid:107)2
A,

(cid:107)x − xt

inf

2

2

(cid:27)
(cid:112)ft(x) − ft(xt

2

(cid:63))

v∈range(A)∩Sn−1

so we have:

(cid:107)x − xt

(cid:63)(cid:107)A ≤

inf v∈range(A)∩Sn−1 (cid:107)Stv(cid:107)2

,

(25)

(24)

From the convergence theory in (Beck & Teboulle, 2009)
which the authors in their Remark 2.1 have stated to hold
for convex constrained sets, for GPIS inner iterates we
have:

ft(xk) − ft(xt

(cid:63)) ≤ βLR supv∈range(A)∩Sn−1 (cid:107)Stv(cid:107)2

2

,

2k

and for Acc-GPIS inner loop we have:

ft(xk) − ft(xt

(cid:63)) ≤ 2βLR supv∈range(A)∩Sn−1 (cid:107)Stv(cid:107)2

2

(26)

,
(27)

(28)

(29)

(cid:107)xt

0 − x(cid:63)(cid:107)A ≤

ρ(cid:63)
t

(cid:107)x1

0 − x(cid:63)(cid:107)A.

(18)

t=1

hence for GPIS:

hence we ﬁnish the proof of Theorem 1.

1.2. The proofs for Theorem 2 and 3

Proof. From the theory of the Classical sketch and Iterative
Hessian Sketch we have following relationships:
(cid:63) − x(cid:63)(cid:107)A ≤ 2ρ0(cid:107)Ax(cid:63) − y(cid:107)2 = 2ρ0(cid:107)e(cid:107)2,

(cid:107)x0

(19)

(cid:107)xt+1

0 − xt

(cid:63)(cid:107)A ≤

for Acc-GPIS,

(cid:107)xt+1

0 − xt

(cid:63)(cid:107)A ≤

(k + 1)2

(cid:114)

βLσtR

2k

,

(cid:115)

2βLσtR
(k + 1)2 ,

and,

(cid:63) − x(cid:63)(cid:107)A ≤ ρt(cid:107)xt
Then by triangle inequality we have:

(cid:107)xt

0 − x(cid:63)(cid:107)A.

(cid:107)x1

0 − x(cid:63)(cid:107)A ≤ (cid:107)x1

0 − x0

(cid:63)(cid:107)A + 2ρ0(cid:107)e(cid:107)2,

and,
(cid:107)xt+1

0 − x(cid:63)(cid:107)A ≤ (cid:107)xt+1

0 − xt

(cid:63)(cid:107)A + ρt(cid:107)xt

0 − x(cid:63)(cid:107)A.

(20)

(21)

(22)

Then by simply towering the inequalities we shall obtain
the desired results in Theorem 2 and 3.

1.3. The proofs for quantitative bounds of αt, ρt and σt

for Gaussian sketches

To prove the results in Proposition 1, 2 and 3 we need the
following concentration lemmas as pillars:
Lemma 1. For any g ∈ Rd, we have:

(cid:26)

(cid:27)

The remaining task of this proof is just bound the term
(cid:107)xt+1
(cid:63)(cid:107)A for both GPIS and Acc-GPIS algorithm and

0 − xt

sup

v∈C∩Bd

vT g = max

0,

sup

u∈C∩Sd−1

uT g

(30)

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

Proof. By the deﬁnition of cone projection operator we
have:

vT g = (cid:107)PC(g)(cid:107)2 ≥ 0

(31)

sup

v∈C∩Bd

if supv∈C∩Bd vT g > 0:

sup

vT g = sup

v∈C∩Bd

v∈C∩Bd
and meanwhile since C ∩ Sd−1 ∈ C ∩ Bd we have:

u∈C∩Sd−1

(cid:107)v(cid:107)2

vT g
(cid:107)v(cid:107)2

≤ sup

uT g, (32)

Proof. This Lemma follows the result of the simpliﬁed
form of the Gordon’s Lemma [Lemma 6.7](Oymak et al.,
2015):

≥ √

(cid:107)SAv(cid:107)2 ≥ (bm − W(AC ∩ Sn−1) − θ)(cid:107)Av(cid:107)2
µ(bm − W(AC ∩ Sn−1) − θ)(cid:107)v(cid:107)2
(cid:107)SAv(cid:107)2 ≤ (bm + W(AC ∩ Sn−1) + θ)(cid:107)Av(cid:107)2
L(bm + W(AC ∩ Sn−1) + θ)(cid:107)v(cid:107)2

√

≤

(33)

1.3.1. THE PROOF FOR PROPOSITION 1

Proof. Let’s mark out the feasible region of the step-size η:

sup

v∈C∩Bd

vT g ≥ sup

u∈C∩Sd−1

uT g,

hence we have:

sup

v∈C∩Bd

vT g = sup

u∈C∩Sd−1

uT g,

(34)

Lemma 2. If supu,v∈C∩Bd vT M u > 0, we have:

sup

u,v∈C∩Bd

vT M u =

sup

u,v∈C∩Sd−1

vT M u

(35)

Proof. Since u, v ∈ C ∩ Bd, (cid:107)u(cid:107)2 and (cid:107)v(cid:107)2 are both less
than or equal to 1, we can have the following upper bound:

sup

u,v∈C∩Bd

vT M u =
≤

)(cid:107)v(cid:107)2(cid:107)u(cid:107)2

vT M u
(cid:107)v(cid:107)2(cid:107)u(cid:107)2
vT M u,

sup

u,v∈C∩Bd

(

sup

u,v∈C∩Sd−1

and meanwhile since C ∩ Sd−1 ∈ C ∩ Bd we have:

vT M u ≥

sup

u,v∈C∩Bd

sup

u,v∈C∩Sd−1

vT M u,

(36)

hence we have:

sup

u,v∈C∩Bd

vT M u =

sup

u,v∈C∩Sd−1

vT M u

(37)

Lemma 3. If the entries of the sketching matrix S is i.i.d
drawn from Normal distribution and v ∈ C, we have:

µ(bm − W − θ)(cid:107)v(cid:107)2,

(38)

with probability at least 1 − e− θ2
√
m, W := W(AC ∩ Sn−1))

L(bm + W + θ)(cid:107)v(cid:107)2,
√

2 . (bm =

(39)
2 ) ≈

)

2 Γ( m+1
2
Γ( m

(cid:107)SAv(cid:107)2 ≥ √
√

(cid:107)SAv(cid:107)2 ≤

α(η, StA)
=
sup
u,v∈Bd
≥ sup
v∈Bd
= sup
v∈Bd
≥ sup
v∈Bd

vT (I − ηAT ST SA)u
vT (I − ηAT ST SA)v
((cid:107)v(cid:107)2
((1 − ηL(bm +
so if we choose a step size η ≤
√
1
L(bm+
sure that with probability 1 − e− (θ−)2
α(η, StA) > 0 and the Lemma 2 become applicable:

2 − η(cid:107)SAv(cid:107)2
2)
√
d + θ − )2)(cid:107)v(cid:107)2
2),

2

d+θ)2 we can en-
( > 0) we have

=

=

=

=

≤

α(η, StA)
sup
u,v∈Bd
sup

u,v∈Sd−1

vT (I − ηAT ST SA)u
vT (I − ηAT ST SA)u

1
4

sup

[(u + v)T (I − ηAT ST SA)(u + v)

sup

u,v∈Sd−1
−(u − v)T (I − ηAT ST SA)(u − v)]
2 − η(cid:107)SA(u + v)(cid:107)2

[(cid:107)u + v(cid:107)2
1
u,v∈Sd−1
4
2 + η(cid:107)SA(u − v)(cid:107)2
−(cid:107)u − v(cid:107)2
2]
√
[(1 − ηµ(bm −
d − θ)2)(cid:107)u + v(cid:107)2
1
4
√

u,v∈Sd−1

sup

2

2

d + θ)2 − 1)(cid:107)u − v(cid:107)2
2]

+(ηL(bm +

The last line of inquality holds with probability at least 1−
2 according to Lemma 3. Then since we have set η ≤
2e− θ2
d+θ+)2 , and meanwhile notice the fact that (cid:107)u +
√
1

L(bm+

v(cid:107)2

2 ≤ 4 we have:

α(η, StA)
1
4

sup

≤
≤ (1 − ηµ(bm −

(1 − ηµ(bm −
d − θ)2)

u,v∈Sd−1

√

√

d − θ)2(cid:107)u + v(cid:107)2

2

Then let  → 0, we shall get the result shown in Proposition
1.

1.3.2. THE PROOF FOR PROPOSITION 2

Proof. Recall that ρt is deﬁned as:

ρ(St, A) =

supv∈AC∩Sn−1 vT ( 1
1

inf v∈AC∩Sn−1

m StT
m(cid:107)Stv(cid:107)2

2

St − I)z

,

(41)

we start by lower-bounding the denominator, by simpliﬁed
Gordon’s lemma [Lemma 6.7](Oymak et al., 2015) we di-
rectly have:

with probability at least (1 − e− θ2
upper bound for the numerator:

2 ).Then we move to the

(cid:33)

St

StT
m

− I

z

(cid:32)

vT

=

1
4

St

− (v − z)T (

− I)(v + z)

{(v + z)T (
StT
m

StT
m
St
− I)(v − z)}
(cid:107)St(v + z)(cid:107)2 − (cid:107)v + z(cid:107)2
(cid:107)St(v − z)(cid:107)2},

{ 1
m

=
+ (cid:107)v − z(cid:107)2 − 1
m

1
4

(43)

and,

m

StT
m

(cid:26) 1
(cid:26) 1
(cid:26)
(cid:26)

(cid:32)

+

≤ 1
4
1
4
1
4
1
4

=

+

(cid:32)

(cid:27)
(cid:27)

(bm(cid:107)v + z(cid:107)2 + W + θ)2 − (cid:107)v + z(cid:107)2

2

(bm(cid:107)v − z(cid:107)2 + W + θ)2 − (cid:107)v − z(cid:107)2

2

(

m
b2
m
m
(1 − b2
m
m

− 1)(cid:107)v + z(cid:107)2

2 +

)(cid:107)v − z(cid:107)2

2 +

2bm(W + θ)

m

2bm(W + θ)

m

(cid:107)v + z(cid:107)2

(cid:107)v − z(cid:107)2

,

(cid:27)
(cid:27)

with probability at least (1− 8e− θ2
√
(cid:107)v − z(cid:107)2 ≤ 2

2 and (cid:107)v + z(cid:107)2

(45)
8 ). Note that (cid:107)v + z(cid:107)2 +
2 ≤ 4, we have:

2 + (cid:107)v − z(cid:107)2

(cid:33)

vT

St

StT
m

− I
≤ 2bm(W + θ)
√
2bm(W + θ)
≤

m

m

z
(cid:107)v + z(cid:107)2 + (cid:107)v − z(cid:107)2

4
− 1|

+ | b2
m
m

+ | b2
m
m

− 1|

(46)

1.3.3. THE PROOF FOR PROPOSITION 3

Proof. Recall that σt is deﬁned as:

σ(St, A) =

supv∈range(A)∩Sn−1 (cid:107)Stv(cid:107)2
inf v∈range(A)∩Sn−1 (cid:107)Stv(cid:107)2
6.7](Oymak et al., 2015), with W(ASd−1) ≤ √

by simply apply again the Gordon’s lemma [Lemma
d, we with

,

(47)

2

2

obtain the upper bound on the numerator:
√

(cid:107)Stv(cid:107)2

2 ≤ (bm +

d + θ)2,

sup

v∈range(A)∩Sn−1

and the lower bound:

inf

v∈range(A)∩Sn−1

(cid:107)Stv(cid:107)2

2 ≥ (bm −

√

d − θ)2,

both with probability at least 1 − e− θ2
2 .

(48)

(49)

1.4. Details of the implementation of algorithms and

numerical experiments

For our GPIS and Acc-GPIS algorithms, we have several
key points of implemenations:

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

If we chose η =

√
1
L(bm+

(cid:32)

d+θ)2 we have:
(bm − √
√

(bm +

1 − µ
L

d − θ)2
d + θ)2

(cid:33)

α(η, StA) ≤

,

(40)

vT

(cid:33)

St

− I

z

hence we have the following by [Lemma 6.8](Oymak et al.,
2015):

inf

v∈AC∩Sn−1

1
m

(cid:107)Sv(cid:107)2

2 ≥ (bm − W − θ)2

m

thus ﬁnishes the proof.

,

(42)

W(AC ∩ Sn−1 − z) = Eg(

gT (v − z))

v∈AC∩Sn−1

sup
= Eg(gT z +
= W(AC ∩ Sn−1)

sup

v∈AC∩Sn−1

vT g)

(44)

• Count sketch

As described in the main text.

Gradient Projection Iterative Sketch for large-scale constrained Least-squares

• Line search

We implement the line-search scheme given by (Nes-
terov, 2007) and is described by Algorithm 3 for GPIS
and Acc-GPIS in our experiments with parameters
γu = 2, and γd = 2. Such choice of line-search pa-
rameters simply means: when even we ﬁnd the con-
dition ft(PK(xi − η(cid:79)ft(xi))) ≤ mL does not hold,
we shrink the step size by a factor of 2; and then at the
beginning of each iteration, we increase the step size
chosen at previous iteration by a factor of 2, then do
backtracking again. Hence our methods are able to en-
sure we use an aggressive step size safely in each iter-
ation. This is an important advantage of the sketched
gradient method since we observe that for stochas-
tic gradient such as SAGA a heuristic backtracking
method similar to Algorithm 3 may work but it will
demand a very small γd (tends to 1) otherwise SAGA
may go unstable, and an aggressive choice like our
γd = 2 is unacceptable for SAGA. (Hence we suspect
that SAGA is unlikely to be able to beneﬁt computa-
tional gains from line-search as our method does.)

• Gradient restart for Acc-GPIS

(O’Donoghue & Candes, 2015) has proposed two
heuristic adaptive restart schemes - gradient restart
and function restart for the accelerated gradient meth-
ods and have shown signiﬁcant improvements without
the need of the knowledge of the functional parame-
ters µ and L. Such restart methods are directly appli-
cable for the Acc-GPIS by nature due to its sketched
deterministic iterations. Here we choose the gradient
restart since it achieves comparable performance in
practice as function restart but cost only O(d) opera-
tions.

1.4.1. PROCEDURE TO GENERATE SYNTHETIC DATA

SETS

The procedure we used to generate a constrained least-
square problem sized n by 100 with approximately s-sparse
solution and a condition number κ strictly follows:
1) Generate a random matrix A sized n by 100 with i.i.d
entries drawn from N (0, 1).
2) Calculate A’s SVD: A = U ΣV T and replace the singu-
lar values diag(Σ)i by a sequence:

diag(Σ)i =

diag(Σ)i−1

κ 1

d

(50)

3) Generate the ”ground truth” vector xgt sized 100 by
1 randomly with only s non-zero entries in a orthongo-
nal transformed domain Φ, and calculate the l1 norm of it

Figure 1. Experimental results on the average choices of GPIS’s
step sizes given by line-search scheme (Nesterov, 2013)

Table 1. Synthetic data set for step size experiment

DATA SET

SIZE

S Φ

SYN4

(20000, 100)

-

I

(r = (cid:107)Φxgt(cid:107)1). Hence the constrained set can be described
as K = {x : (cid:107)Φx(cid:107)1 ≤ r}.
4) Generate a random error vector w with i.i.d entries such
that (cid:107)Axgt(cid:107)2
(cid:107)w(cid:107)2
5) Set y = Axgt + w

= 10.

1.4.2. EXTRA EXPERIMENT FOR STEP SIZE CHOICE

We explore the step size choices the GPIS algorithm pro-
duce through using the line-search scheme with respect to
different sparsity level of the solution. The result we shown
is the average of 50 random trials.
The result of the step-size simulation demonstrates that the
step sizes chosen on average by the line-search scheme for
the GPIS algorithm is actually related with the sparsity of
the ground truth xgt: at a regime when the xgt is sparse
enough, the step size one can achieve goes up rapidly w.r.t
the sparsity. While in our Proposition 2 we revealed that
the outerloop of GPIS/Acc-GPIS can beneﬁt from the con-
strained set, and here surprisingly we also ﬁnd out numer-
ically that the inner loop’s can also beneﬁt from the con-
strained set by aggressively choosing the large step sizes.
Such a result echos the analysis of the PGD algorithm on
constrained Least-squares with a Gaussian map A (Oymak
et al., 2015). Further experiments and theoretical analysis
of such greedy step sizes for sketched gradients and full

02468101214161820sparsity of the solution00.010.020.030.040.050.06average step size given by line-search for GPISm = 200m = 400m = 800Gradient Projection Iterative Sketch for large-scale constrained Least-squares

gradients on general maps is of great interest and will go
beyond the state of the art analysis for convex optimization.

References
Beck, Amir and Teboulle, Marc. A fast iterative shrinkage-
thresholding algorithm for
inverse problems.
SIAM Journal on Imaging Sciences, 2(1):183–202, 2009.
doi: 10.1137/080716542. URL http://dx.doi.
org/10.1137/080716542.

linear

Nesterov, Yurii. Gradient methods for minimizing compos-

ite objective function. Technical report, UCL, 2007.

Nesterov, Yurii. Gradient methods for minimizing com-
posite functions. Mathematical Programming, 140(1):
125–161, 2013.

O’Donoghue, Brendan and Candes, Emmanuel. Adaptive
restart for accelerated gradient schemes. Foundations of
computational mathematics, 15(3):715–732, 2015.

Oymak, Samet, Recht, Benjamin, and Soltanolkotabi,
Mahdi. Sharp time–data tradeoffs for linear inverse prob-
lems. arXiv preprint arXiv:1507.04793, 2015.

Pilanci, Mert and Wainwright, Martin J. Randomized
sketches of convex programs with sharp guarantees. In-
formation Theory, IEEE Transactions on, 61(9):5096–
5115, 2015.

Pilanci, Mert and Wainwright, Martin J. Iterative hessian
sketch: Fast and accurate solution approximation for
constrained least-squares. Journal of Machine Learning
Research, 17(53):1–38, 2016.

