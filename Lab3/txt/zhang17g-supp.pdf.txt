Projection-free Distributed Online Learning in Networks: Appendix

A. Proof of Lemma 4
Proof. Combining Lemma 2 and Lemma 3, we can obtain
the concrete recursion

ht+1,i ≤ (1 − σt,i)ht,i + σ2
t,iD2
√

+ ηi(

1 + σ2(P )
1 − σ2(P )

n + 1)L(cid:112)ht+1,i.

√

n + 1)L(cid:112)ht+1,i ≤ σ2

As the parameters ηi and σt,i are chosen such that
ηi( 1+σ2(P )
t,iD2, we can then ob-
1−σ2(P )
tain the following compact recursion
ht+1,i ≤ (1 − σt,i)ht,i + 2D2σ2
t,i.

Now based on this recursion, we can prove the bound in the
lemma by induction.
First, the base case of induction is true for t = 1 since by
deﬁnition we have

h1,i = F1,i(xi(1)) − F1,i(x∗

i (1))
= (cid:107)xi(1) − x1(1)(cid:107)2 − (cid:107)x∗
≤ 2D2
≤ 4D2σ1,i.

i (1) − x1(1)(cid:107)2

Second, assuming that the bound is true for t, we now show
that it also holds for t + 1:

ht+1,i ≤ (1 − σt,i)ht,i + 2D2σ2

t,i

t,i

≤ 4D2σt,i(1 − σt,i) + 2D2σ2
= 4D2σt,i(1 − σt,i +
= 4D2σt,i(1 − σt,i
2
≤ 4D2σt+1,i.

σt,i
2

)

)

The last inequality follows from the deﬁnition of σt,i,
which can be proved in the following section.

B. Proof of the last inequality in Lemma 4
For the sequence σt,i = 1√
inequality holds

, t = 1, 2,··· , T , the following

t

σt,i(1 − σt,i
2

) ≤ σt+1,i.

Proof. The inequality we need to prove is

1√
t

√
(1 − 1
2

t

) ≤

1√
t + 1

.

Note that, for the right side, we have the following identity

1√
t + 1

=

1√
t

√
t√
t + 1

.

Thus, dividing both sides by the common 1√
following equivalent inequality

t

√
1 − 1
2

t

≤

√
t√
t + 1

.

, we reach the

By rewriting, we have

√
1 − 1
2

t

≤ 1 −

√

t + 1 − √
√

t + 1

t

.

It then follows that

√

t + 1 − √
√
t + 1
√

t

√
≤ 1
2
√

.

t

Multiplying

t + 1

√
(

t + 1 − √

t in both sides, we obtain
√
t)

t ≤

√

,

t + 1
2

which is equivalent to the following

(cid:112)

√

t2 + t ≤

t + 1
2

+ t.

Squaring both sides, we have

t2 + t ≤ t2 +

t + 1

4

√
+ t

t + 1.

Clearly, this inequality holds for any t = 1,··· , T , since

√
+ t

t + 1.

t ≤ t + 1
4

C. Proof of Lemma 6
Proof. We adopt the same notations used in the proof of
Lemma 3. From there, we have

t−1(cid:88)

n(cid:88)

r=1

j=1

zi(t) =

P t−r−1

ij

gj(r).

Projection-free Distributed Online Learning in Networks: Appendix

To proceed, we ﬁrst introduce another auxiliary sequence
which are composed of the averages of the subgradients
over all nodes i at each iteration

we have

(cid:107)zi(t) − ¯z(t)(cid:107) ≤ L

Then we can show that the averaged dual variable ¯z(t)
evolves in a quite simple way

n(cid:88)

i=1

¯g(t) =

1
n

gi(t).

n(cid:88)
n(cid:88)

i=1

n(cid:88)
n(cid:88)

j=1

(

j=1

i=1

¯z(t + 1) =

=

1
n

1
n

= ¯z(t) + ¯g(t).

Pijzj(t) + gi(t))

Pijzj(t) + ¯g(t)

t−1(cid:88)

n

r=1

σ2(P )t−r−1√
√
(1 − σ2(P )t−1)
1 − σ2(P )
nL

√

nL

1 − σ2(P )

.

=

≤

The above equation and the last inequality follow respec-
tively from the summation formula of geometric series and
the fact that σ2(P ) < 1 when P is a doubly stochastic ma-
trix (Berman & Plemmons, 1979).

D. Proof of Lemma 7
Proof. According to (Hosseini et al., 2013), the D-ODA
algorithm with parameters α(t) applied to loss functions
that are L-Lipschitz with respect to a general norm attains
the following regret bound

T (xi,x) ≤ L2
Ra
2

1

t=1

ψ(x)

α(T )

α(t) +

T−1(cid:88)
T(cid:88)
α(t)(cid:107)zi(t) − ¯z(t)(cid:107)∗
T(cid:88)

n(cid:88)

t=1

α(t)

t=1

j=1

+ L

+

2L
n

(cid:107)zj(t) − ¯z(t)(cid:107)∗,

where (cid:107)·(cid:107)∗ denotes the corresponding dual norm.
Note that the norm we utilize is the L2 norm and its
dual norm is itself. Thus we can apply the bound for
(cid:107)zi(t) − ¯z(t)(cid:107) in Lemma 6 here. Combining it with the
fact that
the fact that ψ(x) =
(cid:107)x − x1(1)(cid:107)2 ≤ D2 and setting α(t) = η yields the stated
regret bound in the lemma.

α(t) ≤ T(cid:80)

T−1(cid:80)

α(t),

t=1

t=1

E. Veriﬁcation of the validity of ηi
Proof. As ηi =

(1−σ2(P ))D
√

2(

n+1+(

n−1)σ2(P ))LT 3/4 , we have

√
√

n + 1)L(cid:112)ht+1,i =

D(cid:112)ht+1,i

2T 3/4

.

ηi(

1 + σ2(P )
1 − σ2(P )

By Lemma 4 and deﬁnition of σt,i, we have

ht+1,i ≤ 4D2σt+1,i ≤ 4D2σt,i.

It then follows that

D(cid:112)ht+1,i

2T 3/4

≤ σ1/2
t,i
T 3/4

D2.

The last equation follows from the doubly stochastic prop-
erty of matrix P . Based on the above recursion, we can
easily deduce that

t−1(cid:88)

r=1

t−1(cid:88)

n(cid:88)

r=1

j=1

¯g(r) =

1
n

gj(r).

¯z(t) =

Hence,

zi(t) − ¯z(t) =

t−1(cid:88)

n(cid:88)

r=1

j=1

(P t−r−1

ij

− 1
n

)gj(r).

Then using the fact that (cid:107)gi(t)(cid:107) ≤ L, and the properties of
norm functions and matrices, we obtain

ij

=

j=1

r=1

)gj(r)

− 1
n

(P t−r−1

(cid:107)zi(t) − ¯z(t)(cid:107)

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)t−1(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
n(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:13)(cid:13)gj(r)(cid:13)(cid:13)
(cid:12)(cid:12)(cid:12)(cid:12)P t−r−1
n(cid:88)
≤ t−1(cid:88)
t−1(cid:88)
(cid:13)(cid:13)P t−r−1
− 1/n(cid:13)(cid:13)1
t−1(cid:88)
(cid:13)(cid:13)P t−r−1ei − 1/n(cid:13)(cid:13)1.

− 1
n

≤ L

= L

r=1

r=1

j=1

ij

i

r=1

Since the following inequality holds for any non-negative
integer s

(cid:107)P sei − 1/n(cid:107)1 ≤ σ2(P )s√

n,

Projection-free Distributed Online Learning in Networks: Appendix

We thus only need to verify that the following inequality
holds for any t = 1,··· , T

σ1/2
t,i
T 3/4

D2 ≤ σ2

t,iD2.

This clearly holds since for any t = 1,··· , T

1

T 3/4

≤ σ3/2

t,i =

1
t3/4

.

Thus, the choice of ηi satisﬁes the constraint required in
Lemma 4.

References
Berman, Abraham and Plemmons, Robert J. Nonnegative
Matrices in the Mathematical Sciences. Academic Press,
1979.

Hosseini, Saghar, Chapman, Airlie, and Mesbahi, Mehran.
Online distributed optimization via dual averaging.
In
IEEE Conference on Decision and Control, pp. 1484–
1489. IEEE, 2013.

