Differentially Private Clustering in High-Dimensional Euclidean Spaces

Supplementary Material of

Maria-Florina Balcan 1 Travis Dick 1 Yingyu Liang 2 Wenlong Mou 3 Hongyang Zhang 1

1. Additional Related Work
Non-Private Clustering: There is a wide range of prior work on the problem of center-based clustering in the absence of
privacy requirement. It is known that exact optimization of objective function in Rd is not computationally possible (Das-
gupta, 2008) even for the problem of 2-means clustering. To avoid the computational obstacle, several approximation
algorithms have been developed, e.g., by the local swap (Kanungo et al., 2002; Arya et al., 2004), careful seeding (Arthur
& Vassilvitskii, 2007), or enumeration via sample-based loss estimator (Kumar et al., 2010). Another line of research
focuses on the recovery of optimal data partition under stability or separation assumption (Balcan et al., 2009; Awasthi &
Balcan, 2014).
It is worth noting that most of existing work for clustering in Rd with reasonable approximation guarantee relies on the
construction of a candidate set of centers. In particular, Matouˇsek (2000) constructed a (1 + )-approximate candidate set
via griding argument, which was widely applied in the later work (Kanungo et al., 2002; Makarychev et al., 2015). Kumar
et al. (2010) took the average of a randomly sampled subset of data points as the set of candidate centers. However, none
of these approaches can be easily adapted to the differentially private settings.

2. Why Direct Extension Failed
It would be natural to ask, if one has read (Matouˇsek, 2000), why their discretization methods cannot directly extend to
private setting, with randomized decision in the sub-division procedure. In the following example, we will see that, the
privatized vanilla recursive partition, i.e., direct use of discretization routine in Algorithm 1, will lead to arbitrarily bad
performance in our settings.

Figure 1. Illustration for the Worst Case for Partition Procedure

Consider a set of n points S in Rp, with p ≥ 1+log n. The initial cube we are working on is [−1, 1]p, which contains set S.
1Carnegie Mellon University, Pittsburgh, PA, USA 2Princeton University, Princeton, NJ, USA 3Peking University, Beijing, China.

Correspondence to: Wenlong Mou <mouwenlong@pku.edu.cn>.

Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

Data PointsCandidate Centers(Center of each cube)Differentially Private Clustering in High-Dimensional Euclidean Spaces

i=1 and xi = [1/2, 0, 0,··· , 0]T + ησi, where η = 1

p2n2 and σi ∈ [0,±1,±1,··· ,±1]T , σi (cid:54)= σj,∀i (cid:54)= j.
Let S = {xi}n
Apparently S is a small cluster with clustering loss less than 1. Using the hierarchical partition procedure, we will get
2p cubes corresponding to 2phyperoctants. According to their deﬁnition, each points in S is divided into a separate cube.
Due to calculation similar to Lemma 1, none of these cubes will be further divided, with high probability, and the partition
2 − η to each of xi, and the resulted clustering
procedure ends up returning a set of candidate centers with distance at least 1
loss becomes at least Ω(n). In Figure 1, we illustrate the bad case for discretization procedure.
What’s wrong with this method? In (Matouˇsek, 2000), the discretization procedure can carry on until meeting a threshold
that guarantees candidate cost at the same scale of optimal clustering loss. However, for privacy reasons, the partition
procedure has to stop, in order to hide the accurate location of a single data point. Thus we do not want a cluster to be
divided into too many parts during the procedure, as in the example above.
Fortunately, this worst case is extremely atypical in high dimensions, and can be avoided via repeated random shift. Using
probabilistic arguments in high dimensions, we can make sure that each optimal cluster is fully contained in the cube with
a proper scale.

3. Omitted Proofs in Section 4
Theorem 1 (Theorem 1 in the Main Body). The set C generated by Algorithm 1 satisﬁes |C| ≤ n log n, with probability
1 − δ.
Proof. First of all, it is easy to verify that the function f (·) deﬁned in Algorithm 1 satisﬁes

Pr (Qi partitioned|Qi empty) = f (0) ≤ δ

n10 .

Consider a (non-private) tree T partitioning generated as follows: the root node corresponds to the initial cube Q. For
each round, we subdivide the cube in each dimensions evenly, resulting in 2p cubes. A smaller cube will be active when
it contains at least one point, until the depth of this node in the tree grows to be log n. The tree has at most log n levels,
while each level has at most n nodes. So the size of this tree is upper bounded with n log n. Let ˜T denote the partitioning
tree generated by Algorithm 1, we have

(cid:16) ˜T (cid:42) T(cid:17) ≤ Pr
(cid:16)∃n ∈ leaf(T ), n is partitioned in ˜T(cid:17)
≤ (cid:88)

n is partitioned in ˜T(cid:17)

(cid:16)

Pr

Pr

n∈leaf(T )

≤|T |f (0) ≤ δ.

With probability 1 − δ, the number of cells generated by Algorithm 1 is no more than |T |, and the size of C is thus
bounded.
Theorem 2 (Theorem 2 in the Main Body). Algorithm 1 preserves -differential privacy.
Proof. Consider a layers of discretization with a ≤ log n. By adaptive composition lemma, it sufﬁces to show that
given the active cubes and candidate centers constructed in upper layers, the points added to C in this layer preserves
log n-differential privacy.



Given current C and A ﬁxed, modiﬁcation on a single data point will inﬂuence the point counts of at most two active cubes.
By deﬁnition we only need to show
∀i, Pr

l=1 ⊆ A(cid:17) ≤ exp

l=1 ⊆ A(cid:17)

(cid:16){Q(l)

(cid:16){Q(l)

(cid:18) 

i }2d

(cid:19)

i }2d

.

2 log n

Pr
S(cid:48)

S

It is easy to verify our construction of f (·) satisﬁes this bound.
Theorem 3 (Theorem 3 in the Main Body). The following event holds with probability at least 1− δ: In Algorithm 1, when

a cube Qi is removed from A and its subdivided cubes are not inserted to A, then we have either |Qi ∩ X| ≤ O(cid:0)γ log n

(cid:1),

δ

n .
or the edge length of Qi is at most Λ

Proof. It is easy to verify that f (·) satisﬁes

Differentially Private Clustering in High-Dimensional Euclidean Spaces

(cid:16)

Pr

Qi partitioned

(cid:12)(cid:12)(cid:12)|Qi ∩ X| ≥ 10γ log

n
δ

(cid:17) ≥ f

(cid:16)

10γ log

(cid:17) ≥ 1 − δ

n
δ

n10 .

Consider a (non-private) partition tree T (cid:48). The construction of this tree is based on bisection for each dimension, similar to
the proof of Theorem 1. However, this time a cell will stop being partitioned when the number of data points inside is less
δ . Apparently we have |T (cid:48)| ≤ |T | ≤ 2pn log n, where T is the tree constructed in the proof of Theorem 1.
than 10γ log n
Since leaf nodes of T (cid:48) satisfy the desired properties of this theorem, it sufﬁces to show that T (cid:48) ⊆ ˜T with probability 1− δ.
Actually, we have

Pr

(cid:16)∃n ∈ internal(T (cid:48)), n is not partitioned in ˜T(cid:17)
(cid:16)T (cid:48) (cid:42) ˜T(cid:17) ≤ Pr
≤ (cid:88)
≤|T (cid:48)|(cid:16)

n is not partitioned in ˜T(cid:17)

n∈internal(T (cid:48))
1 − f

(cid:17)(cid:17) ≤ δ.

10γ log

(cid:16)

(cid:16)

Pr

n
δ

Theorem 4 (Theorem 4 in the Main Body). Algorithm 2 preserves -differential privacy.

Proof. The proof of privacy is simply done by T -fold composition theorem over the T independent trials of the private
partition procedure, each of which preserves 

Theorem 5 (Theorem 5 in the Main Body). Algorithm 2 outputs an (cid:0)O(log3 n), O(kγ( 

δ )(cid:1) candidate set with

T -differential privacy.

T ) log n

j be the cluster induced by u∗

j , i.e.

j with at least

j

j =

j , Lr∗

T ) log n

(cid:113) 1|S∗

(cid:107)xi − ˜uj(cid:107)2 ≤ k(cid:88)

k, ﬁxed but unknown. Let S∗
(cid:107)xi − ul(cid:107)2.

probability at least 1 − δ, where γ(c) = 40
c log n
Proof. Suppose the optimal set of centers are u∗
Sj = {i : j = argminl (cid:107)xi − ul(cid:107)}. Let r∗
For ∀j = 1, 2,··· , k, we say u∗
γ( 

δ log n, and T = k log n
δ .
(cid:80)
2,··· , u∗
1, u∗
i∈S∗
j |
δ is captured by C with factor L, let ˜uj ∈ B(cid:0)u∗
n(cid:88)
(cid:107)xj − ˜uj(cid:107)2 ≤ k(cid:88)
(cid:88)

j is captured by C with factor L iff B(cid:0)u∗
n )(cid:1) ∩ C, we have
(cid:88)
j(cid:107)2 + |Sj| · (cid:107)u∗
(cid:88)
j − ˜uj(cid:107)2 +
(cid:16)

j /∈captured
u∗

) log
T
j with |S∗
Thus, to prove the quality of our candidate set, it sufﬁces to show that each u∗
with factor O(log
For ∀j ∈ {1, 2,··· , k} ﬁxed, since 1|S∗
j |

≤OPT + L2 · OPT + O(Λ2) + O

2 n), with high probability.

(cid:107)xi − ul(cid:107)2 = r∗

Λ2(cid:17)

j | ≥ kγ( 

|Sj| · (cid:107)u∗

|Sj| · (cid:107)u∗

(cid:107)xj − u∗

j ∈captured
u∗

(cid:88)

j + O( 1

j + O( 1

j , Lr∗

(cid:80)

=OPT +

i∈S∗

j

i∈S∗

j

i∈S∗

min

j

kγ(

n
δ

j=1

j=1

i=1

.

3

n )(cid:1) ∩ C (cid:54)= ∅. If each u∗


j − ˜uj(cid:107)2

j − ˜uj(cid:107)2

2, using Chebyshev Inequality, we have

T ) log n

δ is captured by C

j

(cid:12)(cid:12)B(u∗

j , 2r∗

j ) ∩ S∗

j

j

(cid:12)(cid:12) ≥ 1

2

(cid:12)(cid:12)S∗

j

(cid:12)(cid:12) .

√

Given the randomized shift vector v in Algorithm 2 ﬁxed, consider an (inﬁnite) rectangular grid Gv, constructed by recursive
j ∈ ¯Q(j) and the edge length of ¯Q(j)
bisection on each dimension of Qv. There must exist a cube ¯Q(j) ∈ Gv, such that u∗
is between 2pr∗
j . If any point in ¯Q(j) is added during the execution of Algorithm 1, its distance to u∗
j is at most
p = r∗
4r∗
2 n). Therefore, if ¯Qj is ever active during the discretization, this cluster center must have been
j p
captured by C.
According to Theorem 3, conditioned on the event that no failure occurs (which has negligible probability), there can only
be two cases for which ¯Q(j) does not appear in C:

j and 4pr∗
j · O(log

3

Differentially Private Clustering in High-Dimensional Euclidean Spaces

• The edge length of ¯Q(j) is less than 1
n;

• (cid:12)(cid:12) ¯Q(j) ∩ S∗

j

(cid:12)(cid:12) ≤ O(cid:0)γ( 

T ) log n

δ

(cid:1).

n ) additive error on the radius, u∗

If it is the ﬁrst case, we can turn to a cube ¯Q(cid:48)(j) ∈ Gv that contains ¯Q(j) with edge length exactly Θ( 1
deﬁnition of capturing allows O( 1
¯Q(cid:48)(j) has larger size and potentially more points in S∗
at least 2pr∗
j is likely to a large number of points in S∗
Let’s now turn to the randomness of v, in terms of captures for u∗
to shifting ¯Q(j) uniformly at random, given the fact that it contains u∗
j .
j may vary in the grid. However, u∗
(The location of the cell of this scale that contains u∗
contains it, its capture only depends on its relative location within this axis-aligned cell.)
Therefore, if |Sj| ≥ Ω(γ( 

n ). Since our
j will also be captured if ¯Q(cid:48)(j) becomes active. Since
j with edge length
j , shifting Qv and Gv uniformly at random is equivalent

j , we only need to show that a cube containing u∗
j and thus becomes active in Algorithm 1.

j is indifferent with ”which” cell

T ) log n

Pr(cid:0)u∗

δ ), we will have

(cid:16)(cid:12)(cid:12)S∗
j ∩ ¯Q(j)(cid:12)(cid:12) ≥ Ω(γ(
j ∈ captured(v)(cid:1) ≥ Pr
(cid:18)(cid:12)(cid:12)S∗
j ∩ ¯Q(j)(cid:12)(cid:12) ≥ 1
≥ Pr(cid:0)B(u∗
(cid:1)
(cid:19)p ≥ 1
(cid:18)

2
j ) ⊆ ¯Qj

j , 2r∗

≥ Pr

=

.

(cid:19)

T
j |)
|S∗

1 − 2
p

27

(cid:17)

) log

n
δ

)

For each j ∈ {1, 2,··· , k}, we assign 27 log n
probability at least 1 − δ
probability at least 1 − δ, we have

n. By aggregating the 27k log n

δ independent trials to it, and it is easy to see that it is captured by C with
δ trails together and applying union bound, we conclude that with

(cid:16)

(cid:17) ⇒ ∃˜uj ∈ C captures u∗

j .

∀j ∈ {1, 2,··· , k},

|S∗
j | ≥ Ω

So the proof is completed.

γ(


T

) log

n
δ

4. Omitted Proofs in Section 5
Theorem 6 (Theorem 6 in the Main Body). Algorithm 3 preserves -differential privacy.

Proof. The privacy guarantee is straightforward using composition theorem over T rounds of the algorithm, and an
additional exponential mechanism that selects the best one.
It is easy to verify the sensitivity of loss increments
L(Z − {x} + {y}) − L(Z) is 8Λ2, the privacy guarantee of exponential mechanism in each round follows.
Theorem 7 (Theorem 7 in the Main Body). With probability 1 − δ, the output of Algorithm 3 satisﬁes

L(Z) ≤ 30OPT + O

(cid:18) k2Λ2



(cid:19)

.

log2 n|C|

δ

Proof. The proof inspires from (Gupta et al., 2010). Basically, we use the following fact, which is derived from the
construction of swap pairs and Lemma 2.2 in (Kanungo et al., 2002):
For clustering centers Z, there exists a set of k swaps {(xi, yi)}k

i=1 such that

k(cid:88)

n(cid:88)

L(Z) − L(Z − {xi} + {yi}) ≥ 3L(Z) − OPT − 2

(cid:107)xi − z(oi)(cid:107)2,

i=1

i=1

where oi is the optimal cluster to which xi is assigned, and z(o) is point o’s nearest center in Z.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

Following the proof for local swap heuristics, we can bound the extra term.

n(cid:88)

n(cid:88)
≤2OPT + L(Z) + 2(cid:112)OP T · L(Z).

i=1

i=1

(cid:107)xi − z(oi)(cid:107)2 ≤ 2OPT + L(Z) + 2

n(cid:88)

i=1

R =

Putting them together, we have that ∃x ∈ Z, y ∈ C such that

(cid:107)xi − oi(cid:107) · (cid:107)xi − zi(cid:107)

L(Z) − L(Z − {x} + {y}) ≥ 1
k
≥ 1
k
≥ 1
2k

(3L(Z) − OPT − 2R)

(cid:16)L(Z) − 5OPT − 4(cid:112)OPT · L(Z)
(cid:17)

(L(Z) − 30OPT) .

(cid:16) 1
 log n|C|

δ

(cid:17)

The rest of this proof proceeds just as in (Gupta et al., 2010): exponential mechanism guarantees the bound 4 holds with
n. T rounds of iteration guarantees the multiplicative term being
an additive term O
reduced to constant order, except for the case of OPT = O( k2
δ ) where all the excess loss goes to the additive
term. Combining these facts together using union bound over failure probability we conclude the result.

with failure probability δ

 log2 n|C|

Theorem 8 (Theorem 8 in the Main Body). Under the following assumptions:

• Algorithm candidate ({xi}n
• Given any C, Algorithm localswap ({xi}n

i=1, , δ) preserves -differential privacy for {xi}n

i=1.

i=1, C, , δ) preserves -differential privacy for {xi}n

i=1.

Algorithm 4 preserves -differential privacy.

6T -DP. Given the centers u1, u2,··· , uk in projected
Proof. In each of T -rounds, the two sub-routines each preserves
space ﬁxed, changing the position for one of data points (and also resulting the change in projected space) will affect at
12T -DP, and the recovery procedure also
most two clustering centers. The noised version of cluster count sj preserves
preserves
6-DP, putting
them together using composition theorem, we have the privacy guarantee.

12T -DP given the cluster counts known and ﬁxed. At last, the exponential mechanism preserves 







Theorem 9 (Theorem 9 in the Main Body). Instantiated by algorithms that guarantee:

• With probability 2
• With probability 2

3 , algorithm candidate ({xi}n
3 , algorithm localswap ({xi}n

i=1, , δ) can output an (α, σ1())-approximate candidate set.

i=1, C, , δ) achieves clustering loss with multiplicative approxima-

tion factor c and additive term σ2(), compared with optimal clustering centers in the discrete space.

Algorithm 4 achieves the following bound with probability 1 − δ:

L(cid:16){zj}k

j=1

(cid:17) ≤ 3cαOPT + 3Cσ(cid:48)

1 + 3σ(cid:48)

2 + O

(cid:32)

(cid:33)

,

dΛ2 log3 1
δ

2

(cid:16)



2 log 1/δ

(cid:17)

where σ(cid:48)

i = σi

Proof. With probability 1

for i = 1, 2

3, the centers u1, u2,··· , uk in projected space satisﬁes
n(cid:88)

(cid:107)yi − uj(cid:107)2 ≤ cOPTdiscrete + σ2 ≤ cα(OPTprojected + σ1) + σ2.

min

j

i=1

Differentially Private Clustering in High-Dimensional Euclidean Spaces

The JL transform guarantees that with probability 1

2, the pairwise distances between points uniformly satisﬁes

∀i, j ∈ {1, 2,··· n},

(cid:107)yi − yj(cid:107)2 ≤

1 ± 1
2

(cid:107)xi − xj(cid:107)2.

(cid:18)

(cid:19)

This event is also independent with randomized algorithms in projected spaces.
Since the optimal clustering loss for k-means depends only on the pairwise distances between data points, we have the
following fact under the event that JL transform successfully preserves pairwise distances: (Cj denotes optimal clustering
assignment)

OPTprojected ≤ (cid:88)
k(cid:88)

i∈C∗

j

(cid:107)ui − ν∗

k(cid:88)
j (cid:107)2 =
(cid:88)
(cid:88)

j=1

i∈C∗

j

l∈C∗

j

≤ 3
2

1
j |
2|C∗

j=1

(cid:107)xi − xl(cid:107)2 =

3
2

OPT.

(cid:88)

(cid:88)

i∈C∗

j

l∈C∗

j

1
j |
2|C∗

(cid:107)ui − ul(cid:107)2

On the other hand, we have

(cid:88)
k(cid:88)

i∈Sj

(cid:107)xi − µj(cid:107)2 =

k(cid:88)
(cid:88)

j=1

(cid:88)

(cid:88)

(cid:88)
(cid:88)

1
2|Sj|

i∈Sj

l∈Sj
(cid:107)ui − ul(cid:107)2 = 2

≤2

1
2|Sj|

(cid:107)xi − xl(cid:107)2

(cid:107)ui − νj(cid:107)2,

j=1

i∈Sj

l∈Sj
j , resp.) in Rd, and νj (ν∗
j , resp.) are clustering center for Sj (C∗

where µj (µ∗
resp.) in Rp.
The noise added to cluster sizes, as well as the Laplacian mechanism, leads to the additional term. Thus we have the
desired bound to hold with constant probability. By repeating it with T independent trials and selecting the best, the failure
probability is reduced to δ. The additive loss induced by exponential mechanism in the last step is dominated by previous
terms.

j , resp.) are clustering center for Sj (C∗
j ,

i∈Sj

5. Omitted Proofs in Section 6
Theorem 10 (Theorem 10 in the Main Body). Algorithm 5 preserves -differential privacy.

Proof. The privacy proof is straightforward: for 2s
companied by Laplacian mechanism. Using the fact that both parts preserves 
composition.

η rounds of the algorithm, each round is exponential mechanism ac-
2T -DP, the privacy proof then follows via

Theorem 11 (Theorem 11 in the Main Body). The output of Algorithm 5 satisﬁes the following with probability at least
1 − δ:

n(cid:88)

i=1

(cid:107)xi − v(cid:107)2 ≤ 1
1 − η

OPT + O

(cid:32) Λ2s2 ln ds

ηδ

(cid:33)

.

η2

Proof. Let π(j) denote the index of entries in µ with the j-th largest absolute value. Note that removing j entries from
{1, 2,··· d} makes the largest absolute value at least µj+1, exponential mechanism guarantees that, for the index rj sampled
in j-th round, we have the following with probability at least ηδ
2s :

(cid:32) Λs ln ds

ηδ

(cid:33)

ηn

|µrj| ≥ |µπ(j)| − O

j = 1, 2,··· ,

,

s
η

.

Let ˜µ =(cid:80)s/η

Differentially Private Clustering in High-Dimensional Euclidean Spaces

j=1 µπ(j)eπ(j), For ∀j ∈ {1, 2,··· , d}, let Sj = {i ∈ [n] : xij (cid:54)= 0}, cj = |Sj| and vj = 1

cj

(cid:80)

i∈Sj

xij.

n(cid:107)˜µ − µ(cid:107)2 = n

µ2
π(j) =

j=s/η+1

j=s/η+1

d(cid:88)

c2
π(j)v2
n

π(j)

.

On the other hand, we have

n(cid:88)

i=1

(cid:107)xi − µ(cid:107)2 =

d(cid:88)

j=1

(cid:88)

i∈Sj

|xij − cj|2

 ≥ d(cid:88)

j=1

cj(n − cj)

n

v2
j .

v2
j +

Sort the d entries again according to the value of cj and let τ (j) denote the index of entries in µ with the j-th largest cj, we
have

d(cid:88)
 cj(n − cj)
d(cid:88)

n

Since(cid:80)d

j=1 cj ≤ ns, Markov inequality implies |{j ∈ [d] : cj ≥ ηn}| ≤ s

d(cid:88)

n(cid:107)˜µ − µ(cid:107)2 =

c2
π(j)v2
n

π(j)

≤

c2
τ (j)v2
n

τ (j)

.

j=s/η+1

j=s/η+1

d(cid:88)

η . Thus we have
cτ (j)(n − cτ (j))v2

τ (j)

n

d(cid:88)

≤ η
1 − η

τ (j)

c2
τ (j)v2
n

≤ η
1 − η
cτ (j)(n − cτ (j))v2

d(cid:88)

j=1

n

j=s/η+1

j=s/η+1

τ (j)

≤ η
1 − η

OPT.

Putting them together, we have

n(cid:88)

i=1

(cid:107)xi − v(cid:107)2 =

n(cid:88)

i=1

(cid:107)xi − µ(cid:107)2 + n(cid:107)µ − v(cid:107)2

(cid:32) Λ2s2 ln ds

ηδ

(cid:33)

(cid:32) Λ2s2 ln ds

ηδ

η2

(cid:33)

.

η2

≤OPT + n(cid:107)µ − ˜µ(cid:107)2 + O

≤ 1
1 − η

OPT + O

Theorem 12 (Theorem 12 in the Main Body). For k-median clustering problem, there is an -differential private al-
gorithms that runs in poly(k, d, n) time, which releases a set of centers ˜z1, ˜z2,··· , ˜zk, that satisﬁes the following with
probability 1 − δ:

L(˜z1, ˜z2,··· , ˜zk) ≤ O(log3/2 n) · OPT + O

Proof. By plugging in the error bounds and simple calculation, the centers u1, u2,··· , uk in projected space satisfy the
following with probability 1 − δ:

(cid:18) (k2 + d)Λ

log3 n
δ



(cid:19)

.

(cid:18) (k2 + d)Λ

(cid:19)

.

n(cid:88)

i=1

(cid:107)yi − uj(cid:107)2 ≤ O(log3/2 n) · OPTprojected + O

min

j

log3 n
δ



The JL transform preserves distances with up to 1 ± 1
replace sum-of-square decomposition with triangle inequality, using data points as intermediate step.

2 multiplicative error with probability 2

3. To make use of this fact, we

Differentially Private Clustering in High-Dimensional Euclidean Spaces

OPTprojected ≤ (cid:88)
k(cid:88)

i∈C∗

j

(cid:107)ui − ν∗

j (cid:107) ≤ k(cid:88)
(cid:88)

j=1

(cid:88)

i∈Cj

min

l

(cid:107)ui − ul(cid:107)

≤ 3
2

min

l

i∈Cj

j=1

(cid:107)xi − xl(cid:107) =

3
2

OPT.

(cid:88)
(cid:107)xi − µj(cid:107) ≤ k(cid:88)
k(cid:88)

(cid:88)

i∈Sj

min

l

j=1

l∈Sj
(cid:107)ui − ul(cid:107) ≤ 4

min

(cid:88)
(cid:88)

i∈Sj

≤2

(cid:107)xi − xl(cid:107)

(cid:107)ui − νj(cid:107),

On the other hand, we have

l∈Sj
j , resp.) are clustering center for Sj (C∗

j=1

l

where µj (µ∗
resp.) in Rp.
The excess losses incurred by log-concave sampling and discrete exponential mechanisms in the algorithm are dominated
by the previous term. Putting them altogether, we have the bound.

j , resp.) are clustering center for Sj (C∗
j ,

j , resp.) in Rd, and νj (ν∗

6. Additional Experimental Details
In this section we provide additional details about experiments in our paper. We will ﬁrst give a detailed description on
the real-world and synthetic datasets we are using, and then provide additional details on the comparison to existing works
including (Nock et al., 2016) and (Su et al., 2016). We will also present results about effect of number of clusters k and
dimension d on the clustering loss.

6.1. Description of Datasets

We compare our algorithm against the non-private k-means++ algorithm, SuLQ, k-variates++ (Nock et al., 2016), low-
dimensional algorithm (Su et al., 2016) and Sample and Aggregate on the following datasets.
MNIST: We used the raw pixels of the MNIST (LeCun et al., 1998) dataset. It has 70000 examples and 784 features.
CIFAR-10: We used the CIFAR10 dataset (Krizhevsky, 2009). Rather than the raw pixels, we obtained our feature represen-
tations from layer in3c (160 features) of a Google Inception (Szegedy et al., 2015) network trained for the classiﬁcation
task. We also created a second version of this dataset using the feature representations from layer in4d (144 features).
Our dataset contains 100000 randomly sampled examples from this dataset.
Synthetic: For experiments in the main body, we used a synthetic datasets of 100000 samples drawn from an equal-weight
mixture of 64 Gaussians in R100. Each Gaussian’s covariance matrix is the identity and the mean is randomly sampled from
[0, 100]d. For further experiments in Appendix, we change the value of k and d to illustrate the effect of these parameters.
In these experiments we always set k equal to the number of Gaussians in the mixture, with k ∈ {8, 16, 32, 64}, and we
also select d ∈ {5, 50, 500, 5000} to illustrate effect of dimension.

6.2. Additional Comparisons to Existing Works

We can also compare our approach with recent existing works such as (Nock et al., 2016) and (Nock et al., 2016) through
experiments. Based on experimental comparisons, we ﬁnd that our algorithm is the only one that works reasonably well
simultaneously for large d and large k, while keeping good performance with small k and d.
It has been noticed that, the griding algorithm in (Su et al., 2016) only works for spaces with constant dimensions (see
e.g. (Park et al., 2016)). So we are unable to evaluate Su et al.’s griding algorithm on the above datasets. Actually their
algorithm has time and space complexity that is exponential in the data dimension (because it constructs a regular grid in
d dimensions). In experiments, their grids caused memory allocation failure for d > 6, with 16GB RAM memory.

Differentially Private Clustering in High-Dimensional Euclidean Spaces

On the other hand, k-variates++ algorithm (Nock et al., 2016) is not designed for clustering problem with k more than
constant. In their experimental part for privacy, (Nock et al., 2016) only did experiments with k ≤ 5. Actually, its noise
scale ˜ relies upon empirical estimates of data-dependent parameter δw and δs. We estimated these parameters on MNIST
dataset with varying k and plug them into the formula. As k becomes larger than 5, the formula for setting parameter ˜ in
Theorem 12 of (Nock et al., 2016) becomes negative, which makes the algorithm invalid.
Therefore, we carried on three sets of experiments to compare our methods with recent baselines.

• To compare with (Nock et al., 2016) in high dimensions, we did experiments on MNIST dataset with k ∈ {2, 3, 4, 5}.
• To compare with (Su et al., 2016) with many clusters, we did experiments on Gaussian mixture dataset with k = 32

and d = 5, which will be discussed in the results about varying d.

• To compare all approaches together, we also carried out a small scale experiment on a mixture of Gaussian dataset

with 100000 samples in d = 3 dimensions with k = 4.

In Figure 2, we do the comparison in the ﬁrst setting, where we set the privacy parameter  = 0.5. The plot is in
logarithmic scale, and we can easily seen that 2 ends up adding too much noise for slightly larger k, while our algorithm
performs reasonably well.

Figure 2. Comparison to (Nock et al., 2016) on MNIST dataset with k = 2, 3, 4, 5

We show the results of the third setting in Table 1, and compare all existing approaches together. Among all private
methods, the griding algorithm in (Su et al., 2016) achieves the lowest objective value in this setting, while our algorithm
and SuLQ both achieved reasonably good performance. As our focus is on modern big data setting with high-dimensional
datasets, in this cases the griding algorithm cannot be applied.

Table 1. Objective values for all baseline algorithms on small-scale synthetic dataset.

Algorithm
k-means++

(Su et al., 2016)
SuLQ k-means

Objective Value
2.636e5
2.638e5
2.927e5
2.985e5
1.390e6
2.831e6

(Nock et al., 2016)

Ours

S&A

It is also worth noticing that running (Su et al., 2016) after randomized dimensionality reduction is not a polynomial-time
algorithm. Actually the regular grids need nΩ(d) cells to preserve good performance, and for the case of d ∼ log n, the
running time and storage is still prohibitive. Even for constant-dimensional case like d = 5, the griding algorithm (Su
et al., 2016) runs more than 5 hours while our algorithm runs within 15 minutes on the same machine. As the time and
space costs for (Su et al., 2016) grows rapidly with d ∼ log n > 5, the costs will become prohibitive.

2345k10101012101410161018k-means objectivek-variates++oursDifferentially Private Clustering in High-Dimensional Euclidean Spaces

6.3. Additional CIFAR-10 Dataset

In this section we present the empirical comparison of algorithms on our second CIFAR-10 dataset, which is identical to
the ﬁrst except features are instead taken from a later layer of the inception network (layer in4d). Figure 3 shows the
k-means objective of each algorithm when run for values of k from 2 to 64. Results are averaged over 5 runs.

Figure 3. Objective values for various algorithms on the CIFAR-10 dataset with features extracted from layer in4d of a Google inception
network.

6.4. Effect of dimension

In this section we directly evaluate the effect of the dimension on the objective value of the various algorithms. For
this evaluation, we generate samples of size n = 100000 sampled from a mixture of 32 Gaussians with dimensions
d ∈ {5, 50, 500, 5000}. All algorithms are run with the privacy parameter  = 0.5 and k = 32. Figure 4 shows the results
of this experiment. This provides further justiﬁcation for our claim that our algorithm scales to larger dimensions better
than the existing algorithms. Again, the sample and aggregate algorithm is omitted from the plot because its objective
value is several orders of magnitude worse and makes the plot difﬁcult to read, even in log scale.

Figure 4. Objective values for various algorithms on synthetic datasets of increasing dimension.

To make a comprehensive comparison, we also run the griding algorithm in (Su et al., 2016) for the case of d = 5. (As we
have discussed before, their algorithm is prohibitive for larger d). Their algorithm got clustering loss 1.84 × 107, which
is much smaller compared to our clustering loss 6.43 × 107. However, the running time of their algorithm is much longer
than us in this setting. And there’s no obvious approach for their algorithm to scale up with k, as we have discussed.

212223242526k345678k-means objective×104non-privateoursSuLQ5505005000dimension106108101010121014k-means objectivenon-privateoursSuLQDifferentially Private Clustering in High-Dimensional Euclidean Spaces

6.5. Effect of number of intrinsic clusters

Finally, we directly evaluate the effect of the number of intrinsic clusters in the dataset. To do this, we generate datasets
of n = 100000 sampled from mixtures of G ∈ {8, 16, 32, 64} Gaussians in 100 dimensional space. All algorithms are run
with k set to the true number of intrinsic clusters for each dataset. Figure 5 shows the results of this comparison.

Figure 5. Objective values for various algorithms on synthetic datasets of growing intrinsic numbers of clusters.

102030405060Number of Clusters024681012k-means objective×109non-privateoursSuLQDifferentially Private Clustering in High-Dimensional Euclidean Spaces

References
Arthur, David and Vassilvitskii, Sergei. k-means++: The advantages of careful seeding. In ACM-SIAM Symposium on

Discrete Algorithms, pp. 1027–1035, 2007.

Arya, Vijay, Garg, Naveen, Khandekar, Rohit, Meyerson, Adam, Munagala, Kamesh, and Pandit, Vinayaka. Local search

heuristics for k-median and facility location problems. SIAM Journal on computing, 33(3):544–562, 2004.

Awasthi, Pranjal and Balcan, Maria-Florina. Center based clustering: A foundational perspective. 2014.

Balcan, Maria-Florina, Blum, Avrim, and Gupta, Anupam. Approximate clustering without the approximation. In ACM-

SIAM Symposium on Discrete Algorithms, pp. 1068–1077, 2009.

Dasgupta, Sanjoy. The hardness of k-means clustering. Department of Computer Science and Engineering, University of

California, San Diego, 2008.

Gupta, Anupam, Ligett, Katrina, McSherry, Frank, Roth, Aaron, and Talwar, Kunal. Differentially private combinatorial

optimization. In ACM-SIAM symposium on Discrete Algorithms, pp. 1106–1125, 2010.

Kanungo, Tapas, Mount, David M, Netanyahu, Nathan S, Piatko, Christine D, Silverman, Ruth, and Wu, Angela Y. A local
search approximation algorithm for k-means clustering. In Annual Symposium on Computational Geometry, pp. 10–18,
2002.

Krizhevsky, Alex. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009.

Kumar, Amit, Sabharwal, Yogish, and Sen, Sandeep. Linear-time approximation schemes for clustering problems in any

dimensions. Journal of the ACM, 57(2):5, 2010.

LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. Gradient-based learning applied to document recognition. In Proceedings

of the IEEE, 1998.

Makarychev, Konstantin, Makarychev, Yury, Sviridenko, Maxim, and Ward, Justin. A bi-criteria approximation algorithm

for k means. arXiv preprint arXiv:1507.04227, 2015.

Matouˇsek, Jirı. On approximate geometric k-clustering. Discrete & Computational Geometry, 24(1):61–84, 2000.

Nock, Richard, Canyasse, Rapha¨el, Boreli, Roksana, and Nielsen, Frank. k-variates++: more pluses in the k-means++.

arXiv preprint arXiv:1602.01198, 2016.

Park, Mijung, Foulds, Jimmy, Chaudhuri, Kamalika, and Welling, Max. Practical privacy for expectation maximization.

arXiv preprint arXiv:1605.06995, 2016.

Su, Dong, Cao, Jianneng, Li, Ninghui, Bertino, Elisa, and Jin, Hongxia. Differentially private k-means clustering.

Proceedings of the Sixth ACM Conference on Data and Application Security and Privacy, pp. 26–37. ACM, 2016.

In

Szegedy, Christian, Liu, Wei, Jia, Yangqing, Sermanet, Pierre, Reed, Scott, Anguelov, Dragomir, Erhan, Dumitru, Van-
houcke, Vincent, and Rabinovich, Andrew. Going deeper with convolutions. IEEE Conference on Computer Vision and
Pattern Recognition, 2015.

