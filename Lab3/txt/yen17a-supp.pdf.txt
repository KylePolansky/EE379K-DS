7. Appendix
7.1. Comparison on Time Complexity

The proposed LatentLasso algorithm runs signiﬁcantly
faster than other methods in our experiments. For exam-
ple, on the Syn1 dataset (N=1000, D=1000, K=35), the
runtime of LatentLasso is 398s, while MCMC, Variational,
MF-Binary and BP-Means all take more than 10000s to ob-
tain their best results reported in the Figures (and the im-
plementation of Spectral Method we obtained from the au-
thors has memory requirement that restricts K¡14). On the
real data sets, we report only up to K=50 because most of
the compared methods already took one day to train.
The complexity of each algorithm can be summarized in
Table 2. The reason for the smaller runtime of LatentLasso
is due to the decoupling of factor N D from the factor re-
lated to K, where the factor O(N D) comes from the cost
of solving a MAX-CUT-like problem using the method of
(Boumal et al., 2016) or (Wang & Kolter, 2016), while the
factor O(K 2D) comes from the cost of solving a least-
square problem given by (11) with the maintenance cost
of Z T Z amortized.

7.2. Proof for Theorem 1
Let L(M ) be a smooth function such that ∇L(M ) is
Lipschitz-continuous with parameter β, that is,

L(M(cid:48)) − L(M ) − (cid:104)∇L(M ), M(cid:48) − M(cid:105) ≤ β
2

(cid:107)M(cid:48) − M(cid:107)2
F .

Then

is Lipschitz-continuous with parameter γ, which is of or-
der O(1) when loss function L(.) is an empirical average
normalized by N D.
Let A be the active set before adding ˆj. Consider the de-
scent amount produced by minimizing F (c) w.r.t.
the cˆj
given that 0 ∈ ∂jF (c) for all j ∈ A due to the subproblem
solved in the previous iteration. Let j = ˆj, for any ηj we
have

F (c + ηjej) − F (c) ≤ ∇jf (c)ηj + λ|ηj| +

γ
2
≤ µ∇j∗ f (c)ηj + λ|ηj| +

η2
j
γ
2

η2
j

∇jf (c) = zT

j ∇L(M )zj

+ (1 − µ)λ

|ηk|

Latent Feature Lasso

Minimize w.r.t ηj gives

|ηk|

(cid:88)

k∈A

F (c + ηjej) − F (c)

µ∇j∗ f (c)ηj + λ|ηj| +

γ
2

η2
j

µ∇kf (c)ηk + λ|ηk|

min
ηj
≤ min

ηj

(cid:18)

k /∈A

(cid:18)
(cid:88)
(cid:88)
(cid:88)

k /∈A

k /∈A

= min
ηk:k /∈A

≤ min

ηk:k /∈A µ
+ (1 − µ)λ

∇kf (c)ηk + λ|ηk|

(cid:19)
(cid:19)

(cid:33)2
(cid:33)2

|ηk|

|ηk|

(cid:32)(cid:88)
(cid:32)(cid:88)

k /∈A

k /∈A

+

+

γ
2

γ
2

where the last equality is justiﬁed later in Lemma 1. For
k ∈ A, we have

0 = min

ηk:k∈A µ

(∇kf (c)ηk + λ|ck + ηk| − λ|ck|)

min
ηˆj
≤ min

Combining cases for k /∈ A and k ∈ A, we can obtain a
global estimate of descent amount compared to some opti-
mal solution x∗ as follows
(cid:18)
F (c + ηˆjeˆj) − F (c)
(cid:32)(cid:88)
(cid:18)

(cid:104)∇f (c), η(cid:105) + λ(cid:107)c + η(cid:107)1 − λ(cid:107)c(cid:107)1

+ (1 − µ)λ

(cid:88)

(cid:33)2

(cid:33)2

|ηk|

|ηk|

(cid:19)

(cid:19)

k /∈A

k /∈A

γ
2

+

µ

η

≤ min

η

F (c + η) − F (c)

µ

+

γ
2

|ηk|

(cid:32)(cid:88)
(cid:19)

k /∈A

(cid:88)
(cid:18)

µ

≤ min
α∈[0,1]
+ α(1 − µ)λ(cid:107)c∗(cid:107)1
≤ min
α∈[0,1]
+ α(1 − µ)λ(cid:107)c∗(cid:107)1.

−αµ

(cid:18)

k /∈A
F (c + α(c∗ − c)) − F (c)

+

(cid:107)c∗(cid:107)2

1

αγ
2

(cid:19)

F (c) − F (c∗)

+

α2γ
2

(cid:107)c∗(cid:107)2

1

It means we can always choose an α small enough to guar-
antee descent if

F (c) − F (c∗) >

(1 − µ)

µ

λ(cid:107)c∗(cid:107)1.

(23)

In addition, for

F (c) − F (c∗) ≥ 2(1 − µ)

µ

λ(cid:107)c∗(cid:107)1,

(24)

Latent Feature Lasso

Table 2: Comparison of Time Complexity. (T denotes number of iterations)

Methods

MCMC

Time Complexity

(N K 2D)T

Variational MF-Binary
(N K 2D)T
(N K)2K

BP-Means
(N K 3D)T N D + K 5log(K)

Spectral

LatentLasso

(N D + K 2D)T

we have

F (c + ηˆjeˆj) − F (c)

(cid:18)

min
ηˆj
≤ min
α∈[0,1]

− αµ
2

F (c) − F (c∗)

+

α2γ
2

(cid:107)c∗(cid:107)2
1.

(cid:19)

Minimizing w.r.t. to α gives the convergence guarantee

F (ct) − F (c∗) ≤ 2γ(cid:107)c∗(cid:107)2

1
t
for any iterate with F (ct) − F (c∗) ≥ 2(1−µ)
Lemma 1.

µ2

1

.

µ λ(cid:107)c∗(cid:107)1.

min
ηj

µ∇j∗ f (c)ηj + λ|ηj| +

γ
2

η2
j

= min
ηk:k /∈A

µ∇kf (c)ηk + λ|ηk|

(cid:18)

(cid:88)

k /∈A

(cid:19)

+

γ
2

(cid:32)(cid:88)

k /∈A

(25)

(cid:33)2

|ηk|

(26)

Proof. The minimization (34) is equivalent to

min
ηk:k /∈A

s.t.

(cid:19)

≤ C1

µ∇kf (c)ηk

k /∈A

k /∈A

|ηk|

(cid:33)2

(cid:18)
(cid:88)
(cid:32)(cid:88)
(cid:88)
(cid:88)
(cid:88)
|ηk| ≤ min{(cid:112)

∇kf (c)ηk

|ηk| ≤ C2

k /∈A

k /∈A

ηk:k /∈A µ
min

s.t.

k /∈A
and therefore is equivalent to

Proof. Since supp(c∗) = A∗, and c∗ is optimal when
restricted on the support, we have (cid:104), c∗(cid:105) = 0 for some
∈ ∂F (c∗). And since F (c) is strongly convex on the sup-
port A∗ with parameter β, we have

F (0) − F (c∗) = F (0) − F (c∗) − (cid:104), 0 − c∗(cid:105)

≥ β
2

(cid:107)c∗ − 0(cid:107)2
2,

which gives us

.

β

(cid:107)c∗(cid:107)2

2 ≤ 2(F (0) − F (c∗))
Combining above with the fact for any c, (cid:107)c(cid:107)2
we obtain the result.
Since F (0) − F (c∗) ≤ 1
(1) and (27), we have

i=1 y2

2N

F (cT )− F (c∗) ≤ 4γ(cid:107)c∗(cid:107)0

βµ2

2(1 − µ)λ

µ

+

(cid:80)N
(cid:19)
(cid:18) 1

T

1 ≤ (cid:107)c(cid:107)0(cid:107)c(cid:107)2
2,

i ≤ 1 , from Theorem

(cid:115)

2(cid:107)c∗(cid:107)0

.

β
(28)

for any c∗ := arg minc:supp(c)=A∗ F (c).

7.4. Proof of Theorem 3

Before delving into the analysis of the Latent Feature Lasso
method, we ﬁrst investigate what one can achieve in terms
of the risk deﬁned in (1) if the combinatorial version of
objective is solved. Let

f (x; W ) := min

z∈{0,1}K

(cid:107)x − W T z(cid:107)2.

1
2

C1, C2}

Suppose we can obtain solution ˆW to the following empir-
ical risk minimization problem:

which is a linear objective subject to a convex set and thus
always has solution that lies on the corner point with only
one non-zero coordinate ηj∗, which then gives the same
minimum as (33).

7.3. Proof of Theorem 2
Lemma 2. Let A∗ ∈ [ ¯K] be a support set and c∗ :=
arg minc:supp(c)=A∗ F (c∗). Suppose F (c) is strongly con-
vex on A∗ with parameter β. We have

(cid:115)

ˆW :=

argmin

W∈RK×D:(cid:107)W(cid:107)F ≤R

1
N

f (xi; W ).

(29)

Then the following theorem holds.
Theorem 7. Let W ∗ be the minimizer of risk (1) and ˆW be
the empirical risk minimizer (29). Then

E[f (x; ˆW )] − E[f (x; W ∗)]

(cid:115)

≤ 3
N

+

DK log(4R2KN )

2N

+

1
2N

log(

1
ρ

)

(cid:107)c∗(cid:107)1 ≤

2(cid:107)c∗(cid:107)0(F (0) − F (c∗))

β

.

(27)

with probability 1 − ρ.

N(cid:88)

i=1

Latent Feature Lasso

W∈RK×D:(cid:107)W(cid:107)F ≤R

Proof Sketch. Let EN [f (x, W )] denote the empirical risk.
We have
E[f (x; ˆW )] − E[f (x; W ∗)]
≤ 2

(cid:32)

(cid:33)

sup

|E[f (x; W )] − EN [f (x; W )]|
(30)
≤
Then by introducing a δ-net N (δ)
, we have

and EN [f (x, ˆW )]

decomposition

from error
EN [f (x, W ∗)].

(cid:107) ˜W − W(cid:107)F ≤ δ for some ˜W ∈ N (δ) and

with covering number |N (δ)| = (cid:0) 4R
(cid:1)DK
(cid:32)
(cid:12)(cid:12)(cid:12)(cid:12) ≤ 
(cid:12)(cid:12)(cid:12)(cid:12)E[f (x; ˜W )] − EN [f (x; ˜W )]
(cid:18) 4R
(cid:19)DK

˜W∈N (δ)

sup

P

δ

(cid:33)

exp(−2N 2).

≥ 1 −

δ

Then since
2(f (x, ˜W ) − f (x, W )) ≤ (cid:107)x − ˜W T z∗(cid:107)2 − (cid:107)x − W T z∗(cid:107)2
= z∗T (W − ˜W )x + (cid:104) ˜W ˜W T − W W T , z∗z∗T(cid:105)
≤ (cid:107)z∗(cid:107)2(cid:107)W − ˜W(cid:107)F + 2R(cid:107) ˜W − W(cid:107)F(cid:107)z∗(cid:107)2

2 ≤ 3RK(cid:107) ˜W − W(cid:107)F ,

we have

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)E[f (x; W )] − EN [f (x; W )]
(cid:115)

(cid:12)(cid:12)(cid:12)(cid:12)
(cid:12)(cid:12)(cid:12)(cid:12)E[f (x; ˜W )] − EN [f (x; ˜W )]

sup

W :(cid:107)W(cid:107)F ≤R
≤ (3RKδ) + sup
˜W∈N (δ)

≤ 3RKδ +

DK
2N

log(

) +

log(

)

4R
δ

1
2N

1
ρ

(32)
with probability 1− ρ. Choosing δ = 1/(RKN ) yields the
result.

Now we establish the proof of Theorem (3) for bounding
risk of the Latent Feature Lasso estimator.

1

N (cid:107)X−ZW ∗(cid:107)2

Proof. Let Z∗ ∈ arg minZ∈{0,1}NK
F and
S∗ be the set of column index of Z with the same 0-1 pat-
terns to columns in Z∗. Let c∗ be indicator vector with
k = 1, k ∈ S∗ and c∗
c∗
F + λ(cid:107)c∗(cid:107)1
F (¯c) ≤ F (c∗) ≤ EN [f (x; W ∗)] +
(33)
where ¯c ∈
F (c). Then let (c, W ) with
supp(c) = ˆS be the output obtained from running T it-

k = 0, k /∈ S∗. We have
(cid:107)W ∗(cid:107)2

c:supp(c)=S∗

argmin

τ
2

erations of the greedy algorithm, we have

EN [f (x, DcW )] +

(cid:107)W(cid:107)2

F + λ(cid:107)c(cid:107)1

τ
2

N(cid:88)

i=1

1
=
2N
≤ F (c)

min

z∈{0,1}(cid:107)c(cid:107)0

(cid:107)xi − W T DT

c z(cid:107)2 +

(cid:107)W(cid:107)2

F + λ(cid:107)c(cid:107)1

τ
2

(34)
Combining (33), (34) and (28), we obtain a bound on the
bias and optimization error of the Latent Feature Lasso es-
timator

EN [f (x, DcW )] ≤ F (c) ≤ EN [f (x; W ∗)]

(cid:18) 1

(cid:19)

(cid:115)
(cid:123)(cid:122)

+

optimization error

2(1 − µ)K

µβ

λ

(cid:125)

+

τ
2

(cid:124)

(31)

(cid:123)(cid:122)

(cid:107)W ∗(cid:107)2 + λK

+

2γK

β

T

(cid:125)

(cid:124)

regularize bias

(35)
To bound the estimation error, notice that the matrix ˆW :=
DcW is ˆK × D with ˆK ≤ T . Furthermore, the descent
condition F (c) ≤ F (0) guarantees that

τ
2

F + λ(cid:107)c(cid:107)1 ≤ 1
N

(cid:107)W(cid:107)2
F ≤ 1/τ, (cid:107)c(cid:107)1 ≤ 1/λ.

and thus (cid:107)W(cid:107)2

Let W(T, λ, τ ) := { ˆW ∈ (RT×D) | (cid:107) ˆW(cid:107)F ≤(cid:112)1/(λτ )}.

(cid:107)X − 0(cid:107)2 ≤ 1

We have

sup

(c,W )∈W(T,λ,τ )

(cid:115)

E[f (x; ˆW )] − EN [f (x, ˆW )]

≤

DT log(4T N/(τ λ))

2N

+

1
2N

log(

1
ρ

)

with probability 1 − ρ through the same argument as in
the case of combinatorial objective (32). Combining the
above estimation error with the bias and optimization error
in (35), we have

E[f (x; W )] − E[f (x; W ∗)]

(cid:115)

R2 + λK +

2γK
βT

+

2(1 − µ)K

µβ

λ

DT log(4T N/(τ λ))

2N

+

)

1
ρ

log(

1
2N
and N (cid:38) DT

2 =

Choosing T = 2γK
3 gives the result.
DK

β ( 1

 ), λ = τ = 1√

N

7.5. Proof of Theorem 4
Proof. Since W ∗ is of rank K, we have span(Θ∗) =
span(Z∗). Therefore, from condition 2,

span(Θ∗) ∩ {0, 1}N \ {0} = {Z∗

:,j}K

j=1.

(36)

≤ τ
2

(cid:115)

+

Latent Feature Lasso

For any (Z, W ) : ZW = Θ∗, we have Z ∈ span(Θ∗)
since Z = Θ∗V Σ−1U T where U ΣV T is the SVD of W
with Σ : K×K. Then by (36) we know that Z = Z∗. Then
it follows W = W ∗ since the linear system Θ∗ = Z∗W
has unique solution for W .

7.6. Proof of Theorem 5

Proof. The solution of (21) satisﬁes

ZSWS = X = Z∗W ∗.

Since WS has full row-rank, we have rank(ZS) =
rank(X) = rank(Z) = K by condition 1 in Theo-
rem 4. Then let WS = U ΣV T be the SVD of WS with
Σ : |S| × |S|, we have

where A∗ = (I − P ) + (I − P )(ZSW ∗) where

P = ZS(Z T

S ZS + N τ I)−1Z T
S .

Let κn be the restricted strong convexity term deﬁned as :

κn := inf

∆∈C {f (c∗ + ∆) − f (c∗) − (cid:104)∇f (c∗), ∆(cid:105)} ,

where C = {c|(cid:107)cSc(cid:107)1 ≤ 3(cid:107)cS(cid:107)1}. Then, if the regulariza-
tion parameter is set as λ ≥ ρn, we have the following
bound on the norm of the error ˆc − c∗:
√
K∗

(cid:107)ˆc − c∗(cid:107)2 ≤ ρn

.

κn

ZS = XV Σ−1U T = Z∗W ∗V Σ−1U T ∈ span(Z∗).

7.9. Proof of Theorem 6

Proof. Note that the optimization problem in Equation (22)
can be rewritten as:

1

2N (cid:107)E + (Z∗ − Z)(cid:107)2

2

(Ei + (Z∗

i − Zi))2

argmin
Zi∈{0,1}

(38)

argmin
Z∈{0,1}N

N(cid:88)

i=1

= 1
2N

(cid:40)

So, we have the following closed form expression for ˆZ:

√

K∗λ
κn

,

ˆZi =

i + Ei ≥ 0.5

if Z∗
o.w

.

1
0

Then by condition 2 in Theorem 4, the columns of ZS can
only be in {Z∗
j=1, which implies ZS equal to Z∗ up to
a permutation. Then we know |S| = K and by Theorem 4
WS also equals W ∗ up to a permutation.

:,j}K

7.7. Proof of Theorem 8

Proof. By an application of Theorem1 of (Negahban et al.,
2009), for λ ≥ (cid:107)∇f (c∗)(cid:107)∞, we have the following bound
on the (cid:96)2 norm of ˆc − c∗:

(cid:107)ˆc − c∗(cid:107)2 ≤
where (cid:107)∇f (c∗)(cid:107)∞ is given by:
(cid:107)∇f (c∗)(cid:107)∞ = max
z∈{0,1}N

1

2N 2τ

(cid:107)A∗T z(cid:107)2
2,

where A∗ is deﬁned as:
A∗ =

(I − ZS(Z T

S ZS + N τ I)−1Z T

S )X

= (I − ZS(Z T

S ZS + N τ I)−1Z T

Given P = ZS(Z T
A∗ can be rewritten as :

S ZS + N τ I)−1Z T

S )(ZSW ∗ + )
(37)
S , it can be seen that

A∗ = (I − P ) + (I − P )(ZSW ∗).

7.8. ell2 error bounds on the coefﬁcient vector ˆc
Theorem 8. Let c∗ be the true underlying vector, with sup-
port S and sparsity K∗. Let ˆc be the minimizer of F (c),
deﬁned in Equation (7). Deﬁne the noise-level term

ρn := max

z∈{0,1}N

1

2N 2τ

(cid:107)A∗T z(cid:107)2
2,

We now compute the probability that Z∗
i (cid:54)= ˆZi) = P(Ei ≥ 0.5) ∗ P(Z∗
P(Z∗

i (cid:54)= ˆZi:

i = 0)
+P(Ei ≤ −0.5) ∗ P(Z∗

i = 1)

≥ min{P(Ei ≥ 0.5), P(Ei ≤ −0.5)} ≥ c,
(39)
for some positive constant c. We now use the fact that
(cid:54)= ˆZi) to complete the proof of
E((Z∗
the Lemma.

i − ˆZi)2) = P(Z∗

i

