Supplementary Material: Asynchronous Stochastic Gradient Descent with

Delay Compensation

A. Theorem 3.1 and Its Proof
Theorem 3.1:
Assume the loss function is L1-Lipschitz. If (cid:21) 2 [0; 1] make the following inequality holds,

K∑

1

(cid:21) 2

k(x; wt)
(cid:27)3

k=1

(

K∑

24Cij

)2

35 ;

1

(cid:27)k(x; wt)

k=1

′
ijL2

1jϵtj

+ C

(1)

′
ij =

p
1+(cid:21) ( uiuj (cid:12)

1

lilj

(cid:11) )2, C

(1+(cid:21))(cid:11)(lilj )2 , and the model converges to the optimal model, then the MSE of (cid:21)G(wt)

where Cij = 1
is smaller than the MSE of G(wt) in approximating Hessian H(wt).
Proof:
For simplicity, we abbreviate E(Y jx;w(cid:3)) as E, Gt as G(wt) and Ht as H(wt). First, we calculate the MSE of Gt, (cid:21)Gt
to approximate Ht for each element of Gt. We denote the element in the i-th row and j-th column of G(wt) as Gt
ij and
H(wt) as Hij(t).
The MSE of Gt
ij:

E(Gt

ij (cid:0) EH t

ij)2 = E(Gt

ij (cid:0) EGt

ij)2 + (EH t

ij (cid:0) EGt

ij)2 = E(Gt

ij)2 (cid:0) (EGt

ij)2 + ϵ2
t

The MSE of (cid:21)gij:

E((cid:21)Gt

ij (cid:0) EH t

ij)2 = (cid:21)2E(Gt
= (cid:21)2E(Gt

ij)2 + (EH t

ij (cid:0) EGt
ij)2 (cid:0) (cid:21)2(EGt

ij (cid:0) (cid:21)EGt

ij)2
ij)2 + (1 (cid:0) (cid:21))2(EGt

ij)2 + ϵ2

t + 2((cid:21) (cid:0) 1)EGt

ijϵt

The condition for E(Gt

ij

(cid:0) EH t

ij)2 (cid:21) E((cid:21)Gt

ij

(cid:0) EH t

ij)2 is

(1 (cid:0) (cid:21)2)(E(Gt

ij)2 (cid:0) (EGt

ij)2) (cid:21) 2(1 (cid:0) (cid:21))(EGt

ij)2 + 2((cid:21) (cid:0) 1)EGt

ijϵt

Inequality (4) is equivalent to

(1 + (cid:21))E(Gt

ij)2 (cid:21) 2[(EGt

ij)2 (cid:0) EGt

ijϵt]

(2)

(3)

(4)

(5)

Next we calculate E(Gt

ij)2, and (EGt

ij)2 which appear in Eqn.(5). For simplicity, we denote (cid:27)k(x; wt) as (cid:27)k, and I[Y =k]

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

as zk. Then we can get:

(

2

)

2

log P (Y jx; wt)

@

@wj

))2

(

(cid:0) zk
(cid:27)k

@(cid:27)k
@wj

E(gij)2 = E(Y jx;wt)

@

@wi

(
(
K∑
(
(cid:21) E(Y jx;w(cid:3))
K∑
(
K∑
(
K∑

E(Y jx;w(cid:3))

= (cid:11) (lilj)2

(cid:20) (cid:12)2 (uiuj)2

k=1

k=1

k=1

)
(
log P (Y jx; wt)

))4
)

(cid:0) zk
(cid:27)k

(lilj)2

)
(cid:1) K∑
)2

k=1

:

k(x; wt)
(cid:27)3

1

(

@(cid:27)k
@wi

(cid:0) zk
(cid:27)k

1

(cid:27)k(x; wt)

k=1

(Ehij)2 =

]

(6)

(7)

(8)

(cid:21)

2

By substituting Ineq.(7) and Ineq.(8) into Ineq.(5), a sufﬁcient condition for Ineq.(5) to be satisﬁed is

[

(∑

)

2

Cij

K
k=1

1

(cid:27)k(x;wt)

2

′
ijL2

1jϵtj

+ C

because Gt
ij

(cid:20) L2

1. (cid:3)

∑

K
k=1

1

k(x;wt)
(cid:27)3

B. Corollary 3.2 and Its Proof
[
Corollary 3.2: A sufﬁcient condition for inequality (1) is (cid:21) 2 [0; 1] and 9k0 2 [K] such that (cid:27)k0
1 (cid:0)

]

; 1

.

K
k=1

(cid:27)k(x;wt)

′
ijL2
1

jϵtj. If 9k1 2 [K] such

K(cid:0)1
′
ij L2
2(Cij K2+C

1ϵt)

Proof:
Denote ∆ = K(cid:0)1
that (cid:27)k1

∑

(∑

1

)2 (cid:0) 2C
)2 (cid:0) 2C
)

′
ijL2
1

jϵtj

1

K
k=1

2Cij K2 and F ((cid:27)1; :::; (cid:27)K) =

(cid:0) 2Cij
2 [1 (cid:0) ∆; 1], we have for k ̸= k1 (cid:27)k 2 [0; ∆]. Therefore
(
)2

k(x;wt)
(cid:27)3

+

∆3

1
(cid:27)k1

F ((cid:27)1; :::; (cid:27)K) (cid:21) 1
((cid:27)k1 )3 +
(cid:21) K (cid:0) 1
(cid:21) K (cid:0) 1
(
(cid:0) 2Cij
K (cid:0) 1
(
∆2
K (cid:0) 1
(
∆2
K (cid:0) 1
∆

1
∆
(cid:21) 1
∆
(cid:21) 1
∆2

K (cid:0) 1
((
(cid:0) 2Cij
∆3
K (cid:0) 1
(
(cid:0) 2Cij
∆
(K (cid:0) 1)2
(
(

1
+
+
(cid:27)2
k1
2K (cid:0) 1
(cid:27)k1 ∆

K (cid:0) 1
∆
2(K (cid:0) 1)
)
(cid:27)k1 ∆
))
jϵtj
(cid:0) 2C
′
ijL2
+
1
∆2
2K (cid:0) 1
(K (cid:0) 1)2
))
(cid:0) 2C
(cid:27)k1
(K (cid:0) 1)2 + 2K (cid:0) 1
)
(cid:0) 2C

(cid:0) 2Cij
(cid:0) 2Cij
(cid:0) 2CijK 2 (cid:0) 2C

′
ijL2
1

jϵtj

∆3

∆

∆

+

=

(cid:0) 2C

′
ijL2
1

′
ijL2
1

′
ijL2
1

jϵtj
jϵtj

= 0

jϵtj

(9)

(10)

(11)

(12)

(13)

(14)

(15)

where Ineq.(11) and (13) is established since (cid:27)k1 > ∆; and Eqn.(15) is established by putting ∆ =
Eqn.(14). (cid:3)

K(cid:0)1
′
ij L2
2(Cij K2+C
1

in

jϵtj)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

C. Uniform upper bound of MSE
Lemma C.1 Assume the loss function is L1-Lipschitz, and the diagonalization error of Hessian is upper bounded by ϵD,
i.e., jjDiag(H(wt)) (cid:0) H(wt)jj (cid:20) ϵD, 1 then we have, for 8t,

mset(Diag((cid:21)G)) (cid:20) 4(cid:21)2V1 + 4(1 (cid:0) (cid:21))2L4

1 + 4ϵ2

t + 4ϵD;

where V1 is the upper bound of the variance of G(wt).

Proof:

mset(Diag((cid:21)G))
(cid:20)E∥Diag((cid:21)G(wt)) (cid:0) H(wt)∥2
(cid:20)4E∥Diag((cid:21)G(wt)) (cid:0) E(Diag((cid:21)G(wt)))∥2 + 4∥E(Diag((cid:21)G(wt))) (cid:0) E(Diag(G(wt)))∥2

+ 4∥E(Diag(G(wt))) (cid:0) E(Diag(H(wt)))∥2 + 4∥E(Diag(H(wt))) (cid:0) EH(wt)∥2

(cid:20)4(cid:21)2V1 + 4(1 (cid:0) (cid:21))2L4

1 + 4ϵ2

t + 4ϵD

(16)

(17)
(18)
(19)
(20)
(21)

D. Convergence Rate for DC-ASGD: Convex Case
DC-ASGD is a general method to compensate delay in ASGD. We ﬁrst show the convergence rate for convex loss function.
∥w∥2 to make the objective function
If the loss function f (w) is convex about w, we can add a regularization term (cid:26)
2
F (w) + (cid:26)
2
Theorem 4.1: (Strongly Convex) If f (w) is L2-smooth and (cid:22)-strongly convex about w, ∇f (w) is L3-smooth about w
and the expectation of the ∥ (cid:1) ∥2
2 norm of the delay compensated gradient is upper bounded by a constant G. By setting the
learning rate (cid:17)t = 1

∥w∥2 strongly convex. Thus, we assume that the objective function is (cid:22)-strongly convex.

(cid:22)t , DC-ASGD has convergence rate as

EF (wt) (cid:0) F (w

(cid:3)

) (cid:20) 2L2
2G2
t(cid:22)4

(1 + 4(cid:28) C(cid:21)) +

p
2G2L2
2(cid:18)
(cid:22)4t
t

+

L3L3

2(cid:28) 2G3
(cid:22)6t2

;

p
(cid:28)

√

) and C(cid:21) = (1 (cid:0) (cid:21))L2

(cid:22)

L2
(cid:22) (1 + (cid:28) GL3
(cid:22)L2

1 + ϵD, and the expectation is taking with respect to the random

where (cid:18) = 2HKLG
sampling of DC-ASGD and E(yjx;w(cid:3)).
Proof:
We denote gdc(wt) = g(wt) + (cid:21)g(wt) ⊙ g(wt) ⊙ (wt+(cid:28) (cid:0) wt), gh(wt) = g(wt) + Hit (wt)(wt+(cid:28) (cid:0) wt) and ∇F h(wt) =
∇F (wt) + EitHit (wt)(wt+(cid:28) (cid:0) wt). Obviously, we have Egh(wt) = ∇F h(wt). By the smoothness condition, we have
(22)

EF (wt+(cid:28) +1) (cid:0) F (w

(cid:3)

)

(cid:20) F (wt+(cid:28) ) (cid:0) F (w
(cid:20) F (wt+(cid:28) ) (cid:0) F (w
= F (wt+(cid:28) ) (cid:0) F (w

(cid:3)

(cid:3)

(cid:3)

) (cid:0) ⟨∇F (wt+(cid:28) ); wt+(cid:28) +1 (cid:0) wt+(cid:28)⟩ +
) (cid:0) (cid:17)t+(cid:28)⟨∇F (wt+(cid:28) ); gdc(wt)⟩ +
) (cid:0) (cid:17)t+(cid:28)⟨∇F (wt+(cid:28) );∇F (wt+(cid:28) )⟩ + (cid:17)t+(cid:28)⟨∇F (wt+(cid:28) );∇F (wt+(cid:28) ) (cid:0) ∇F h(wt)⟩

L2
2
L2(cid:17)2
t+(cid:28) G2
2

∥wt+(cid:28) +1 (cid:0) wt+(cid:28)∥2

+(cid:17)t+(cid:28)⟨∇F (wt+(cid:28) ); Egh(wt) (cid:0) gdc(wt)⟩ +

L2(cid:17)2
t+(cid:28) G2
2

Since f (w) is L2-smooth and (cid:22) strongly convex, we have

(cid:0)⟨∇F (wt+(cid:28) );∇F (wt+(cid:28) )⟩ (cid:20) (cid:0)(cid:22)2∥wt+(cid:28) (cid:0) w

(cid:3)∥2 (cid:20) (cid:0) 2(cid:22)2
L2

(F (wt+(cid:28) ) (cid:0) F (w

(cid:3)

)):

(23)

(24)

(25)

(26)

(27)

1(LeCun, 1987) demonstrated that the diagonal approximation to Hessian for neural networks is an efﬁcient method with no much

drop on accuracy

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

For the term (cid:17)t+(cid:28)⟨∇F (wt+(cid:28) );∇F (wt+(cid:28) ) (cid:0) ∇F h(wt)⟩, we have

(cid:17)t+(cid:28)⟨∇F (wt+(cid:28) );∇F (wt+(cid:28) ) (cid:0) ∇F h(wt)⟩
(cid:20) (cid:17)t+(cid:28)∥∇F (wt+(cid:28) )∥∥∇F (wt+(cid:28) ) (cid:0) ∇F h(wt)∥
(cid:20) (cid:17)t+(cid:28) G∥∇F (wt+(cid:28) ) (cid:0) ∇F h(wt)∥

(cid:17)t+j(∥E((cid:21)g(wt) ⊙ g(wt) (cid:0) g(wt) ⊙ g(wt)∥ + ∥g(wt) ⊙ g(wt) (cid:0) Diag(H(wt))∥ + ∥Diag(H(wt)) (cid:0) H(wt)∥)

(28)
(29)
(30)

(31)

(32)
(33)

(34)

(35)

(36)

(37)

(38)

(39)

By the smoothness condition for ∇F (w), we have

∥∇F (wt+(cid:28) ) (cid:0) ∇F h(wt)∥ (cid:20) L3
2

∥wt+(cid:28) (cid:0) wt∥2 (cid:20) L3(cid:28) G2

2

∑

(cid:28)(cid:0)1∑

j=0

(cid:17)2
t+j

(cid:20) 2L2
Let (cid:17)t = L2
For the term (cid:17)t+(cid:28)⟨∇F (wt+(cid:28) ); Egh(wt) (cid:0) gdc(wt)⟩, we have

(cid:22)2t, we can get

(cid:20) L2
(cid:22)4 (cid:1)

(cid:28)
j=1 (cid:17)2

t(t+(cid:28) )

t+j

2(cid:28)

(cid:22)4(t+(cid:28) )2 .

(cid:28)

2

⟨∇F (wt+(cid:28) ); E(gh(wt) (cid:0) gdc(wt))⟩
(cid:20) ∥∇F (wt+(cid:28) )∥∥E((cid:21)g(wt) ⊙ g(wt) (cid:0) H(wt))(wt+(cid:28) (cid:0) wt)∥
(cid:20) G2(cid:28)

(cid:28)(cid:0)1∑

j=0

(cid:20) 2G2L2(cid:28)

(t + (cid:28) )(cid:22)2 (C(cid:21) + ϵt);
where C(cid:21) = (1 (cid:0) (cid:21))L2
Using Lemma F.1, ϵt (cid:20) (cid:18)

√

1 + ϵD.

(cid:20) (cid:18)

1
t

√

(cid:28)

t+(cid:28) . Putting inequality 27 and 31 in inequality 26, we have

EF (wt+(cid:28) +1) (cid:0) F (w

(cid:3)

) (cid:20)

1 (cid:0) 2
t + (cid:28)

+

2G2L2
2(cid:28)
(cid:22)4(t + (cid:28) )2

)
(EF (wt) (cid:0) F (w

√

(cid:3)

)) +

C(cid:21) + (cid:18)

(cid:28)

t + (cid:28)

+

L3L3
2(cid:28) 2G3
(cid:22)6(t + (cid:28) )3
L2

2G2

2(t + (cid:28) )2(cid:22)4

(

)
(

We can get

EF (wt) (cid:0) F (w

(cid:3)

) (cid:20) 2L2
2G2
t(cid:22)4

(1 + 4(cid:28) C(cid:21)) +

p

p
2G2L2
2(cid:18)
(cid:22)4t
t

(cid:28)

+

L3L3

2(cid:28) 2G3
(cid:22)6t2

:

by induction. (cid:3)
Discussion:
(1). Following the above proof steps and using ∥∇F (wt+(cid:28) ) (cid:0) ∇F (wt)∥ (cid:20) L2∥wt+(cid:28) (cid:0) wt∥, we can get the convergence
rate of ASGD is

EF (wt) (cid:0) F (w

(cid:3)

) (cid:20) 2L2
2G2
t(cid:22)4

(1 + 4(cid:28) L2) :

p

p
Compared the convergence rate of DC-ASGD with ASGD, the extra term 2G2L2
2(cid:18)
(cid:22)4t
t
2G2
than 2L2
t(cid:22)4
t is large and the term can be neglected. Then the condition for DC-ASGD outperforming ASGD is L2 > C(cid:21).

converge to zero faster
(1 + 4(cid:28) C(cid:21)) in terms of the order of t. Thus, when t is large, the extra term has smaller value. We assume that

2(cid:28) 2G3
(cid:22)6t2

+ L3L3

(cid:28)

E. Convergence Rate for DC-ASGD: Nonconvex Case
Theorem 5.1: (Nonconvex Case) Assume that Assumptions 1-4 hold. Set the learning rate

√

(cid:17)t =

2(F (w1) (cid:0) F (w(cid:3))

bT V 2L2

;

(40)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

where b is the mini-batch size, and V is the upper bound of the variance of the delay-compensated gradient. If T (cid:21)
maxfO(1=r4); 2D0bL2=V 2g and delay (cid:28) is upper-bounded as below,
√

√

√

√

{

}

(cid:28) (cid:20) min

L2V
C(cid:21)

L2T
2D0b

;

V
C(cid:21)

L2T
2D0b

;

T V
~C

L2
bD0

;

V L2T
4 ~C

T L2
2D0b

:

then DC-ASGD has the following ergodic convergence rate,

E(∥∇F (wt)∥2) (cid:20) V

min

t=f1;(cid:1)(cid:1)(cid:1) ;Tg

√

2D0L2

bT

;

(41)

(42)

).

where the expectation is taken with respect to the random sampling in SGD and the data distribution P (Y jx; w(cid:3)
Proof:
We denote gm(wt) + (cid:21)gm(wt)⊙ gm(wt)⊙ (wt+(cid:28) (cid:0) wt) as gdc
minibatch. From the proof the Theorem 1 in ASGD (Lian et al., 2015), we can get

m (wt) where m 2 f1;(cid:1)(cid:1)(cid:1) ; bg is the index of instances in the

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

Egdc

m (wt)

E

0@(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) b∑
1A
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∇F (wt+(cid:28) ) (cid:0) b∑

gdc
m (wt)

m=1

(cid:0)

m=1

2

E

m=1

m=1

+

2

L2
2

Egdc

Egdc

m (wt)

(cid:17)2
t+(cid:28) L2

(cid:17)2
t+(cid:28) L2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

m (wt)⟩ +

(cid:20) (cid:0) b(cid:17)t+(cid:28)
2

∥wt+(cid:28) +1 (cid:0) wt+(cid:28)∥2

(cid:20) (cid:0)(cid:17)t+(cid:28)⟨∇F (wt+(cid:28) );

EF (wt+(cid:28) +1) (cid:0) F (wt+(cid:28) )
(cid:20) ⟨∇F (wt+(cid:28) ); wt+(cid:28) (cid:0) wt⟩ +
b∑
0@∥∇F (wt+(cid:28) )∥2 +
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) b∑
1A
0@(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) b∑
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)∇F (wt+(cid:28) ) (cid:0)∑
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∇F (wt+(cid:28) ) (cid:0) b∑
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∇F (wt+(cid:28) ) (cid:0) ∇F h(wt) + ∇F h(wt) (cid:0) b∑
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)∇F h(wt) (cid:0) b∑
(cid:13)(cid:13)(cid:13)(cid:13) L3

∥wt+(cid:28) (cid:0) wt∥2

(cid:13)(cid:13)(cid:13)(cid:13)2

gdc
m (wt)

m (wt)

Egdc

b
m=1

T1 =

Egdc

m (wt)

+ 2

(cid:20)

m=1

m=1

m=1

Egdc

m (wt)

Egdc

m (wt)
t ))∥wt+(cid:28) (cid:0) wt∥2

m=1

3(cid:25)2=2 + 2(((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2

(cid:20) 2
(cid:20) (L2

2

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)2

For the term T1 =

, by using the smooth condition of g, we have

Thus by following the proof of ASGD, we have

E(T1) (cid:20) 4(L2

3(cid:25)2=4 + ((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2
t )

((cid:13)(cid:13)(cid:13)∑

)

(cid:13)(cid:13)(cid:13)2

For the term T2 = E

b
m=1 gdc

m (wt)

(

b(cid:28) (cid:17)2

t+(cid:28) V 2 + (cid:28) 2(cid:17)2

t+(cid:28)

(cid:13)(cid:13)(cid:13)bEgdc

m (wt)

)

(cid:13)(cid:13)(cid:13)2

:

, it has

E(T2) (cid:20) bV 2 +

(cid:13)(cid:13)(cid:13)bEgdc

m (wt)

(cid:13)(cid:13)(cid:13)2

:

1A

(43)

(44)

(45)

(46)

(47)

(48)

(49)

(50)

(51)

(52)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

By putting Ineq.(51) and Ineq.(52) in Ineq.(46), we can get

(cid:20)

E(F (wt+(cid:28) +1) (cid:0) F (wt+(cid:28) )
(
(cid:0) b(cid:17)t+(cid:28)
E∥∇F (wt+(cid:28) )∥2 +
2
(cid:17)2
t+(cid:28) bL2

+

2

(

)

E

(cid:17)2
t+(cid:28) L2

2

(cid:0) (cid:17)t+(cid:28)
2b

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2

t )b2(cid:28) (cid:17)3

t+(cid:28)

+(L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2

t )b(cid:28) 2(cid:17)3

t+(cid:28)E

m (wt)

m (wt)

((cid:13)(cid:13)(cid:13)bEgdc
((cid:13)(cid:13)(cid:13)bEgdc

)
(cid:13)(cid:13)(cid:13)2
)
(cid:13)(cid:13)(cid:13)2

)

V 2

Summarizing the Ineq.(55) from t = 1 to t + (cid:28) = T , we have

T∑
EF (wT +1) (cid:0) F (w1)
(cid:20) (cid:0) b
(
T∑
2

t=1

(cid:17)tE∥∇F (wt)∥2 +

(

T∑

t=1

(cid:17)2
t+(cid:28) bL2

2

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2
)

1 + ϵD)2 + ϵ2

t )b2(cid:28) (cid:17)3

t+(cid:28)

V 2

(cid:13)(cid:13)(cid:13)bEgdc

(cid:17)2
t L2
2

+

t=1

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2

t )b(cid:28) 2(cid:17)3

t (cid:0) (cid:17)t

E

m (wmaxft(cid:0)(cid:28);1g)

:

By Lemma F.1 and under our assumptions, we have when t > T0, wt will goes into a strongly convex neighbourhood of
some local optimal wloc. Thus, ϵt (cid:20) ϵnc + (cid:18)
. It follows that
Let (cid:17)t =

√
1=(t (cid:0) T0), when t > T0 and ϵt < maxs21;(cid:1)(cid:1)(cid:1) ;T0 ϵs when t < T0.

2(F (w1)(cid:0)F (w(cid:3))

2b

bT V 2L2

)
(cid:13)(cid:13)(cid:13)2

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2

t )b(cid:28) 2(cid:17)2
t

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + 2ϵ2

nc)b(cid:28) 2(cid:17)2
t

(cid:17)tL2

2

}

+ 2b(cid:28) 2(cid:17)2

t (4T0 max

s21;(cid:1)(cid:1)(cid:1) ;T0

(ϵs)2 + 4(cid:18)2 log(T (cid:0) T0))

(59)

(60)

We ignore the log(T (cid:0) T0) term and regards ~C 2 = 4T0 maxs21;(cid:1)(cid:1)(cid:1) ;T0 (ϵs)2 + 4(cid:18)2 log(T (cid:0) T0) as a constant, which yields

√
T∑
(cid:20) T∑

t=1

t=1

(cid:17)tL2

{

2

(53)

(54)

(55)

(56)

(57)

(58)

(61)

(62)

(63)

(64)

(65)

(66)

(67)

T∑
(cid:20) T∑

t=1

t=1

(cid:17)tL2

{

2

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + ϵ2

t )b(cid:28) 2(cid:17)2
t

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + 2ϵ2

nc)b(cid:28) 2(cid:17)2
t

(cid:17)tL2

2

(cid:17)t should be set to make

(

T∑

t=1

(cid:17)2
t L2
2

+ (L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + 2ϵ2

nc)b(cid:28) 2(cid:17)3

t +

}

+ 2(cid:28) 2(cid:17)2

t b ~C 2

)

(cid:20) 0:

t b ~C 2
2(cid:28) 2(cid:17)3
T

(cid:0) (cid:17)t
2b

Then we can get

1
T

T∑

E∥∇F (wt)∥2

t=1

(cid:3)

(cid:20) 2(F (w1) (cid:0) F (w
(cid:20) 2(F (w1) (cid:0) F (w

(cid:3)

)

bT (cid:17)t

) + T b((cid:17)2

t L2 + 2(L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + 2ϵ2

nc)b(cid:28) (cid:17)3

t )V 2 + (cid:17)3

t

~C24b(cid:28)
T

V 2

bT (cid:17)t

+ ((cid:17)tL2 + 2(L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + 2ϵ2

nc)b(cid:28) (cid:17)2

t )V 2 +

(cid:17)2
t

~C 24b(cid:28) V 2

T

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

We set (cid:17)t to make

(2(L2

3(cid:25)2=2 + 2((1 (cid:0) (cid:21))L2

1 + ϵD)2 + 2ϵ2

nc)b(cid:28) (cid:17)2

t ) +

(cid:17)2
t

~C 24b(cid:28)
T

(cid:20) (cid:17)tL2

√

Thus let (cid:17)t =

2(F (w1)(cid:0)F (w(cid:3))

bT V 2L2

,

And we can get the condition for T by putting (cid:17) in ineq.63 and ineq.68, we can get that

√

2D0L2

bT

:

E∥∇F (wt)∥2 (cid:20) V

T∑

t=1

√

√

1
T

√

{

√

}

(cid:28) (cid:20) min

L2V
C(cid:21)

L2T
2D0b

;

V
C(cid:21)

L2T
2D0b

;

T V
~C

L2
bD0

;

V L2T
4 ~C

T L2
2D0b

:

(68)

(69)

(70)

K

F. Decreasing rate of the approximation error ϵt
Since ϵt is contained the proof of the convergence rate for DC-ASGD , in this section we will introduce a lemma which
describes the approximation error ϵt the for both convex and nonconvex cases.
Lemma F.1 Assume that the true label y is generated according to the distribution P(Y = kjx; w

) and
k=1(I[y=k] log (cid:27)k(x; w)). If we assume that the loss function is (cid:22)-strongly convex about w. We denote wt is

f (x; y; w) = (cid:0)∑

) = (cid:27)k(x; w

the output of DC-ASGD by using the outerproduct approximation of Hessian, we have

(

(

@w2 f (x; y; wt) (cid:0) E(x;yjw(cid:3))

(cid:12)(cid:12)(cid:12)E(x;yjw(cid:3))
(cid:12)(cid:12)(cid:12) (cid:20) H, 8k; x; w, each (cid:27)k(w) is L-Lipschitz continuous about w. We denote wt is the out-

the loss function is (cid:22)-strongly convex in a neighborhood of each local optimal d(wloc; r),
1

@
@w f (x; y; wt)

@
@w f (x; y; wt)

(cid:22) (1 + L2+(cid:21)L2

)(cid:12)(cid:12)(cid:12) (cid:20) (cid:18)

(cid:12)(cid:12)(cid:12) @2P(Y =kjx;w)

where (cid:18) = 2HKLV L2

If we assume that

ϵt =

√

√

)

1

(cid:28) ).

1
t

;

(cid:2)

(cid:10)

@2

L2

(cid:22)2

(cid:3)

(cid:3)

1

@2w

put of DC-ASGD by using the outerproduct approximation of Hessian, we have

@2

@w2 f (x; y; wt) (cid:0) E(x;yjw(cid:3))

@
@w f (x; y; wt)

1

t (cid:0) T0

+ ϵnc:

(

)
@
@w f (x; y; wt)

(cid:10)

(

√

)(cid:12)(cid:12)(cid:12) (cid:20) (cid:18)

P (Y =kjx;w)

(cid:12)(cid:12)(cid:12)E(x;yjw(cid:3))

ϵt =

where t > T0 (cid:21) O( 1
r8 ).
Proof:

E(yjx;w(cid:3))

@2

@w2 f (x; Y; wt) = (cid:0)E(yjx;w(cid:3))
= (cid:0)E(yjx;w(cid:3))

= (cid:0)E(yjx;w(cid:3))
= (cid:0)E(yjx;w(cid:3))

= (cid:0)E(yjx;w(cid:3))

= (cid:0)E(yjx;w(cid:3))

(

@2
@w2

K∑
(

k=1

K∏

)

)

(I[y=k] log (cid:27)k(x; wt))

(cid:27)k(x; wt)I[y=k]

k=1

@2
@!2

@2
@w2 log
@w2 log P(yjx; wt)
@2
P(yjx; wt)
P(yjx; wt)
P(yjx; wt)
P(yjx; wt)
P(yjx; wt)
P(yjx; wt)

@2
@!2

@2
@!2

+ E(yjx;w(cid:3))

+ E(yjx;w(cid:3))

+ E(yjx;w(cid:3))

(
(
(

)

@
@!

2

P(yjx; wt)
)
P(yjx; wt)
)
log P(yjx; wt)

2

2

:

@
@!

@
@!

f (x; Y; wt)

:

(71)

@2
@!2

Since E(yjx;wt)

2001), we have(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E(yjx;w(cid:3))

@2
@!2

P(yjx; wt)
P(yjx; wt)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation
P(yjx;wt)
P(yjx;wt) = 0 by the two equivalent methods to calculating ﬁsher information matrix (Friedman et al.,

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

@2
@!2

(cid:0) E(yjx;wt)

P(yjx; wt)
P(yjx; wt)
P(yjx; wt)
P(yjx; wt)
P(Y = kjX = x; wt) (cid:2) P(Y = kjx; w(cid:3)

@2
@!2

) (cid:0) P(Y = kjx; wt)

P(Y = kjx; wt)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) =
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E(yjx;w(cid:3))
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) K∑
(cid:20) H (cid:1) K∑

@2
@!2

k=1

=

jP(Y = kjx; w(cid:3)

) (cid:0) P(Y = kjx; wt)j

k=1

(cid:20) HKL∥wt (cid:0) wloc∥ + HK max
k=1;(cid:1)(cid:1)(cid:1) ;K
(cid:20) HKL∥wt (cid:0) wloc∥ + ϵnc:

jP(Y = kjx; wloc) (cid:0) P(Y = kjx; w(cid:3)

)j

(72)

(73)

(74)

(75)

(76)

(77)

(78)

(79)

(80)

(81)

(82)

(83)

(84)

(85)

(86)

For strongly convex objective functions, ϵnc = 0 and wloc = w
DC-ASGD without using the information of ϵt like before. By the smoothness condition, we have

(cid:3). The only thing we need is to prove the convergence of

EF (wt+(cid:28) +1) (cid:0) F (w

(cid:3)

)

(cid:20) F (wt+(cid:28) ) (cid:0) F (w
= F (wt+(cid:28) ) (cid:0) F (w

(cid:3)

(cid:3)

) (cid:0) (cid:17)t+(cid:28)⟨∇F (wt+(cid:28) ); Egdc(wt)⟩ +
) (cid:0) (cid:17)t+(cid:28)⟨∇F (wt+(cid:28) );∇F (wt+(cid:28) )⟩
L2(cid:17)2
t+(cid:28) V 2
2

L2(cid:17)2
t+(cid:28) V 2
2

+(cid:17)t+(cid:28)⟨∇F (wt+(cid:28) );∇F (wt+(cid:28) ) (cid:0) Egdc(wt)⟩ +

L2

(cid:20) (1 (cid:0) 2(cid:17)t+(cid:28) (cid:22)2
(cid:20) (1 (cid:0) 2(cid:17)t+(cid:28) (cid:22)2
(cid:20) (1 (cid:0) 2(cid:17)t+(cid:28) (cid:22)2

L2

L2

)(F (wt+(cid:28) ) (cid:0) F (w
)(F (wt+(cid:28) ) (cid:0) F (w
)(F (wt+(cid:28) ) (cid:0) F (w

(cid:3)

(cid:3)

(cid:3)

)) + (cid:17)t+(cid:28)∥∇F (wt+(cid:28) )∥∥∇F (wt+(cid:28) ) (cid:0) Egdc(wt)∥ +
)) + (cid:17)t+(cid:28) V (cid:1) (L2 + (cid:21)L2
L2(cid:17)2
t+(cid:28) V 2
2
)) + (cid:17)t+(cid:28) V (cid:1) (L2 + (cid:21)L2
(cid:17)t+(cid:28)(cid:0)jgdc(wt)∥ +

1)∥wt+(cid:28) (cid:0) wt∥ +

1)∥ (cid:28)∑

j=1

L2(cid:17)2
t+(cid:28) V 2
2

L2(cid:17)2
t+(cid:28) V 2
2

(cid:3)

EF (wt+(cid:28) +1) (cid:0) F (w

Taking expectation to the above inequality, we can get
) (cid:20) (1 (cid:0) 2(cid:17)t+(cid:28) (cid:22)2
(cid:20) (1 (cid:0) 2(cid:17)t+(cid:28) (cid:22)2
)

(cid:22)2t, we have

Let (cid:17)t = L2

(

L2

L2

EF (wt+1) (cid:0) F (w

(cid:3)

) (cid:20)

1 (cid:0) 2
t

We can get

EF (wt) (cid:0) F (w

(cid:3)

by induction. Then we can get

∥wt (cid:0) w

)(EF (wt+(cid:28) ) (cid:0) F (w
)(EF (wt+(cid:28) ) (cid:0) F (w

(cid:3)

(cid:3)

)) +

)) +

(cid:17)2
t+(cid:28) (L2 + (cid:21)L2

1)V 2(cid:28)

+

L2(cid:17)2
t+(cid:28) V 2
2

2
(cid:17)2
t+(cid:28) V 2L2

2

L2 + (cid:21)L2
1

(cid:28) ):

(1 +

L2

)

(cid:28)

:

(
)
)

(cid:3)

)) +

V 2L2
2
2(cid:22)4t2

1 +

L2 + (cid:21)L2
1

L2

(EF (wt) (cid:0) F (w
(
(

) (cid:20) 2L2
2V 2
t(cid:22)4

(cid:3)∥2 (cid:20) 4L2
2V 2
t(cid:22)5

1 +

(cid:28)

:

L2 + (cid:21)L2
1

L2

L2 + (cid:21)L2
1

L2

1 +

(cid:28)

:

By putting Ineq.86 into Ineq.73, we can get the result in the theorem.
For nonconvex case, if wt 2 B(wloc; r), we have E(wt(cid:0)wloc) (cid:20) 1
for nonconvex loss function f (x; y; wt), DC-ASGD has ergodic convergence rate. mint=1;(cid:1)(cid:1)(cid:1) ;T E∥ @
O(1=

p
T ), where the expectation is taking with respect to the stochastic sampling.

E∇F (wt) under the assumptions. Next we will prove that,
F (x; y; wt)∥2 =

@wt

(cid:22)

Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

Figure1. Error rates of the global model with Different (cid:21)0 w.r.t. number of effective passes on CIFAR-10

Compared with the proof of ASGD (Lian et al., 2015), DC-ASGD with Hessian approximation has

T1 = ∥∇F (wt+(cid:28) ) (cid:0) Egdc(wt)∥2

= ∥∇F (wt+(cid:28) ) (cid:0) ∇F (wt) (cid:0) (cid:21)Eg(wt) ⊙ g(wt) (cid:1) (wt+(cid:28) (cid:0) wt)∥2
(cid:20) 2∥∇F (wt+(cid:28) ) (cid:0) ∇F (wt)∥2 + 2∥(cid:21)Eg(wt) ⊙ g(wt) (cid:1) (wt+(cid:28) (cid:0) wt)∥2
(cid:20) 2(L2

1)∥wt+(cid:28) (cid:0) wt∥2;

2 + (cid:21)2L4

since L1 is the upper bound of ∇f (w) and L2 is the smooth coefﬁcient of f (w). Suppose that (cid:17) =
upper bounded as Theorem 5.1,

min

t=1;(cid:1)(cid:1)(cid:1) ;T

E∥∇F (wt)∥2 (cid:20) 1
T

E∥∇F (wt)∥2 (cid:20) O(

1

T 1=2

):

T∑

t=1

√

(87)
(88)
(89)
(90)

2D0

bT V 2L2

and (cid:28) is

(91)

Referring to a recent work of Lee et:al (Lee et al., 2016), GD with a random initialization and sufﬁciently small constant
step size converges to a local minimizer almost surely under the assumptions in Theorem 1.2. Thus, the assumption that
F (w) is (cid:22)-strongly convex in the r-neighborhood of arbitrary local minimum wloc is easily to be satistied with probability
one. By the L1-Lipschitz assumption, we have P (Y = kjx; wt)(cid:0) P (Y = kjx; wloc) (cid:20) L1∥wt(cid:0) wloc∥. By the L2-smooth
assumption, we have L2∥wt(cid:0) wloc∥2 (cid:21) ⟨∇F (wt); wt(cid:0) wloc⟩. Thus for wt 2 B(wloc; r), we have ∥∇F (wt)∥ (cid:20) L2∥wt(cid:0)
wloc∥ (cid:20) L2r. By the continuously twice differential assumption, we can assume that ∥∇F (wt)∥ (cid:20) L2∥wt (cid:0) wloc∥ (cid:20) L2r
for wt 2 B(wloc; r) and ∥∇F (wt)∥ (cid:20) L2∥wt (cid:0) wloc∥ > L2r for wt =2 B(wloc; r) without loss of generality 2. Therefore
mint=1;(cid:1)(cid:1)(cid:1) ;T E∥∇F (wt)∥2 (cid:20) L2

2r2 is a sufﬁcient condition for E∥wT (cid:0) wloc∥ (cid:20) r.

We have T0 (cid:21) O(

)

.

1
r4

E∥∇F (wt)∥2 (cid:20) O(

min

t=1;(cid:1)(cid:1)(cid:1) ;T0

) (cid:20) r2:

1

T 1=2
0

(92)

Thus we have ﬁnished the proof for nonconvex case.

G. Experimental Results on the Inﬂuence of (cid:21)
In this section, we show how the parameter (cid:21) affect our DC-ASGD algorithm. We compare the performance of respectively
3. The results are given in Figure 1. This
sequential SGD, ASGD and DC-ASGD-a with different value of initial (cid:21)0
experiment reﬂects to the discussion in Section 5, too large value of this parameter ((cid:21)0 > 2 in this setting) will introduce
large variance and lead to a wrong gradient direction, meanwhile too small will make the compensation inﬂuence nearly
disappear. As (cid:21) decreasing, DC-ASGD will gradually degrade to ASGD. A proper (cid:21) will lead to signiﬁcant better accuracy.

2We can choose r small enough to make it satisﬁed.
3We also compare different (cid:21)0 for DC-ASGD-c and the results are very similar to DC-ASGD-a.

020406080100120140160Epochs0.000.050.100.150.20Training errorM = 8SGDAsync SGDDC-ASGD:¸0=0:5DC-ASGD:¸0=1DC-ASGD:¸0=28090100110120130140150160Epochs0.0850.0900.0950.1000.1050.1100.1150.1200.1250.130Test errorM = 8Supplementary: Asynchronous Stochastic Gradient Descent with Delay Compensation

H. Large Mini-batch Synchronous SGD with Delay-Compensated Gradient
In this section, we discuss how delay-compensated gradient can be used in synchronous SGD. The effective mini-batch
size in SSGD is usually enlarged M times comparing with sequential SGD. A learning rate scaling trick is commonly used
to overcome the inﬂuence of large mini-batch size in SSGD (Goyal et al., 2017): when the mini-batch size is multiplied by
M, multiply the learning rate by M. For sequential mini-batch SGD with learning rate (cid:17) we have:

wt+M = wt (cid:0) (cid:17)

g(wt+j; zt+j);

(93)

M(cid:0)1∑

j=0

^wt+1 = wt (cid:0) ^(cid:17)

1
M

M(cid:0)1∑

j=0

where zt+j is the t + j-th minibatch.
On the other hand, taking one step with M times large mini-batch size and learning rate ^(cid:17) = M (cid:17) in synchronous SGD
yields:

g(wt; zj

t );

(94)

t is the t-th minibatch on local machine j.

t . The assumption g(wt+j; zt+j) (cid:25) g(wt; zj

where zj
Assume that zt+j = zj
However, it often may not hold.
t+1 = wt (cid:0) ^(cid:17) 1
If we denote ~wj

M

∑

t ) was made in synchronous SGD(Goyal et al., 2017).

i<j g(wt; zi

t), we can unfold the summation in Eq.94 to

~wj+1
t+1 = ~wj

t+1

(cid:0) ^(cid:17)

1
M

g(wt; zj

t ); j < M;

(95)

then we have ^wt+1 = ~wM
delay-compensated gradient to update Eq.95 with:

t+1. We propose to use Eq.(5) in the main paper to compensate this assumption and apply

g(wt+j; zt+j) (cid:25) ~g( ~wj
t+1 = ~wj
~wj+1

(cid:0) ^(cid:17)

t+1

1
M

t+1; zj
~g( ~wj

t ) := g(wt; zj
t+1; zj

t ); j < M:

t ) + (cid:21)g(wt; zj

t ) ⊙ g(wt; zj

t ) ⊙ ( ~wj

t+1

;

(96)

t+1 in Eq.97. For j > 1, we need to design an order to make ~wj

(97)
(cid:25) wt+j.
(cid:0) wt∥2 can be used since the smaller distance with wt will

t+1

Please note that we redeﬁne the previous ~wj+1
Choosing ~wj
induce more accurate approximation by using Taylor expansion.

t+1 according to the increasing order of ∥ ~wj

t+1

References
Friedman, Jerome, Hastie, Trevor, and Tibshirani, Robert. The elements of statistical learning, volume 1. Springer series in statistics

Springer, Berlin, 2001.

Goyal, Priya, Dollar, Piotr, Girshick, Ross, Noordhuis, Pieter, Wesolowski, Lukasz, Kyrola, Aapo, Tulloch, Andrew, Jia, Yangqing, and

He, Kaiming. Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677, 2017.

LeCun, Yann. Modèles connexionnistes de lapprentissage. PhD thesis, These de Doctorat, Universite Paris 6, 1987.

Lee, Jason D, Simchowitz, Max, Jordan, Michael I, and Recht, Benjamin. Gradient descent converges to minimizers. University of

California, Berkeley, 1050:16, 2016.

Lian, Xiangru, Huang, Yijun, Li, Yuncheng, and Liu, Ji. Asynchronous parallel stochastic gradient for nonconvex optimization. In

Advances in Neural Information Processing Systems, pp. 2737–2745, 2015.

)
(cid:0) wt)

