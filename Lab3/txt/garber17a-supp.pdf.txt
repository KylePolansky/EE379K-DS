Communication-efﬁcient Algorithms for

Distributed Stochastic Principal Component Analysis

Dan Garber 1 Ohad Shamir 2 Nathan Srebro 3

Abstract

We study the fundamental problem of Principal
Component Analysis in a statistical distributed
setting in which each machine out of m stores
a sample of n points sampled i.i.d. from a sin-
gle unknown distribution. We study algorithms
for estimating the leading principal component
of the population covariance matrix that are both
communication-efﬁcient and achieve estimation
error of the order of the centralized ERM so-
lution that uses all mn samples. On the nega-
tive side, we show that in contrast to results ob-
tained for distributed estimation under convex-
ity assumptions, for the PCA objective, simply
averaging the local ERM solutions cannot guar-
antee error that is consistent with the centralized
ERM. We show that this unfortunate phenomena
can be remedied by performing a simple correc-
tion step which correlates between the individual
solutions, and provides an estimator that is con-
sistent with the centralized ERM for sufﬁciently-
large n. We also introduce an iterative distributed
algorithm that is applicable in any regime of n,
which is based on distributed matrix-vector prod-
ucts. The algorithm gives signiﬁcant acceleration
in terms of communication rounds over previous
distributed algorithms, in a wide regime of pa-
rameters.

1. Introduction
Principal Component Analysis (PCA) (Pearson, 1901;
Hotelling, 1933; Jolliffe, 2002) is one of the most cele-
brated and popular techniques in data analysis and ma-

1Technion -
2Weizmann

Israel

Institute of Technology, Haifa,

Is-
Israel
Illinois, USA. Correspon-
Dan Garber <dangar@technion.ac.il>, Ohad
Srebro

rael
3Toyota Technological
dence to:
Shamir <ohad.Shamir@weizmann.ac.il>, Nathan
<nati@ttic.edu>.

of Science, Rehovot,

Institute

Institute,

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

(cid:80)N
i=1 xix(cid:62)

chine learning. For data that consists of N vectors in
Rd, x1, ..., xN , with normalized covariance matrix ˆX =
i , The PCA method ﬁnds the k-dimensional
1
N
subspace (which corresponds to the span of the top k prin-
cipal components) such that the projection of the data onto
the subspace has largest variance, i.e., it is the solution to
the optimization problem:

max

W∈Rd×k ,WT W=I

(cid:107) ˆXW(cid:107)2
F .

(1)

PCA is often considered in a statistical setting in which the
assumption is that the input vectors are not arbitrary but
sampled i.i.d. from some ﬁxed but unknown distribution
with certain general characteristics D. Then, it is often of
interest to use the observed sample to estimate the top k
principal components of the population covariance matrix,
rather then that of the sample, which leads to the modiﬁed
optimization problem:

(cid:107)Ex∼D(cid:2)xx(cid:62)(cid:3) W(cid:107)2

F .

(2)

max

W∈Rd×k ,WT W=I

Of course the empirical estimation problem (1) and the
population estimation problem (2) are well connected, and
it is well-known that under mild assumptions on the dis-
tribution D and given a sufﬁciently large sample, we can
guarantee small estimation error in (2) by solving optimiza-
tion problem (1).
In this work we consider the problem of estimating the ﬁrst
principal component (i.e., k = 1) in a statistical and dis-
tributed setting. We assume the availability of m machines,
each of which stores a sample of n vectors sampled i.i.d
from a ﬁxed distribution D over Rd, and we are interested
in algorithms that can be applied efﬁciently to solve Prob-
lem (2) for k = 1, with estimation error that approaches
that of a centralized algorithm, which has access to all mn
samples and does not pay for communication between ma-
chines. Indeed, when considering the efﬁciency of algo-
rithms, we will mainly focus on the amount of communi-
cation between machines they require, since this is often
the most expensive resource in distributed computing. We
note that the i.i.d. assumption is standard in many appli-
cations of PCA, and can be leveraged to get more efﬁcient
algorithms than when the data partition is arbitrary. Also,
we will make a standard assumption that the population co-
variance matrix has a non-zero additive gap between the

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

ﬁrst and second eigenvalues, which makes the problem of
estimating the leading principal component meaningful.
A main challenge that often arises in many computational
settings of principal components is that it leads to in-
herently non-convex optimization problems. While many
times these problems turn out to admit efﬁcient algorithms,
the rich toolbox of optimization and statistical estimation
procedures developed for convex problems often cannot be
directly applied to problems such as (1) and (2). Instead,
one often needs to consider a specialized and more involved
analysis, to get analogous convergence results for the PCA
problem. This for instance was the case in a recent wave
of results that applied concepts such as stochastic gradi-
ent updates (Balsubramani et al., 2013; Shamir, 2016a; Jain
et al., 2016b; Allen Zhu & Li, 2016b) and variance reduc-
tion (Shamir, 2015; 2016c; Garber & Hazan, 2015; Garber
et al., 2016; Allen Zhu & Li, 2016a) to the PCA problem.
This is also the case in our distributed setting. For instance,
(Zhang et al., 2013) proposed communication-efﬁcient al-
gorithms for a distributed statistical estimation settings,
similar to ours, but under convexity assumptions. The au-
thors show that under their assumptions, in a wide regime
of parameters (namely when the per-machine sample size
n is large enough), then a simple averaging of the empirical
risk minimizers (ERM), computed locally on each machine,
leads to estimation error of the population parameters of
the order the centralized ERM solution. While averaging
makes perfect sense in a convex setting, it is clear that it
can completely fail in a non-convex setting.
Indeed, we
show that already for the PCA problem with k = 1, simply
averaging the local ERM solutions (and normalizing to ob-
tain a unit vector as required), cannot improve signiﬁcantly
over the estimation error of any single machine. We then
show that a simple ﬁx to the above scheme, namely cor-
relating the directions of individual ERM solutions, reme-
dies this phenomena and results in estimation error similar
to that of the centralized ERM solution. Much like the re-
sults of (Zhang et al., 2013), this result only holds in the
regime when the per-machine sample size n is sufﬁciently
large. As discussed, due to the inherent non-convexity of
the PCA objective, this approach requires a novel analysis
tailored to the PCA problem. In this context, we view this
work as an initiation of a research effort to understand how
to efﬁciently aggregate statistical estimators in a distributed
non-convex setting.
A second line of results for distributed estimation under
convexity assumptions consider iterative algorithms that
perform multiple communication rounds and are based on
distributed gradient computations (some examples include
(Shamir et al., 2014; Zhang & Lin, 2015; Lee et al., 2015;
Shamir, 2016b; Jaggi et al., 2014; Reddi et al., 2016)). The
beneﬁt of these methods is that (a) they provide meaningful
estimation error guarantees in a much wider regime of pa-

rameters than the “one-shot” aggregation methods (namely
in terms of the number of samples per machine), and (b),
due to their iterative nature, they allow to approximate the
centralized ERM solution arbitrary well. Unfortunately,
these methods, all of which rely heavily on convexity as-
sumption, cannot be directly applied to the PCA problem.
Towards designing efﬁcient distributed iterative methods
for our PCA setting, we consider the application of the
recently proposed method of Shift-and-Invert power itera-
tions (S&I) for PCA (Garber & Hazan, 2015; Garber et al.,
2016). The S&I method reduces the problem of computing
the leading eigenvector of a real positive semideﬁnite ma-
trix to that of approximately solving a small number (i.e.
poly-logarithmic in the problem parameters) of systems of
linear equations. These in turn, could be efﬁciently solved
by arbitrary distributed convex solvers. We show that cou-
pling the S&I method with the stochastic pre-conditioning
technique for linear systems proposed in (Zhang & Lin,
2015) and well known fast gradient methods such as the
conjugate gradient method, gives state-of-the-art guaran-
tees in terms of communication costs, and provides a sig-
niﬁcant improvement over distributed variants of classical
fast eigenvector algorithms such as power iterations and the
faster Lanczos algorithm. Much like its convex counter-
parts, which only rely on distributed gradient computations
and simple vector aggregations, our iterative method only
relies on distributed matrix-vector products, i.e., it requires
each machine to only send products of its local empirical
covariance matrix with some input vector.
Beyond the results described so far, (Liang et al., 2014;
Boutsidis et al., 2016) studied distributed algorithms for
PCA in a deterministic setting in which the partition of
the data across machines is arbitrary and communication
is measured in terms of number of transmitted bits. The
approximation guarantees provided in these works are in
terms of the projection of the data onto the leading princi-
pal components (instead of alignment between the estimate
and the optimal solution, studied in this paper). Applying
these results to our setting will give a number of communi-
cation rounds that scales like poly(−1δ−1), where  is the
desired error and δ is the population eigengap. In our set-
ting,  will scale with the inverse of the size of the sample,
i.e.,  ≈ (mn)−1, which for these algorithms will result in
amount of communication that is polynomial in the size of
the data. In contrast, we will be interested in algorithms
whose communication costs does not scale with n at all. In
this context we note that, by focusing on algorithms that
either perform simple aggregation of local ERM solutions,
or perform only distributed matrix-vector products with the
empirical covariance matrix, we can circumvent the need to
measure communication explicitly in terms of the number
of bits transmitted, which often burdens the analysis of nat-
ural algorithms, such as those proposed here.

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

2. Preliminaries
2.1. Notation and problem setting
We write vectors in Rd in boldface lower-case letters (e.g.,
v), matrices in boldface upper-case letters (e.g., X), and
scalars are written as lightface letters (e.g., c). We let (cid:107) ·
(cid:107) denote the standard Euclidean norm for vectors and the
spectral norm for matrices.
We consider the following statistical distributed setting.
Let D be a distribution over vectors in Rd with squared
(cid:96)2 norm at most b, for some b > 0. We consider a setting
in which m machines, numbered 1...m, are each given a
dataset of n samples drawn i.i.d. from D. We let v1 denote
a leading eigenvector of the population covariance matrix
X = Ex∼D[xx(cid:62)]. Our goal is to efﬁciently (mainly in
terms of communication) ﬁnd an estimate w for v1, i.e., a
unit vector that maximizes the product (v(cid:62)
1 w)2 with high
probability. Towards this end, we assume that the popula-
tion covariance matrix X has a non-zero eigengap δ, i.e.,
δ := λ1(X) − λ2(X) > 0, where λi(·) denotes the ith
largest eigenvalue of a symmetric real matrix. Note that
δ > 0 is necessary for v1 to be uniquely deﬁned (up to
sign).
In addition, we let ˆXi denote the empirical covariance ma-
trix of the sample stored on machine i for every i ∈ [m],
i.e., ˆXi = 1
n are the
n
samples stored on machine i. We let ˆX denote the em-
pirical covariance matrix of the union of points across all
machines i.e., ˆX = 1
m
Our model of communication assumes that the m machines
work in rounds during which a central machine (w.l.o.g.
machine 1) can send a single vector in Rd to all other ma-
chines, or every machine can send either the leading eigen-
vector of its local empirical covariance matrix, or the prod-
uct of a single input vector with its local covariance, to ma-
chine 1. We will measure communication complexity in
terms of number of such rounds required to achieve a cer-
tain estimation error.

j x(i)(cid:62)
(cid:80)m

, where x(i)

(cid:80)n

1 ...x(i)

j=1 x(i)

ˆXi.

i=1

j

2.1.1. THE CENTRALIZED SOLUTION

Our primary benchmark for measuring performance will be
the centralized empirical risk minimizer which is the lead-
ing eigenvector of the aggregated empirical covariance ma-
trix ˆX.
The following standard result bounds the error of the cen-
tralized ERM.

Lemma 1 (Risk of centralized ERM). Fix p ∈ (0, 1). Sup-
pose that δ > 0 and let ˆv1 denote the leading eigenvector
of ˆX, i.e., ˆv1 ∈ arg maxv:(cid:107)v(cid:107)=1 v(cid:62) ˆXv. Then it holds w.p.
at least 1 − p that

1 − (v(cid:62)

1 ˆv1)2 ≤ ERM(p) :=

32b2 ln(d/p)

mnδ2

.

(3)

Lemma 1 is a direct consequence of the following stan-
dard concentration argument for random matrices, and the
Davis-Kahan sin(θ) theorem (whose proof is given in the
appendix for completeness):
Theorem 1 (Matrix Hoeffding, see (Tropp, 2012)). Let D
(cid:80)n
be a distribution over vectors with squared (cid:96)2 norm at most
(cid:16)− 2n
(cid:17) ≤ d · exp
(cid:17)
i=1 xix(cid:62)
b, and let X = Ex∼D[xx(cid:62)]. Let ˆX = 1
i ,
where x1, ..., xn are sampled i.i.d. from D. Then, it holds
that ∀ > 0 : Pr
Theorem 2 (Davis-Kahan sin(θ) theorem). Let X, Y be
symmetric real d×d matrices with leading eigenvectors vX
(cid:1)2 ≤ 2
and vY respetively. Also, suppose that δ(X) := λ1(X) −
(cid:107)X−Y(cid:107)2
.
δ(X)2

(cid:16)(cid:107) ˆX − X(cid:107) ≥ 
λ2(X) > 0. Then it holds that 1−(cid:0)v(cid:62)

XvY

16b2

n

.

2.2. Informal statement of main results and previous

algorithms

We now informally describe our main results, followed by a
detailed description of previous approaches that are directly
applicable to our setting. The algorithmic results (both new
and old) are summarized in Table 1.

2.2.1. MAIN RESULTS
Failure of simple averaging of local ERM solutions We
show that a natural approach of simply averaging the in-
dividual leading eigenvectors of the empirical covariance
matrices ˆXi (and normalizing the obtain a unit vector)
cannot signiﬁcantly improve (beyond logarithmic factors)
over the performance of any of the individual eigenvectors.
More concretely, if we let ˆv(i)
1 denote the leading eigen-
vector of ˆXi for any i ∈ [m], and we denote their average
1 , then there exists a distribution D
by ¯v1 = 1
m
over vectors with magnitude O(1) and covariance eigen-
gap δ = 1, such that
∀m, n : ED

(cid:18) ¯v(cid:62)

(cid:19)2(cid:35)

(cid:80)m

(cid:18) 1

i=1 ˆv(i)

(cid:19)

1 −

(cid:34)

= Ω

,

1 v1
(cid:107)¯v1(cid:107)

n

See Theorem 3 in Section 3 for the complete and formal
argument.

A successful single communication round algorithm via
correlation of individual ERM solutions We show that
if prior to averaging the local ERM solutions, as suggested
above, we correlate their directions by aligning them ac-
cording to any single machine (say machine number 1),
1 , then this
i.e., we let ¯v1 = 1
guarantees that for any p ∈ (0, 1), w.p. at least 1 − p,
m

1 )ˆv(i)
ˆv(1)

1

(cid:80)m
i=1 sign(ˆv(i)(cid:62)
(cid:16) dm
(cid:17)
 b2 ln

p
δ2mn

+

(cid:19)2

(cid:18) ¯v(cid:62)

1 v1
(cid:107)¯v1(cid:107)

1 −

= O

b4 ln2(cid:16) dm

(cid:17)

p

δ4n2

 .

(4)

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

Method
Centralized ERM
Distributed Power Method
Distributed Lanczos
“Hot-potato” SGD
Average of ERMs with sign-ﬁxing (Theorem 4)
Distributed Shift&Invert + precond. linear systems (Theorem 6)

1 − (w(cid:62)v1)2 w.p. 3/4

# communcation rounds

ERM = Θ( b2 ln d
δ2mn )
ERM · (1 + o(1))
ERM · (1 + o(1))

O(ERM)

O(ERM) + O

ERM · (1 + o(1))

(cid:17)

-

˜O((cid:112)λ1/δ)

˜O(λ1/δ)

m

1

˜O(min{(b/δ)1/2n−1/4, m1/4})

(cid:16) b4 ln2 d

δ4n2

Table 1. Comparison of estimation error and number of communication rounds. For simplicity we ﬁx the failure probability to p = 1/4
and assume mn is in the regime in which Lemma 1 is meaningful, i.e, mn = Ω(b2δ−2 ln d). The ˜O(·) suppresses logarithmic factors
in b, d, 1/p, 1/ERM. For the result of Theorem 4 we assume the regime m = O(d). The sub-constant o(1) factors could be made, in
principle, arbitrary small in all relevant results by trading approximation with communication.

See Theorem 4 in Section 3 for the complete and formal
result.
In particular, in the likely scenario when m = O(d/p)
=

we have that w.p. at least 1 − p, 1 −(cid:0)¯v(cid:62)
1 v1/(cid:107)¯v1(cid:107)(cid:1)2
ERM(p)) · O(cid:0)1 + m2 · ERM(p)(cid:1) , where ERM(p)) is de-
(up to poly-log factors) when n = Ω(cid:0)δ−2b2m ln(dm/p)(cid:1).

ﬁned in Eq. (3). Another related interpretation of the re-
sults is that the bound in Eq. (4) is comparable with ERM

We also show a matching lower bound that the bound in
Eq. (4) is tight (up to poly-log factors) for this aggregation
method.

A multi communication round algorithm We present a
distributed algorithm based on the Shift-and-Invert frame-
work for leading eigenvector computation (Garber &
Hazan, 2015; Garber et al., 2016) which is applied to ex-
plicitly solving the centralized ERM problem. We show
that for any p ∈ (0, 1), when mn = Ω(b2 ln(d/p)/δ2) (i.e.,
when Lemma 3 is meaningful), the algorithm produces a
solution w such that w.p. at least 1 − p,

1 − (v(cid:62)

1 w)2 ≤ ERM(p)) · (1 + o(1)) ,

(5)

√
where ERM(p)) is deﬁned in Eq. (3). The algorithm per-
bδ−1/2n−1/4) distributed matrix-vector
forms overall ˜O(
products with the centralized empirical covariance matrix
ˆX 1. The ˜O(·) notation hides poly-logarithmic factors in
1/p, 1/δ, d, 1/ERM(p). See Theorem 6 in Section 4 for the
complete and formal result.
We note that in particular, under our assumption that mn =
˜Ω(b2/δ2), it holds that the number of distributed matrix-
vector products is upper bounded by ˜O(m1/4). Moreover,
in the regime n = Ω(b2δ−2), we can see that the number
of distributed matrix-vector products depends only poly-
logarithmically on the problem parameters.
In general, the sub-constant o(1) factor in (5) could be
made arbitrarily small by trading the approximation error

1i.e., on each round, each machine i sends the product of an

input vector in Rd with its local covariance matrix ˆXi.

with the number of distributed matrix-vector products.

2.2.2. PREVIOUS ALGORITHMS
Distributed versions of classical iterative algorithms:
Classical fast iterative algorithms for computing the lead-
ing eigenvector of a positive semideﬁnite matrix, such as
the well-known Power Method and the Lanczos Algo-
rithm, require iterative multiplications of the input ma-
trix ( ˆX in our case) with the current estimate.
It is thus
straightforward to implement these algorithms in our dis-
tributed setting, by multiplying the same vector with the
covariance matrices at each machine, and averaging the
result. Thus, by well-known convergence guarantees of
these two methods, we will have that for a ﬁxed  > 0,
these methods produce a unit vector w such that, for any
p ∈ (0, 1), 1 − (w(cid:62) ˆv1)2 ≤  w.p. at least 1 − p, af-
ˆδ−1 ln(d/p)) rounds for the Power Method and
ter O(ˆλ1
ˆδ−1 ln(d/p)) for the Lanczos Algorithm, where
O(
ˆλ1, ˆδ denote the leading eigenvalue and eigengap of ˆX,
respectively. Moreover, in the regime of mn in which
Lemma 1 is meaningful, we can replace ˆλ1, ˆδ with λ1, δ
in the above bounds, and the result will still hold with high
probability.
Simple calculations show that in the regime of mn in which
Lemma 1 is meaningful, it holds that our Shift-and-Invert-
based algorithm outperforms distributed Lanczos (in terms
of worst-case guarantees) whenever n = ˜Ω(b2/λ2

(cid:112)ˆλ1

1).

“Hot potato” SGD: Another straightforward approach is
to apply a sequential algorithm for direct risk minimiza-
tion that can process the data-points one by one, such as
stochastic gradient descent (SGD), by passing its state from
one machine to the next, after completing a full pass over
the machine’s data. Clearly, this process of making a full
pass over the data of a certain machine before sending the
ﬁnal estimate to the next one, requires overall m commu-
nication rounds in order to make a full pass over all mn
points. SGD for PCA was studied in several results in
recent years (Balsubramani et al., 2013; Shamir, 2016a;c;

Jain et al., 2016a; Allen Zhu & Li, 2016b). For instance
applying the result of (Jain et al., 2016a) in this way will
result in a ﬁnal estimate w satisfying

1 − (w(cid:62)v1)2 = O

w.p. at least 3/4.

(6)

(cid:18) b2 ln d

(cid:19)

δ2mn

We note that in the regime in which the bound in (6) is
meaningful it holds that the number of communication
rounds of our Shift-and-Invert-based algorithm is upper-
bounded by ˜O(m1/4) which for sufﬁciently large m domi-
nates the communication complexity of SGD.

3. Single Communication Round Algorithms

via ERM on Each Machine

In this section we consider distributed algorithms that re-
quire only a single round of communication. Naturally for
this regime, all algorithms will be based on aggregating the
ERM solutions of the individual machines, i.e., each ma-
chine i only sends the leading eigenvector of its empiri-
cal covariance matrix ˆXi to a centralized machine (without
loss of generality, machine 1) which it turn combines them
to a single unit vector in some manner.

3.1. Simple averaging of eigenvectors fail

Perhaps the simplest method to aggregate the individual
eigenvectors of each machine is to average them, and then
normalize to obtain a unit vector. For instance, in the
distributed statistical setting considered in (Zhang et al.,
2013), in which the objective is strongly convex, it was
shown that simply averaging the individual ERM solutions
leads, in a meaningful regime of parameters, to estimation
error of the order of the centralized ERM solution. How-
ever, here we show that for PCA, in which the objective is
certainly not convex, this approach fails practically in any
regime, in the sense that the error of the returned aggre-
gated solution can be no better than that returned by any
single machine.
Theorem 3. There exists a distribution over vectors in R2
with (cid:96)2 norm bounded by a universal constant for which the
eigengap in the covariance matrix is 1 (i.e., δ = 1), such
that if each machine i returns an estimate ˆv(i)
1 which is
an unbiased leading eigenvector of ˆXi (i.e., both outcomes
−ˆv(i)
1 are equally likely), then the aggregated vector
i=1 ˆv(i)
¯v1 = 1
m

(cid:80)m

1 , +ˆv(i)

satisﬁes

1

(cid:34)

(cid:29)2(cid:35)

(cid:28) ¯v1(cid:107)¯v1(cid:107) , v1

∀m, n : E

1 −

= Ω(1/n).

The proof is given in the appendix.

v1

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

3.2. Averaging with Sign Fixing

As evident from the statement of Theorem 3, an important
assumption is that each machine produces an unbiased es-
timate, in the sense that the sign of the outcome is uniform
and independent of the other machines. This hints that cor-
relating the signs of the different estimates can circumvent
the lower bound result in Theorem 3. It turns out that this
is indeed the case, as captured by the following theorem:
Theorem 4. Let ˜wi be the leading eigenvector of ˆXi for
any i ∈ [m], and consider the unit vector
i ˜w1) ˜wi
i ˜w1) ˜wi(cid:107) .
(cid:17)

Then, for any p ∈ (0, 1), it holds w.p. at least 1 − p that
1 − (v(cid:62)

(cid:80)m
(cid:107)(cid:80)m
i=1 sign( ˜w(cid:62)
i=1 sign( ˜w(cid:62)
 b2 log

b4 log2(cid:16) dm

(cid:16) dm

 .

1 w)2 = O

(cid:17)

w =

(7)

+

p

p
δ2mn

δ4n2

For ease of presentation, throughout the rest of this section
we denote the correlated vector ˆwi = sign( ˜w(cid:62)
i ˜w1) ˜wi for
any i ∈ [m].
The main step towards proving Theorem 4 is to consider
each ˆwi as an approximately unbiased perturbation of the
true leading eigenvector v1 and to upper bound the magni-
tude of this perturbation. This is carried out in the follow-
ing much more general and self-contained lemma, which
might be of independent interest. The proof is given in the
appendix.
Lemma 2. Let A be a positive semideﬁnite matrix with
some ﬁxed leading eigenvector v1, a leading eigenvalue λ1
and an eigengap δ := λ1(A)− λ2(A) > 0. Let ˆA be some
positive semideﬁnite matrix such that (cid:107) ˆA − A(cid:107) ≤ δ/4.
Then there is a unique leading eigenvector ˆv1 of ˆA such
that (cid:104)ˆv1, v(cid:105) ≥ 0, and

(cid:13)(cid:13)(cid:13)ˆv1 − v1 − (λ1I − A)†( ˆA − A)v1

(cid:13)(cid:13)(cid:13) ≤ c(cid:107) ˆA − A(cid:107)2

,
where † denotes the pseudo-inverse, and c is a positive nu-
merical constant.

δ2

Lemma 2 is central to the proof of the following Lemma,
of which the proof of Theorem 4 is an easy consequence.
We defer the proof of both the Lemma and that of Theorem
4 to the appendix.
Lemma 3. The following two conditions hold with proba-
bility at least 1−p−d exp(−δ2n/cb2), for some numerical
constants c, c(cid:48) > 0:

λ1( ˆXi) − λ2( ˆXi) > 0.

• The leading eigenvalue of every ˆXi is simple, i.e.,
• Fixing v1,
vectors ˆvi
maxi (cid:107)ˆvi

(cid:13)(cid:13)(cid:13) 1
(cid:80)m
(cid:113) b2 log(2dm/p)
(cid:17)

ˆX1, . . . , ˆXm,
4 , and

leading eigen-
such that
1 −
i=1 ˆvi
.

there
1, . . . , ˆvi
1 − v1(cid:107) ≤ 1
+

(cid:13)(cid:13)(cid:13) ≤ c(cid:48)(cid:16) b2 log(2dm/p)

exist unique

m of

δ2mn

δ2n

m

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

3.3. Lower Bound for Sign Fixing

We now show that the result of Theorem 4 is tight up to
poly-logarithmic factors and cannot be improved in gen-
eral:
Theorem 5. For any δ ∈ (0, 1) and d > 1, there exist a
distribution over vectors in Rd (of norm at most a universal
constant) with eigengap δ in the covariance matrix, such
that for any number of machines m and for per-machine
sample size any n sufﬁciently larger than 1/δ2, the aggre-
1 (even after sign ﬁxing with
gated vector ¯v1 = 1
m
the population eigenvector v1) satisﬁes

i=1 ˆv(i)

(cid:80)m
(cid:29)2(cid:35)
(cid:28) ¯v1(cid:107)¯v1(cid:107) , e1

(cid:34)

E

1 −

(cid:18) 1

= Ω

+

1

δ4n2

δ2mn

(cid:19)

The proof is given in the appendix.

4. A Multi-round Algorithm based on

Shift-and-Invert Iterations

In this section we move on to consider distributed algo-
rithms that perform multiple communication rounds. The
main motivation, beyond improving some poly-logarithmic
factors in the estimation error, is to obtain a result that does
not require the per-machine sample size n to grow with the
number of machines m, as in the result of Theorem 4.
Towards this end we consider the use of the Shift-and-
Invert meta-algorithm, originally described in (Garber &
Hazan, 2015; Garber et al., 2016), to explicitly solve the
centralized ERM objective, i.e., ﬁnd a unit vector that is an
approximate solution to maxv:(cid:107)v(cid:107)=1 v(cid:62) ˆXv.
Throughout this section we let ˆλ1, ˆδ denote the leading
eigenvalue and eigengap of ˆX, respectively. Also, we as-
sume without loss of generality that b = 1 (i.e., all data
points lie in the unit Euclidean ball).
Since our approach is to approximate the population risk
by approximating the empirical risk, we state the follow-
ing simple lemma for completeness (a proof is given in the
appendix).
Lemma 4 (Risk of approximated-ERM for PCA). Let w
be a unit vector such that (w(cid:62) ˆv1)2 ≥ 1− , for some ﬁxed
 > 0, where ˆv1 is the leading eigenvector of ˆX. Then it
holds that 1 − (w(cid:62)v1)2 ≤ 1 − (w(cid:62) ˆv1)2 +

√

2.

4.1. The Shift-and-Invert meta-algorithm

The Shift-and-Invert algorithm (Garber & Hazan, 2015;
Garber et al., 2016) efﬁciently reduces the problem of
computing the leading eigenvector of a positive semidef-
inite matrix ˆX to that of approximately-solving a poly-
logarithmic number of linear systems, i.e., ﬁnding approx-
imate minimizers of convex quadratic optimization prob-
lems of the form

{Fλ,w(z) :=

1
2

min
z∈Rd

z(cid:62)(λI − ˆX)z − z(cid:62)w},

(8)

where λ > λ1( ˆX) is a shifting parameter. The algorithm is
essentially based on applying power iterations to a shifted
and inverted matrix (λI− ˆX)−1, where the shifting param-
eter λ is carefully chosen. The algorithm that implements
this reduction, originally described in (Garber & Hazan,
2015), is given below (see Algorithm 1).

(cid:110) 1

failure probability p

(cid:17)m1+1
(cid:16)˜δ/8

Algorithm 1 SHIFT-AND-INVERT POWER METHOD
1: Input: estimate ˜δ for the gap ˆδ, accuracy  ∈ (0, 1),

(cid:16) 18d
(cid:17)(cid:101)
2: Set: m1 ← (cid:100)8 ln(cid:0)144d/p2(cid:1)(cid:101), m2 ← (cid:100) 3
(cid:17)m2+1(cid:111)
(cid:16)˜δ/8

3: Set: ˜ ← min
4: Set: λ(0) ← 1 + ˜δ , ˆw0 ← random unit vector, s ← 0
5: repeat
6:
7:
8:

s ← s + 1 , Ms ← (λ(s−1)I − ˆX)
for t = 1...m1 do

Find approx. minimizer - ˆwt of Fλ(s−1), ˆwt−1 (z)
such that (cid:107) ˆwt − M−1

s ˆwt−1(cid:107) ≤ ˜

2 ln

, 
4

p2

16

end for

1

s ws(cid:107) ≤ ˜
s vs−˜ , λ(s) ← λ(s−1) − ∆s
w(cid:62)

Find approx. minimizer - vs of Fλ(s−1),ws(z) such
that (cid:107)vs − M−1

9:
10: ws ← ˆwm1/(cid:107) ˆwm1(cid:107)
11:
12: ∆s ← 1
2 ·
13: until ∆s ≤ ˜δ
14: λ(f ) ← λ(s) , Mf ← (λ(f )I − ˆX)
15: for t = 1...m2 do
Find approx. minimizer - ˆwt of Fλ(f ), ˆwt−1(z) such
16:
that (cid:107) ˆwt − M−1

f ˆwt−1(cid:107) ≤ ˜
17: end for
18: Return: wf ← ˆwm2 /(cid:107) ˆwm2(cid:107)

2

Lemma 5 (Efﬁcient reduction of top eigenvector to con-
vex optimization; originally Theorem 4.2 in (Garber &
Hazan, 2015)). Suppose that ˆδ := λ1( ˆX) − λ2( ˆX) > 0
and suppose that the estimate ˜δ in Algorithm 1 satisﬁes
˜δ ∈ [ˆδ/2, 3ˆδ/4]. Then, with probability at least 1 − p,
f ˆv1)2 ≥
Algorithm 1 ﬁnds a unit vector wf such that (w(cid:62)
(cid:17)(cid:17)
1 − , and the total number of optimization problems of
the form (8) solved during the run of the algorithm, is up-
per bounded by O
. More-
over, throughout the run of the algorithm it holds that
1 + ˆδ ≥ λ(s) − ˆλ1 = Ω(ˆδ).

ln(d/p) ln(ˆδ−1) + ln

(cid:16) d

(cid:16)

p

Remark:
the purpose of the repeat-until loop in Algo-
rithm 1 is to efﬁciently ﬁnd a shifting parameter λ(f ) such
ˆδ for some universal constants
that c1
c2 > c1 > 0. When n satisﬁes n = Ω(δ−2 ln(d/p)),
we can directly ﬁnd (w.h.p) such a shifting parameter, by

ˆδ ≤ λ(f ) − ˆλ1 ≤ c2

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

simply estimating ˆλ1, ˆδ from the data of a single machine.
Also, we can take ˆw0 to be the leading eigenvector of any
single machine, since this will already have a constant cor-
relation with ˆv1. Thus, for such n, the total number of
optimization problems can be reduced to O(ln(p−1−1)).
Algorithm 1 is a meta-algorithm in the sense that the choice
of solver for the optimization problems min Fλ,w is un-
speciﬁed, and any solver will do. A simple calculation
shows that a naive application of either the conjugate gra-
dient method or Nesterov’s accelerated gradient method to
solve these optimization problems in a distributed manner,
i.e., the computation of the gradient vector is distributed

across machines, will require overall ˜O(cid:0)(cid:113)

ˆλ1/ˆδ(cid:1) commu-

nication rounds, which does not give any improvement over
the distributed Lanczos approach, described in Subsection
2.2.2. However, this can be substantially improved by tak-
ing advantage of the fact that the data on all machines is
sampled i.i.d.
In particular,
we present below an approach based on applying a pre-
conditioner to the optimization Problem (8), in the spirit of
the one described in (Zhang & Lin, 2015).

from the same distribution.

4.2. Faster Distributed Approximation of Linear

Systems via Local Preconditioning

Let M = λI− ˆX, for some shift parameter λ > ˆλ1, and de-
ﬁne the pre-conditioning matrix C = (λ+µ)I− ˆX1, where
µ is required so C is invertible. Consider now solving the
following modiﬁed quadratic problem:
˜Fλ,w(y) :=
(9)
Note that if y∗ is the optimal solution to Problem (9), i.e.,

y(cid:62)C−1/2MC−1/2y − y(cid:62)C−1/2w.

1
2

y∗ = C1/2M−1C1/2C−1/2w = C1/2M−1w,

then z∗ := C−1/2y∗ is the optimal solution to Problem (8).
The idea behind choosing C this way is very intuitive. Ide-
ally we could have chosen C = M, making the condi-
tion number of ˜Fλ,w equal to κ( ˜Fλ,w) = 1, which is the
best we can hope for. The problem of course is that this
requires us to explicitly compute M−1/2, which is more
challenging then just computing the leading eigenvector of
ˆX. The next best thing is thus to choose C based only
on the data available on any single machine, which allows
computing C−1/2 without additional communication over-
head, and leads to the choice described above. The follow-
ing lemma, rephrased from (Zhang & Lin, 2015), quantiﬁes
exactly how such a choice of C helps in improving the con-
dition number of the new optimization problem, Problem
(9). The proof is given in the appendix.
Lemma 6. Suppose that µ ≥ (cid:107) ˆX − ˆX1(cid:107). Then, ˜Fλ,w(y)
-strongly convex. In particu-
is 1-smooth and
lar, κ( ˜Fλ,w) ≤ 1 + 2µ/(λ− ˆλ1). Moreover, ﬁxing ˜y ∈ Rd,

(cid:16) λ−ˆλ1

(λ−ˆλ1)+2µ

(cid:17)

p ∈ (0, 1), if we set µ = 4(cid:112)ln(d/p)/n, then the above

if we let ˜z := C−1/2 ˜y, then it holds that (cid:107)˜z − M−1w(cid:107) ≤
(λ − ˆλ1)−1/2(cid:107)˜y − C1/2M−1w(cid:107).
In particular, for any
holds with probability at least 1 − p, where this probability
depends only on the randomness in ˆX1.

4.2.1. SOLVING THE PRE-CONDITIONED LINEAR

SYSTEMS

We now discuss the application of gradient-based algo-
rithms for ﬁnding an approximate minimizer of the pre-
conditioned problem, Problem (9), in our distributed set-
ting. Towards this end we require a distributed implemen-
tation for the ﬁrst-order oracle of ˜Fλ,w(y) (i.e., computa-
tion of the value and gradient vector at a queried point).
A straight-forward implementation of the ﬁrst-order oracle
in our distributed setting is given in Algorithm 2.

Algorithm 2 Distributed First-Order Oracle for ˜Fλ,w(y)
1: Input: shift parameter λ > 0, regularization parameter
2: send ˜y := C−1/2y to machines {2, . . . , m} for C :=

µ > 0, vector w ∈ Rd, query vector y ∈ Rd
(λ + µ)I − ˆX1 {executed on machine 1}

3: for i = 1...m do
4:

send ˜∇i := ˆXi ˜y to machine 1 {executed on each
machine i}

(cid:80)m
5: end for
˜∇i {executed on machine 1}
6: aggregate ˜∇ := 1
2 (λy(cid:62)C−1y − y(cid:62)C−1/2 ˜∇) −
7: compute ˜Fλ,w(y) = 1
8: compute ∇ ˜Fλ,w(y) = λC−1y − C−1/2 ˜∇ − C−1/2w
9: return: ( ˜Fλ,w(y),∇ ˜Fλ,w(y))

y(cid:62)C−1/2w {executed on machine 1}
{executed on machine 1}

i=1

m

We have the following lemma, the proof of which is de-
ferred to the appendix.
Lemma 7. Fix some λ > λ1( ˆX) and w ∈ Rd, and let
1 ≥ µ > 0 be as in Lemma 6. Fix  > 0. Consider the
following two-step algorithm:

1. Apply either the conjugate gradient method or Nes-
terov’s accelerated method with the distributed ﬁrst-
order oracle described in Algorithm 2 to ﬁnd ˜y ∈ Rd
such that ˜Fλ,w(˜y) − miny∈Rd ˜Fλ,w(y) ≤ (cid:48)

1 + 2µ
λ−ˆλ1

2. Return ˜z = C−1/2 ˜y.
(λ − ˆλ1) it holds that
Then, for (cid:48) = 
(cid:107)˜z − (λI − ˆX1)−1w(cid:107) ≤ , and the total number dis-
tributed matrix-vector products with the empirical covari-
(cid:17)(cid:19)
(cid:18)(cid:113)
(cid:17)(cid:107)w(cid:107)/[(λ − ˆλ1)]
ance matrix ˆX required to compute ˜z is upper-bounded by

1 + 2µ(λ − ˆλ1)−1 ln

(cid:16)(cid:16)

1 +

O

.

2µ
λ − ˆλ1

(cid:16)

2

(cid:17)−1

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

4.3. Putting it all together

We now state our main result for this section, which is a
simple consequence of the previous lemmas. The full proof
is given in the appendix.
Theorem 6. Fix  ∈ (0, 1) and p ∈ (0, 1). Suppose that

mn = Ω(δ−2 ln(d/p)). Set µ = 4(cid:112)ln(3d/p)/n. Apply-

ing the Shift-and-Invert algorithm, Algorithm 1, with the
parameters , p/3, and applying the algorithm in Lemma
7 with the parameter µ, to approximately solve the linear
systems, yields with probability at least 1 − p a unit vector
wf such that (w(cid:62)

(cid:115)(cid:112)ln(d/p)
(cid:34)
f ˆv1)2 ≥ 1 − , after executing at most
(cid:33)
(cid:18) d
(cid:19)

(cid:33)
(cid:32)(cid:112)ln(d/p)
(cid:32)(cid:115)

(cid:18) d
(cid:18) 1

(cid:19)
(cid:19)(cid:21)(cid:19)

√
n

√

δ

n

p2

δ2

O

ln

ln

+ ln2

p2

ln

δ

= ˜O

√
1

δ

n

distributed matrix-vector products with the empirical co-
variance matrix ˆX.

we generated samples by taking x = (cid:112)3/2X1/2y where

5. Experiments
To validate some of our theoretical ﬁndings we con-
ducted experiments with single-round algorithms on syn-
thetic data. We generated synthetic datasets using two dis-
tributions. For both distributions we used the covariance
matrix X = UΣU(cid:62) with U being a random d × d or-
thonormal matrix and Σ is diagonal satisfying: Σ(1, 1) =
1, Σ(2, 2) = 0.8, ∀j ≥ 3 : Σ(j, j) = 0.9·Σ(j−1, j−1),
i.e., δ = 0.2. One dataset was generated according to the
normal distributions N (0, X), and for the second datasets
y ∼ U [−1, 1]. In both cases we set d = 300.
Beyond the single-round algorithms that are based on
aggregating the individual ERM solutions described so
far, we propose an additional natural aggregation ap-
proach, based on aggregating the individual projection ma-
trices. More concretely, letting {ˆv(i)
i=1 denote the lead-
ing eigenvectors of the individual machines, let ¯P1 :=
. We then take the ﬁnal estimate w to
1
m
be the leading eigenvector of the aggregated matrix ¯P1.
Note that as with the sign-ﬁxing based aggregation, this
approach also resolves the sign-ambiguity in the estimates
produced by the different machines, which circumvents the
lower bound result of Theorem 3.
For both datasets we ﬁxed the number of machines to
m = 25. We tested the estimation error (i.e., the value
1− (w(cid:62)v1)2 where v1 is the leading eigenvector of X and
w is the estimator) of ﬁve benchmarks vs. the per-machine
sample size n:
the centralized solution ˆv1, the average
of the individual (unbiased) ERM solutions (normalized to
unit norm),the average of ERM solutions with sign-ﬁxing,
and the leading eigenvector of the averaged projection ma-

(cid:80)m

1 ˆv(i)(cid:62)

i=1 ˆv(i)

1 }m

1

trix. We also plotted the average loss of the individual ERM
solutions. Results are averaged over 400 independent runs.
The results for the normal distribution appear in Figure 1.
The results for the uniform-based distribution are very sim-
ilar and are deferred to the appendix. We can see that, as
our lower bound in Theorem 3 suggests, simply averaging
and normalizing the individual ERM solutions has signif-
icantly worse performance than the centralized ERM so-
lution. Perhaps surprisingly, the performance of this esti-
mator is even worse than the average error of an estimate
computed using only a single machine. We see that both
aggregation methods that are based on correlating the in-
dividual ERM solutions, namely the sign-ﬁxing-based esti-
mator, and the proposed averaging-of-projections heuristic,
are asymptotically consistent with the centralized ERM.
In particular, the averaging-of-projections scheme, at least
empirically, signiﬁcantly outperforms the sign-ﬁxing ap-
proach, which justiﬁes further theoretical investigation of
this heuristic. For the sign ﬁxing approach, we can see that
as suggested by our bounds, the estimator is not consistent
with the centralized ERM solution for small values of n.

Figure 1. Estimation error vs. the per-machine sample size n for
a normal distribution.

6. Discussion
We presented communication-efﬁcient algorithms for dis-
tributed statistical estimation of principal components. Fo-
cusing on our results for methods based on a single commu-
nication round, we initiated a study of how to correctly ag-
gregate distributed ERM solutions in a non-convex setting.
An important take-home message of our work is that in a
non-convex setting, simply averaging the local solutions is
not a good idea. On the positive side, we show that a very
simple correction (i.e., sign-ﬁxing) is possible by leverag-
ing the speciﬁc structure of the problem at hand. It is thus
interesting to develop a richer theory of how to perform
such aggregations in more involved non-convex problems.

0100200300400500600n00.10.20.30.40.50.60.70.80.9avg. errorcentralized ERMavg. of ERMssign-fix avg. of ERMs.projection avg.avg. machine lossCommunication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

References
Eigenvalues

and

eigenvectors

of

2x2 matrices.

http://www.math.harvard.edu/archive/
21b_fall_04/exhibits/2dmatrices/.

Allen Zhu, Zeyuan and Li, Yuanzhi. Even faster SVD de-
In Advances
composition yet without agonizing pain.
in Neural Information Processing Systems 29: Annual
Conference on Neural Information Processing Systems
2016, December 5-10, 2016, Barcelona, Spain, pp. 974–
982, 2016a.

Allen Zhu, Zeyuan and Li, Yuanzhi. Fast global conver-

gence of online PCA. CoRR, abs/1607.07837, 2016b.

Balsubramani, Akshay, Dasgupta, Sanjoy, and Freund,
Yoav. The fast convergence of incremental PCA.
In
Advances in Neural Information Processing Systems 26:
27th Annual Conference on Neural Information Process-
ing Systems 2013, pp. 3174–3182, 2013.

Boutsidis, Christos, Woodruff, David P, and Zhong, Peilin.
Optimal principal component analysis in distributed and
In Proceedings of the 48th Annual
streaming models.
ACM SIGACT Symposium on Theory of Computing, pp.
236–249. ACM, 2016.

Garber, Dan and Hazan, Elad. Fast and simple pca via
convex optimization. arXiv preprint arXiv:1509.05647,
2015.

Garber, Dan, Hazan, Elad, Jin, Chi, Kakade, Sham M.,
Musco, Cameron, Netrapalli, Praneeth, and Sidford,
Aaron. Faster eigenvector computation via shift-and-
invert preconditioning. CoRR, abs/1605.08754, 2016.

Golub, Gene H and Pereyra, Victor. The differentiation
of pseudo-inverses and nonlinear least squares problems
whose variables separate. SIAM Journal on numerical
analysis, 10(2):413–432, 1973.

Hotelling, H. Analysis of a complex of statistical variables

into principal components. J. Educ. Psych., 24, 1933.

Jaggi, Martin, Smith, Virginia, Tak´ac, Martin, Terhorst,
Jonathan, Krishnan, Sanjay, Hofmann, Thomas, and Jor-
dan, Michael I. Communication-efﬁcient distributed
dual coordinate ascent. In Advances in Neural Informa-
tion Processing Systems, pp. 3068–3076, 2014.

Jain, Prateek, Jin, Chi, Kakade, Sham M, Netrapalli, Pra-
neeth, and Sidford, Aaron. Matching matrix bern-
stein with little memory: Near-optimal ﬁnite sam-
arXiv preprint
ple guarantees for oja’s algorithm.
arXiv:1602.06929, 2016a.

Jain, Prateek, Jin, Chi, Kakade, Sham M, Netrapalli, Pra-
neeth, and Sidford, Aaron. Matching matrix bern-
stein with little memory: Near-optimal ﬁnite sam-
arXiv preprint
ple guarantees for oja’s algorithm.
arXiv:1602.06929, 2016b.

Jolliffe, IT. Principal component analysis. 2002. Spring-

verlag, New York, 2002.

Lee, Jason D., Ma, Tengyu, and Lin, Qihang. Distributed
stochastic variance reduced gradient methods. CoRR,
abs/1507.07595, 2015.

Liang, Yingyu, Balcan, Maria-Florina F, Kanchanapally,
Improved distributed

Vandana, and Woodruff, David.
principal component analysis. In NIPS, 2014.

Magnus, Jan R. On differentiating eigenvalues and eigen-

vectors. Econometric Theory, 1(02):179–191, 1985.

Pearson, K. On lines and planes of closest ﬁt to systems of
points in space. Philosophical Magazine, 2(6):559–572,
1901.

Reddi, Sashank J., Konecn´y, Jakub, Richt´arik, Peter,
P´oczos, Barnab´as, and Smola, Alexander J. AIDE:
fast and communication efﬁcient distributed optimiza-
tion. CoRR, abs/1608.06879, 2016.

Shamir, Ohad. A stochastic PCA and SVD algorithm with
an exponential convergence rate. In Proceedings of the
32nd International Conference on Machine Learning,
ICML 2015, Lille, France, 6-11 July 2015, pp. 144–152,
2015.

Shamir, Ohad. Convergence of stochastic gradient descent
for PCA:. In Proceedings of the 33nd International Con-
ference on Machine Learning, ICML 2016, New York
City, NY, USA, June 19-24, 2016, pp. 257–265, 2016a.

Shamir, Ohad. Without-replacement sampling for stochas-
tic gradient methods. In Advances in Neural Information
Processing Systems 29: Annual Conference on Neural
Information Processing Systems 2016, December 5-10,
2016, Barcelona, Spain, pp. 46–54, 2016b.

Shamir, Ohad. Fast stochastic algorithms for svd and pca:
Convergence properties and convexity. In Proceedings of
The 33rd International Conference on Machine Learn-
ing, pp. 248–256, 2016c.

Shamir, Ohad, Srebro, Nathan,

and Zhang, Tong.
Communication-efﬁcient distributed optimization using
an approximate newton-type method. In Proceedings of
the 31th International Conference on Machine Learning,
ICML 2014, Beijing, China, 21-26 June 2014, pp. 1000–
1008, 2014.

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

Tropp, Joel A. User-friendly tail bounds for sums of ran-
dom matrices. Foundations of Computational Mathe-
matics, 12(4):389–434, 2012.

Yu, Yi, Wang, Tengyao, and Samworth, Richard J. A use-
ful variant of the davis–kahan theorem for statisticians.
Biometrika, 102(2):315–323, 2015.

Zhang, Yuchen and Lin, Xiao. Disco: Distributed opti-
mization for self-concordant empirical loss. In Proceed-
ings of the 32nd International Conference on Machine
Learning (ICML-15), pp. 362–370, 2015.

Zhang, Yuchen, Duchi, John C, and Wainwright, Martin J.
Communication-efﬁcient algorithms for statistical opti-
mization. Journal of Machine Learning Research, 14:
3321–3363, 2013.

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

A. Proofs Omitted from Section 3
A.1. Proof of Theorem 3
Proof. Consider the following distribution over R2.

where e1 is the ﬁrst standard basis vector in R2.
The population covariance matrix and the empirical covariance matrix of a sample of size n are clearly given by

(cid:18) 1

(cid:19)

1, 2 ∼ U{−1, +1},

,

x = e1 +

2

(cid:19)

(cid:18) 2 0

0 1

X =

,

ˆX(n) =

(cid:18) 2

yn

(cid:19)

,

yn
1

where yn is a random variable which is the average of n U{−1, +1} random variables. By elementary calculations we
have that the leading eigenvector of ˆX(n) is given by

where

ˆv1 = σ · C(yn) ·

(cid:32)

1 +

C(yn) :=

(cid:32)

1,

2yn

(cid:33)
1 +(cid:112)1 + 4y2
(cid:33)2−1/2

n

,

1 +(cid:112)1 + 4y2

2yn

n

2 ≤ C(yn) ≤ 1. The
is the normalization factor that guarantees that ˆv1 is a unit vector. In particular, it holds that 1/
random variable σ ∼ U{−1, +1} is independent of yn and determines the sign of ˆv1, which follows from our assumption
that ˆv1 is generated by unbiased ERM.
Consider now the average of m such unit vectors ˆv(1)
¯v1/(cid:107)¯v1(cid:107), and recall that the leading eigenvector of the population covariance matrix is e1. It holds that

and the normalized estimate

given by ¯v = 1
m

(cid:80)m

1 ..ˆv(m)

i=1 ˆv(i)

1

1

√

(cid:104) ¯v1(cid:107)¯v1(cid:107) , e1(cid:105)2 =

¯v1(1)2

¯v1(1)2 + ¯v1(2)2 = 1 −

¯v1(2)2

¯v1(1)2 + ¯v1(2)2 .

(10)

Towards upper-bounding the RHS of (10) in expectation, the main step is to lower bound the random variable |¯v1(2)| using
Chebyshev’s inequality.
It holds that

E[|¯v1(2)|] = E

E

=
(a)

m

(cid:20)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1

m

m(cid:88)

(cid:113)

2C(y(i)

n )y(i)
n
1 + 4y(i)2

m

ˆv(i)
1 (2)

i=1

1 +

= E

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:113)
σ(i) 2C(y(i)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
m(cid:88)
 1
m(cid:88)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)
(cid:35)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:21)
m(cid:88)
E{y(i)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)E{y(i)
(cid:34)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) 1
m(cid:88)

σ(i)

n }

n }

i=1

i=1

m

m

m

i=1

n )|y(i)
n |
1 + 4y(i)2

n

i=1

1 +

σ(i)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)


(cid:113)
σ(i) 2C(y(i)
(cid:113)
σ(i) 2C(y(i)
(cid:34)
1 +(cid:112)1 + 4y2

n )|y(i)
n |
1 + 4y(i)2
n )|y(i)
n |
1 + 4y(i)2

1 +
2C(yn)|yn|

1 +

n

n

(cid:35)

n

· Eyn

n

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) | {σ(i)}

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:18) 1√

| {σ(i)}

Θ

=
(d)

mn

= E{σ(i)}

≥

(b)

=
(c)

E{σ(i)}

E{σ(i)}

(cid:19)

,

(11)

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

n ∼ σ(i)|y(i)

where (a) follows since σ(i)y(i)
n )/(1 +
triangle inequality, and (c) follows since {σ(i)}i∈[m] and {y(i)

to verify that (d) follows since(cid:80)m

n |, (b) follows from the
n }i∈[m] are independent random variables. Finally, it is easy
i=1 σ(i)/m is the average of m U{−1, +1} random variables and hence its expected
n) is lower

√
m). Similarly the expected absolute value of yn is Θ(1/

n ) depends only on |y(i)

n | and C(y(i)

1 + 4y(i)2

(cid:113)

√
absolute value is Θ(1/
bounded by a positive constant.
Also, observe that

n) and C(yn)/(1 +(cid:112)1 + 4y2
(cid:32)

1 +(cid:112)1 + 4y2

(cid:33)2

2C(yn)yn

n

E[¯v1(2)2] = E

ˆv(i)
1 (2)

(cid:34)(cid:18) 1

m

(cid:19)2(cid:35)
(cid:18) 1

E[ˆv1(2)2] =

E

1
m

1
m

=

(cid:19)

(13)

(14)

≥

E[y2

1
2m

n] = Θ
where the inequality follows since |yn| ≤ 1 and 1/
Combining Eq. (11) and Eq. (12), we have by an application of Chebyshev’s inequality to the random variable |¯v1(2)| that
there exists universal constants c1 > 0 such that

mn
√
2 ≤ C(yn) ≤ 1.

,

(12)

(cid:18)

(cid:19)

|¯v1(2)| ≤

√
1
mn

c1

≤ 1
4

.

Pr

Also, it is easy to verify that

E[¯v1(1)2] = O(1/m),

E[¯v1(2)2] = O(1/m).

Thus, by a simple application of Markov’s inequality we have that there exists a universal constant c2 > 0 such that

(cid:18)

(cid:19)

Pr

max{¯v1(1)2, ¯v1(2)2} ≥ 1
c3m

≤ 1
4

.

Using Eq. (10), (13) and (14) we ﬁnally have that

(cid:20)
(cid:104) ¯v1(cid:107)¯v1(cid:107) , e1(cid:105)2

(cid:21)

E

(cid:20)

(cid:21)

= 1 − Ω

(cid:18) 1

(cid:19)

n

.

¯v1(2)2

¯v1(1)2 + ¯v1(2)2

= 1 − E

A.2. Proof of Lemma 2
Proof. The proof is based on viewing ˆA as an unbiased perturbation of the matrix A, and computing a Taylor expansion
of ˆv1 around v1. For notational convenience, let E = ˆA − A, and deﬁne A(t) = A + tE for t ∈ [0, 1]. Also, deﬁne λ(t)
to be the leading eigenvalue of A(t).
First, we note that for any t ∈ [0, 1], A(t) has an eigengap of at least δ/2 between its ﬁrst two eigenvalues (since by
Weyl’s inequality, its eigenvalues are at most (cid:107)tE(cid:107) ≤ (cid:107)E(cid:107) ≤ δ/4 different than A, and we know that A has an eigengap
of δ). Therefore, the leading eigenvalue of A(t) is simple. This means that the function v(t), which equals the leading
eigenvector of A(t), is uniquely deﬁned up to a sign. This sign will be chosen so that (cid:104)v(t), v1(cid:105) ≥ 0, which makes v(t)
unique and well-deﬁned2. By Theorem 1 in (Magnus, 1985), we have that both λ(t) and v(t) are inﬁnitely differentiable
at any t ∈ [0, 1], and satisfy3

λ(cid:48)(t) = v(t)(cid:62)Ev(t) , v(cid:48)(t) = (λ(t)I − A(t))†Ev(t) .

(Theorem 2),(cid:112)1 − (cid:104)v(t), v1(cid:105)2 ≤ 2(cid:107)A(t)−A(cid:107)

δ

≤ 2(cid:107)E(cid:107)

δ ≤ 1
2 .

2Note that ties are impossible, since that can only happen if (cid:104)v(t), v1(cid:105) = 0, yet by applying the Davis-Kahan sin(θ) theorem

3Formally speaking, the theorem only ensures v(t), λ(t) exist and are inﬁnitely differentiable in some open neighborhood of t.
However, since the result holds for any t ∈ [0, 1], and the proof implies that these functions are unique in each such neighborhood
(where the uniqueness of v(t) holds once we ﬁxed the sign as above), it follows that the same holds in all of t ∈ [0, 1].

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

We will also need to bound the second derivative of v(t). By the product rule and the equations above, this derivative
equals

(cid:0)(λ(t)I − A(t))†(cid:1) Ev(t) + (λ(t)I − A(t))†E
(cid:0)(λ(t)I − A(t))†(cid:1) Ev(t) + (λ(t)I − A(t))†E(λ(t)I − A(t))†Ev(t).

∂
∂t

v(t)

v(cid:48)(cid:48)(t) =

=

∂
∂t
∂
∂t

(15)

(cid:19)

−B†(cid:18) ∂

To compute the derivative above, we apply the chain rule. The derivative of a pseudo-inverse B† of a matrix-valued
function B = B(t) with respect to t (assuming B and hence its pseudo-inverse is symmetric for all t) is given by (see
Theorem 4.3 in (Golub & Pereyra, 1973))

(cid:19)(cid:0)B†(cid:1)2
∂t (λ(t)I − A(t))(cid:13)(cid:13) =
λ(t)I − A(t) (which indeed has a ﬁxed rank of d − 1 by the eigengap assumption), noting that(cid:13)(cid:13) ∂
(cid:13)(cid:13)v(t)(cid:62)Ev(t)I − E(cid:13)(cid:13) ≤ 2(cid:107)E(cid:107), and using the facts that (cid:107)v(t)(cid:107) = 1, (cid:107)I − B†B(cid:107) ≤ 1,(cid:107)I − BB†(cid:107) ≤ 1 and

This formula is true assuming the rank of B is constant in some open neighborhood of t. Applying this for B =

B† +(cid:0)B†(cid:1)2(cid:18) ∂

(cid:107)(λ(t)I − A(t))†(cid:107) ≤ 2/δ (since the smallest non-zero eigenvalue of λ(t)I − A(t) is at least δ/2), we have that

(I − BB†) + (I − B†B)

(cid:18) ∂

(cid:19)

∂t

∂t

B

∂t

B

B

.

(cid:13)(cid:13)(cid:13)(cid:13) ∂

∂t

(cid:0)(λ(t)I − A(t))†(cid:1)(cid:13)(cid:13)(cid:13)(cid:13) ≤ 24 · (cid:107)E(cid:107)

δ2

.

Plugging this into (15), and again using the fact that (cid:107)(λ(t)I − A(t))†(cid:107) ≤ 2/δ, we get that

(cid:107)v(cid:48)(cid:48)(t)(cid:107) ≤ c(cid:107)E(cid:107)2

δ2

for some numerical constant c.
By a ﬁrst-order Taylor expansion of v(t) with an explicit remainder term4,

v(1) = v(0) + v(cid:48)(0) +

1
2

(1 − t)2v(cid:48)(cid:48)(t)dt ,

which by the equations above and the deﬁnition of v(t) implies that

(cid:90) 1
(cid:90) 1

t=0

1
2

t=0

ˆv1 = v1 + (λ1I − A)†Ev1 +

(1 − t)2v(cid:48)(cid:48)(t)dt .

This implies

(cid:13)(cid:13)ˆv1 − v1 − (λ1I − A)†Ev1

(cid:13)(cid:13) ≤ 1

2

(cid:90) 1

t=0

(1 − t)2(cid:107)v(cid:48)(cid:48)(t)(cid:107)dt ≤ c(cid:107)E(cid:107)2

2λ2

(cid:90) 1

t=0

(1 − t)2dt,

which is at most c(cid:48)(cid:107)E(cid:107)2/λ2 for some appropriate numerical constant c(cid:48). Plugging back E = ˆA−A, the result follows.

A.3. Proof of Lemma 3

Proof. Using the matrix Hoeffding inequality (Theorem 1) and a union bound, we that
− δ2n
c(cid:48)b2

∃i, (cid:107) ˆXi − X(cid:107) >

≤ md exp

δ
12

Pr

(cid:18)

(cid:19)

(cid:18)

(cid:19)

(16)

for some constant c(cid:48) > 0. Thus, with high probability, maxi (cid:107) ˆXi − X(cid:107) ≤ δ/12. By Weyl’s inequality, it follows that the
eigenvalues of X and ˆXi are at most δ/12 apart, and since X has an eigengap of δ between its two leading eigenvalues,
4Since v(t), v(cid:48)(t), v(cid:48)(cid:48)(t) are all vectors, this is a direct consequence of the standard Taylor expansion of the scalar function t (cid:55)→
v(t)j, mapping t to the j-th coordinate of v(t), using the fact that this mapping is differentiable to any order (see Theorem 1 in (Magnus,
1985), and in particular twice continuously differentiable.

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

it follows that ˆXi has an eigengap of at least δ − δ/12 − δ/12 > 0, which proves the ﬁrst part of the lemma. To
handle the second part, note that by a variant of the Davis-Kahan sinθ theorem (see Corollary 1 in (Yu et al., 2015)), if
maxi (cid:107) ˆXi − X(cid:107) ≤ δ/12, then the leading eigenvectors ˆvi
1, v1(cid:105) ≥ 0)
are all at a distance of at most 1/4 from v1. Moreover, by Lemma 2,
1 − v1 − (λ1I − X)†( ˆXi − X)v1

1 of ˆXi (after choosing the sign appropriately, i.e. (cid:104)ˆvi

(cid:107) ˆXi − X(cid:107)2.

m(cid:88)

(cid:13)(cid:13)(cid:13)ˆvi

By the triangle inequality, this implies

i=1

1 − v1 − (λ1I − X)†
ˆvi

(cid:107) ˆXi − X(cid:107)2,

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ c

and therefore (as (cid:107)v1(cid:107) = 1),

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) .
(cid:13)(cid:13)(λ1I − X)†(cid:13)(cid:13) ≤ 1/δ. As to the other terms, recall that ˆXi is the average of n i.i.d. matrices with mean X, and 1

Since X has an eigengap of δ, it follows that the minimal non-zero eigenvalue of λ1I − X is at least δ, and therefore
ˆXi
is the average of mn such i.i.d. matrices. Thus, by a matrix Hoeffding inequality (Theorem 1) and a union bound, it holds
with probability at least 1 − p that

(cid:80)m

1 − v1
ˆvi

δ2 · 1

ˆXi − X

m(cid:88)

(17)

i=1

i=1

i=1

i=1

m

m

m

(cid:32)

1
m

m

i=1

m(cid:88)

δ2 · 1

(cid:13)(cid:13)(cid:13) ≤ c
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ c
(cid:33)
m(cid:88)
δ2 · 1
(cid:107) ˆXi − X(cid:107)2 +(cid:13)(cid:13)(λ1I − X)†(cid:13)(cid:13) ·

( ˆXi − X)

v1

i=1

m

i=1

m(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
m(cid:88)

m

1
m

m(cid:88)
m(cid:88)

i=1

m

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

as well as

(cid:114)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ c1
(cid:114)

b2 log(2dm/p)

n

b2 log(2dm/p)

mn

(cid:114)

∀i, (cid:107) ˆXi − X(cid:107) ≤ c1

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

m

ˆXi − X

i=1

m(cid:88)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤ cc2

(cid:16)− δ2n
(cid:17)
,(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) 1

c(cid:48)b2

m

m(cid:88)

i=1

1 − v1
ˆvi

1b2 log(2dm/p)

δ2n

+ c1

b2 log(2dm/p)

δ2mn

.

for some constant c1. Combining this with (16) using a union bound, and plugging into (17), it follows that with probability
at least 1 − p − d exp

Slightly simplifying, the result follows.

A.4. Proof of Theorem 4

Proof of Thm. 4. The proof is an easy consequence of Lemma 3. Assuming the events in the lemma occur, we have that
the leading eigenvalues of X as well as ˆXi for all i are simple, hence the leading eigenvectors are all unique up to a sign.
In particular, let v1 be the eigenvector closest to ˜w1 = ˆw1, with ties broken arbitrarily, so that (cid:107) ˆw1 − v1(cid:107) ≤ (cid:107) ˆw1 + v1(cid:107).
1 is as deﬁned in Lemma 3), since otherwise, by the inequality above, we would get
This implies that ˆw1 = ˆv1
(cid:107)− ˆv1
is at most 1/4 by Lemma 3.
Having established that ˆw1 = ˆv1

1 − v1(cid:107) =(cid:112)2 − 2(cid:104)ˆv1, v1(cid:105)

1, we note that by Lemma 3 and the triangle inequality, for any i > 1,

1, v1(cid:105) ≤ 0, contradicting the fact that (cid:107)ˆv1

1 + v1(cid:107), which implies in turn (cid:104)ˆv1

1 − v1(cid:107) ≤ (cid:107)− ˆv1

1 (where ˆv1

(cid:107)ˆvi

1(cid:107) ≤ 1
2

and therefore (cid:107)ˆvi

As ˆvi
the sign chosen based on which vector is closest to ˆw1, it follows that ˆwi = ˆvi

1 − ˆv1
1, ˆw1 are unit vectors, this implies that (cid:107)ˆvi
m(cid:88)

we get that with probability at least 1 − p − d exp(cid:0)−δ2n/cb2(cid:1),
(cid:114)

(cid:13)(cid:13)(cid:13) ≤ c(cid:48)(cid:16) b2 log(2dm/p)
(cid:17)

1 − ˆw1(cid:107) < (cid:107) − ˆvi

(cid:13)(cid:13)(cid:13) 1

ˆwi − v1

b2 log(2dm/p)

1 − ˆw1(cid:107) ≤ 1
2

δ2n

i=1

m

.

+

δ2mn

.

1 − ˆw1(cid:107). Since for any i, we have ˆwi ∈ {−ˆvi

1}, with
1,
1 for all i. Applying Lemma 3 with ˆwi = ˆvi

1, ˆvi

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

Squaring both sides and using the fact that (x + y)2 ≤ 2x2 + 2y2, we get that

(cid:13)(cid:13)(cid:13) 1

m

m(cid:88)

i=1

(cid:13)(cid:13)(cid:13)2 ≤ 2(c(cid:48))2(cid:16) b4 log2(2dm/p)

δ4n2

ˆwi − v1

(cid:17)

This holds with probability at least 1 − p − d exp(cid:0)−δ2n/cb2(cid:1). To simplify things a bit, note that we can assume

d exp(−δ2n/cb2) ≤ p without loss of generality, since otherwise the bound in the displayed equation above is at least
a constant and therefore trivially true (holds with probability 1) if we make the constant c(cid:48) sufﬁciently large. Therefore, we
can argue that (18) (with an appropriate c(cid:48)) holds with probability at least 1 − 2p. Absorbing the 2 factor into the p term,
slightly increasing c(cid:48) appropriately, and simplifying a bit, the result ﬁnally follows from the simple observation that

b2 log(2dm/p)

δ2mn

(18)

+

.

1
2

(cid:0)2 − (cid:107)w − v1(cid:107)2(cid:1) ≥
(cid:13)(cid:13)(cid:13)w − 1
(cid:13)(cid:13)(cid:13)2 − 2
(cid:13)(cid:13)(cid:13) 1
m(cid:88)
(cid:13)(cid:13)(cid:13)2
(cid:13)(cid:13)(cid:13) 1
m(cid:88)

m
ˆwi − v1

ˆwi

i=1

,

m

m

i=1

(v(cid:62)

(cid:16)

1 w)2 =
2 − 2

1
2

≥ 1 − 2

(cid:13)(cid:13)(cid:13)2(cid:17)

ˆwi − v1

m(cid:88)

i=1

where the ﬁrst inequality follows from the triangle inequality and the inequality (a + b)2 ≤ 2a2 + 2b2, and the second
inequality follows since v1 is a unit vector, and by deﬁnition, w is the unit vector closest to 1
m

i=1 ˆwi.

(cid:80)m

A.5. Proofs of Theorem 5

The proof is a combination of the following two lemmas, each proves one of the lower bounds. We ﬁrst state the two
lemmas and then prove them.
Lemma 8. For any δ ∈ (0, 1) and d > 1, there exist a distribution over vectors in Rd (of norm at most 2) such that the
covariance matrix has eigengap δ, and for any number of machines m and per-machine sample size n, the aggregated
vector ¯v1 = 1
m

(even after sign ﬁxing) satisﬁes

(cid:80)m

i=1 ˆv(i)

1

(cid:18)

(cid:26) 1

= Ω

min

,

1

δ2mn

m

(cid:27)(cid:19)

.

E

(cid:21)

(cid:20)
1 − (cid:104) ¯v1(cid:107)¯v1(cid:107) , e1(cid:105)2
(cid:80)m

Lemma 9. For any δ ∈ (0, 1) and d > 1, there exist a distribution over vectors in Rd (of norm at most 2) with eigengap
δ in the covariance matrix, such that for any number of machines m and for per-machine sample size any n sufﬁciently
larger than 1/δ2, the aggregated vector ¯v1 = 1
(even after sign ﬁxing with the population eigenvector v1)
m
satisﬁes

1

(cid:20)
i=1 ˆv(i)
1 − (cid:104) ¯v1(cid:107)¯v1(cid:107) , e1(cid:105)2

(cid:21)

E

(cid:18) 1

(cid:19)

.

= Ω

δ4n2

proof of Lemma 8. We will prove the result for d = 2 (i.e. a distribution in R2). This is without loss of generality, since
we can always embed the distribution below in Rd for any d > 2 (say, by having all coordinates other than the ﬁrst two
identically zero).
Consider the distribution deﬁned by the random vector x =
and e1 = (1, 0), e2 = (0, 1) are the standard basis vectors. Clearly, the population covariance matrix is

1 + δe1+σe2, where σ is uniformly distributed on {−1, +1},

√

(cid:18) 1 + δ

0

(cid:19)

,

0
1

X := E[xx(cid:62)] =

with a leading eigenvector (1, 0). Let us now consider the distribution of the output of a machine i. Given n samples, the
empirical covariance matrix is

(cid:18) 1 + δ

yn

(cid:19)

yn
1

ˆX(n) =

, yn :=

√

1 + δ · 1
n

n(cid:88)

i=1

i,

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

where i are i.i.d. and uniformly distributed on {−1, +1}. Using a standard formula for the leading eigenvector of a 2 × 2
matrix (eig), we have that the leading eigenvector (and hence the output of any machine i) is of the form

(cid:32)

(cid:114)

(cid:33)

ˆv1 =

1
(cid:107)ˆu(cid:107) ˆu where ˆu :=

δ
2

+

δ2
4

+ y2

n , yn

.

(19)

Note that with this formula, the leading eigenvector is always closer to (1, 0) than (−1, 0), and converges to (1, 0) as
n → ∞. Thus, we can view the random variable ˆv(i) as the output of any machine i, given n samples and after ﬁxing the
sign.

Consider now the average of m such vectors given by ¯v = 1
m

i=1 ˆv(i)

1 . Using (19), we have that

E[¯v1(2)2] = E

m(cid:88)

i=1

1
m

ˆv(i)
2

(cid:32)


E

(cid:80)m
(cid:33)2 =
(cid:113) δ2

y2
n

1
m2

m(cid:88)
 .

i=1

δ2
2 + 2y2

n + δ

4 + y2
n

=

1
m

E[(ˆv(i)

2 )2] =

E[(ˆv(2))2]

1
m

(20)

By deﬁnition of yn and recalling that δ ∈ [0, 1], we have that there exist universal constants c1, c2 > 0 such that with
n ≥ c2/n. Using this fact and considering the two cases 1/n ≥ δ2 and
constant probability it holds that c1/n ≥ y2
1/n < δ2 in the RHS of Eq. (20) separately, we can see that

(cid:18) 1

(cid:19)

E[¯v1(2)2] = Ω

min{1,

1
δ2n

}

m

.

(21)

Using Eq. (21) we have that

(cid:20)
(cid:104) ¯v1(cid:107)¯v1(cid:107) , e1(cid:105)2

(cid:21)

E

(cid:20)

(cid:21)
≤ 1 − E(cid:2)¯v1(2)2(cid:3) = 1 − Ω

¯v1(1)2 + ¯v1(2)2

= E

¯v1(1)2

= 1 − E

(cid:18)

min

where the inequality follows since (cid:107)¯v1(cid:107) ≤ 1.

(cid:20)
(cid:26) 1

¯v1(2)2

¯v1(1)2 + ¯v1(2)2

(cid:27)(cid:19)

,

,

1

δ2mn

m

(cid:21)

proof of Lemma 9. As in Lemma 8, we prove the result for d = 2, however, using a different construction. Consider the
deﬁned by the random vector

where ξ is an independent random variable deﬁned as:

√

(cid:26) √

x =

1 + δ · e1 + ξ · e2,

ξ =

w.p. 1/3
2 w.p. 2/3

√
2
−1/
√

It is easy to verify that E[ξ] = 0, E[ξ2] = 1, E[ξ3] = 1/
2. As we shall see, choosing ξ to be asymmetric (as opposed to 
in the proof of Lemma 9) will be key to our construction. Clearly, the population covariance and the empirical covariance
of a sample of size n are given by we have

X = E[xx(cid:62)] =

,

ˆX(n) =

where

√

1 + δ · 1
n

yn :=

ξi ,

zn :=

(cid:18) 1 + δ

0

0
1

(cid:19)
n(cid:88)

i=1

(cid:19)

,

yn
zn

(cid:18) 1 + δ
n(cid:88)

yn

ξ2
i ,

1
n

i=1

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

with ξ1, . . . , ξn being i.i.d. copies of the random variable ξ.
Clearly the leading eigenvector of X is e1 = (1, 0). Consider now ˆv(1)
empirical covariance matrices of n samples, ˆX(1)
to the leading eigenvector of the population covariance e1. In the following, we let ˆvi
eigenvector ˆv(i)
1 .
It holds that

(n), . . . , ˆXm)

1 , . . . , ˆv(m)

1

to be the leading eigenvectors of m i.i.d.
(n), and let ¯v1 denote their average after sign-ﬁxings according
j denote the jth coordinate in the

(cid:20)
(cid:104) ¯v1(cid:107)¯v1(cid:107) , e1(cid:105)2

(cid:21)

E

(cid:20)
m(cid:88)

¯v1(2)2

¯v1(1)2 + ¯v1(2)2

sign(ˆvi

1)ˆvi
2

(cid:21)
(cid:33)2

(cid:20)

¯v1(1)2

= E

¯v1(1)2 + ¯v1(2)2

(cid:21)
≤ 1 − E(cid:2)¯v1(2)2(cid:3) = 1 − E
E(cid:2)sign(ˆvi
2](cid:1)2

(cid:32)

1
m

i=1

= 1 − E

(cid:32)
(cid:3)(cid:33)2

1
m

i=1

1)ˆvi
2

≤ 1 −

m(cid:88)
= 1 −(cid:0)E[sign(ˆv1
2](cid:1)2, where ˆv1 is the leading eigenvector computed by machine 1.

1)ˆv1

,

(22)
where the ﬁrst inequality follows since (cid:107)¯v1(cid:107) ≤ 1, the second inequality follows from Jensen’s inequality, and the last
equality follows from the fact that ˆv(1)
are i.i.d. random variables. From this chain of inequalities, it follows

1 , . . . , ˆv(m)

that it is enough to lower bound(cid:0)E[sign(ˆv1
 δ + 1 − zn

1
1)ˆv1

Let us now consider the distribution of the leading eigenvector of the empirical covariance matrix ˆX(n). Using a standard
formula for the leading eigenvector of a 2 × 2 matrix (eig), we have that this leading eigenvector ˆv1 is proportional to

(cid:115)(cid:18) δ + 1 − zn

(cid:19)2



+

2

2

+ y2

n , yn

(23)

Assume for now that zn ≤ 1 + cδ for some positive constant c to be ﬁxed later (note this happens with arbitrarily high
probability as n → ∞, as zn converges to 1 in probability). In that case, the sign of the ﬁrst coordinate in the formula
above is positive, and has the same sign as the ﬁrst coordinate of the leading eigenvector v1 = (1, 0). Moreover, we know
that ˆv(1)

1 must have unit norm, from which follows that

(cid:18)
(cid:115)

(cid:113)(cid:0) δ+1−zn

(cid:1)2
(cid:113)(cid:0) δ+1−zn

2

(cid:1)2

+ y2

n , yn

(cid:19)
(cid:19)2

+ y2
n

δ+1−zn

+

2

(cid:18)

y2
n +

δ+1−zn

2

+

sign(ˆv1

1) · ˆv(1)

1 =

.

(24)

2

(cid:1)2

sign(ˆv1

1) · ˆv1

2 =

=

y2
n +

(cid:115)
(cid:18)
(cid:118)(cid:117)(cid:117)(cid:116)y2
n +(cid:0) δ+rn

2

(cid:1)2

δ+rn

2 +

yn

(cid:113)(cid:0) δ+rn
(cid:32)
(cid:114)

yn

2

1 +

1 +

+ y2
n

(cid:19)2
(cid:17)2(cid:33)2
(cid:16) 2yn

δ+rn

.

(25)

In particular, letting rn = 1 − zn, we have that if rn ≥ −cδ, then

Towards using Eq. (22) to derive the lower bound, the main step is to bound the expectation of the RHS of Eq.(25) away
from zero. To get an intuition why this is possible, observe that when n → ∞ (in particular, when it is signiﬁcantly larger
than 1/δ2), it holds that

RHS of (25) ≈

yn(cid:112)y2

,

n + Θ(δ2)

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

(cid:35)

(cid:35)

(cid:34)

(cid:34)

since in this regime, with high probability, rn << δ and yn << 1. Now comes to play our choice of ξ to be an asymmetric
random variable. If, just for sake of intuition, we set n = 1, it is easy to verify that despite the fact that E[yn] = 0, it holds
that

Note in particular that taking ξ to be uniformly distributed on {−1, +1}, as in Lemma 8, will still give zero expectation,
and hence will not work. We now formalize this intuition. We will use a Taylor expansion of the formula above, in order to

(cid:3)(cid:1)2 would follow. To that end, deﬁne

E

= E

n + Θ(δ2)

yn(cid:112)y2

ξ(cid:112)ξ2 + Θ(δ2)
bound its expectation (over yn, rn), from which a lower bound on(cid:0)E(cid:2)sign(ˆv1
(cid:17)2(cid:33)2
(cid:16) 2tyn

(cid:118)(cid:117)(cid:117)(cid:116)(tyn)2 +(cid:0) δ+trn

the function

(cid:114)

(cid:1)2

(cid:32)

g(t) =

1 +

1 +

tyn

< 0.

1) · ˆv1

2

, t ∈ [0, 1],

2

δ+trn

1) · ˆv1

and note that g(1) equals sign(ˆv1

s3
6
for some s ∈ [0, 1]. A tedious calculation of g’s derivatives5 reveals that this implies

2 as deﬁned above. By a Taylor expansion, we have
1) · ˆv1
g(cid:48)(cid:48)(cid:48)(s)

2 = g(1) = g(0) + g(cid:48)(0) +

g(cid:48)(cid:48)(0) +

sign(ˆv1

1
2

(cid:19)

,

(cid:18)|yn|3 + |rn|3
δ2 ± O
(cid:17)
δ2 ± O(cid:16)|yn|3+|rn|3

δ3

δ3

1) · ˆv1

− rnyn

yn
δ

2 =

sign(ˆv1

1) · ˆv1

(26)
assuming max{|yn|,|rn|} ≤ cδ for some constant c (hence ﬁxing c we used in our earlier assumptions on rn, zn). To
simplify notation, let qn = sign(ˆv1
be the expression on the right-hand side
of the equation above, and let A be the event that max{|yn|,|rn|} ≤ cδ indeed holds. Also, note that with probability 1,
|qn| ≤ 1 and |bn| = O(1/δ3). Thus, by Eq. (26), we have that E[qn|A] = E[bn|A], and therefore

2, let bn = yn

δ − rnyn

E[qn] = Pr(¬A) · E[qn|¬A] + Pr(A) · E[qn|A]
= Pr(¬A) · E[qn|¬A] + Pr(A) · E[bn|A]
= Pr(¬A) · E[qn|¬A] + E[bn] − Pr(¬A) · E[bn|¬A]

E(cid:2)sign(ˆv1

Plugging back the deﬁnitions of qn, bn, A, we get that
δ2 ± O
√

Recalling that yn =
bounded random variable satisfying E[ξ3] = 1/
equals

= E[bn] ± O(cid:0)Pr(¬A)/δ3(cid:1) .
(cid:18)|yn|3 + |rn|3
(cid:19)(cid:21)
(cid:20) yn
(cid:3) = E
(cid:80)n
(cid:80)n
δ
i=1 ξi and rn = 1 − zn = 1 − 1
(cid:18) 1
(cid:19)
δ3 exp(−Ω(nδ2))

(cid:19)
(cid:1) assuming n is sufﬁciently larger than 1/δ2. As a result, we get that(cid:0)E(cid:2)sign(ˆv1

(cid:18) 1
(cid:19)
δ3 Pr(max{|yn|,|rn|} > cδ)
i , where ξi are i.i.d. copies of a zero-mean,
2, and using Hoeffding’s inequality, it is easily veriﬁed that the above

which is Ω(cid:0) 1

(cid:3)(cid:1)2

= Ω(cid:0) 1

δ4n2

(cid:1) as

1) · ˆv1
√

1√
2δ2n

1 + δ · 1

− rnyn

1) · ˆv1

(δ2n)3/2

i=1 ξ2

± O

± O

± O

(cid:18)

1 + δ

0 +

√

δ2n

δ3

1

n

n

2

2

.

,

required.

B. Proofs Omitted from Section 4
B.1. Proof of Lemma 4
Proof. Let • denote the standard inner product for matrices, i.e., A • B = Tr(AB(cid:62)). It holds that
1 − (cid:107)ww(cid:62) − ˆv1 ˆv(cid:62)
1 (cid:107)F · (cid:107)v1v(cid:62)
1 (cid:107)
√
2(1 − 1(w(cid:62) ˆv1)2) ≥ (w(cid:62)v1)2 −
2.

= (w(cid:62)v1)2 −(cid:113)

(w(cid:62)v1)2 = ww(cid:62) • v1v(cid:62)

1 ≥ ˆv1 ˆv(cid:62)

1 • v1v(cid:62)

5Using MATLAB’s symbolic math toolbox together with some straightforward manual calculations

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

B.2. Proof of Lemma 6
Proof. Observe that C = M + ( ˆX − ˆX1) + µI. Thus, by our assumption on µ it follows that

M + 2µI (cid:23) C (cid:23) M.

(27)

Since ˜Fλ,w(y) is twice differentiable, in order to bound its smoothness and strong-convexity parameters, it sufﬁces to
upper bound the largest eigenvalue and lower bound the smallest eigenvalue of its Hessian, respectively.
The Hessian of ˜Fλ,w(y) is given by ∇2 ˜Fλ,w(y) = C−1/2MC−1/2.
From Eq. (27) it follows that we can write M = C − ∆ where ∆ (cid:23) 0.
Thus we have that

λ1(C−1/2MC−1/2) = λ1(C−1/2(C − ∆)C−1/2) ≤ λ1(I) = 1,

where the inequality follows since C−1/2∆C−1/2 is positive semideﬁnite.
Since M, C are invertible and positive deﬁnite, Eq. (27) implies that

M−1 (cid:23) C−1 (cid:23) (M + 2µI)−1.

Thus we have that

λd(C−1/2MC−1/2) = λd(M1/2C−1/2C−1/2MC−1/2C1/2M−1/2) = λd(M1/2C−1M1/2)

≥ λd(M1/2(M + 2µI)−1M1/2) = min
i∈[d]

{

λi(M)

λi(M) + 2µ

}

=

λd(M)

λd(M) + 2µ

=

λ − ˆλ1

(λ − ˆλ1) + 2µ

,

(28)

(29)

(30)

where the ﬁrst equality follows from matrix similarity and the fact that M, C are invertible, and the ﬁrst inequality follows
from Eq. (29).
To prove the second part of the lemma we observe that

(cid:107)˜z − M−1w(cid:107) = (cid:107)C−1/2 ˜y − C−1/2C1/2M−1w(cid:107) ≤ (cid:107)C−1/2(cid:107) · (cid:107)˜y − C1/2M−1w(cid:107)

1(cid:113)

≤

λ − λ1( ˆX)

(cid:107)˜y − C1/2M−1w(cid:107),

where the second inequality follows from Eq. (29).
Finally, the last part of the lemma follows from a direct application of Theorem 1 to upper bound (cid:107)X − ˆX1(cid:107).

B.3. Proof of Lemma 7
Proof. Let z∗ := (λI− ˆX)−1w, y∗ := C1/2(λI− ˆX)−1w, and recall that z∗ and y∗ are the global minimizers of Fλ,w(z)
and ˜Fλ,w(y), respectively. Using the results of Lemma 6 we have that

(cid:107)˜z − z∗(cid:107) ≤ (λ − ˆλ1)−1/2(cid:107)˜y − y∗(cid:107) ≤ (λ − ˆλ1)−1/2

2

1 +

(cid:115)

(cid:18)

(cid:19)

2µ
λ − ˆλ1

(cid:48),

where the second inequality follows from the strong-convexity of ˜Fλ,w(y). Thus, it sufﬁces to set (cid:48) as stated in the lemma
in order to obtain the approximation guarantee for ˜z.

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

To upper-bound the total number of communication rounds required to obtain ˜y with the guarantee prescribed in the lemma,
we note that both the conjugate gradient method and Nesterov’s accelerated gradient method require

(cid:32)(cid:114)

O

ln ((cid:107)y∗(cid:107)/(cid:48))

β
α

(cid:33)

(31)

calls to the ﬁrst-order oracle of ˜Fλ,w(y) to obtain ˜y satisfying ˜Fλ,w(˜y) − miny∈Rd ˜Fλ,w(y) ≤ (cid:48), where α and β are
the strong-convexity and smoothness parameters of ˜Fλ,w, respectively, and assuming w.l.o.g.
that the initial iterate is
y0 = (cid:126)0. Thus, by our construction of a distributed ﬁrst-order oracle given in Algorithm 2, we have that the total number
of communication rounds is upper bounded by (31). The lemma now follows from noticing that by Lemma 6 we have that
β/α = 1 + 2µ
λ−ˆλ1

and that
(cid:107)y∗(cid:107) = (cid:107)C1/2(λI − ˆX1)w(cid:107) ≤ λ1(C1/2)(λ − ˆλ1)−1(cid:107)w(cid:107) = O

(cid:16)(cid:107)w(cid:107)/(λ − ˆλ1)

(cid:17)

.

B.4. Proof of Theorem 6
Proof. Under our assumption that mn = Ω(δ−2 ln(d/p)), the following three events all hold with probability at least 1− p
(each of which holds w.p. at least 1 − p/3):
1. the output wf satisﬁes (w(cid:62)
2. ˆδ = Θ(δ) (by applying Theorem 1)
3. (cid:107) ˆX − ˆX1(cid:107) ≤ µ, where µ is as prescribed in the Theorem (by applying Theorem 1)

f ˆv1)2 ≥ 1 −  (holds w.p. 1 − p/3 by applying Lemma 5 with our choice of parameters)

The approximation guarantee of wf follows directly from Lemma 5. It thus remains to upper-bound the number of matrix-
vector products. Thus, combining Lemmas 5 and 7 we have that when using either the conjugate gradient method or
Nesterov’s accelerated method to approximately solve the linear systems in Algorithm 1, as prescribed in Lemma 7, the
total number of distributed matrix-vector products with ˆX is:

1 +

2µ
δ

ln δ−1 ln

(cid:32)(cid:114)
(cid:18)
(cid:18)

ln

(cid:32)
(cid:32)(cid:114)
(cid:32)(cid:114)

(cid:18) d

(cid:19)

p

·

1 +

1 +

2µ
δ

2µ
δ

O

O

O

ln δ−1 ln2

ln δ−1 ln2

(cid:18)
(cid:18) d
(cid:18) d

p

p

(cid:19)
(cid:19)

+ ln

p

(cid:18) d
(cid:18) d
(cid:18) d

p

(cid:19)
(cid:19)(cid:18)
(cid:19)

(cid:19)(cid:19)(cid:33)(cid:33)
(cid:18) (1 + 2µ/δ)
(cid:18) 1
(cid:19)
(cid:18) (1 + 2µ/δ)
(cid:19)
(cid:18) (1 + 2µ/δ)
(cid:18) d

+ ln

δ˜

=

δ

˜

ln

ln

p

δ

+ ln2

p

+ ln

+ ln

(cid:19)(cid:19)(cid:19)(cid:33)
(cid:18) 1
(cid:19)

ln

δ

=

(cid:19)(cid:19)(cid:33)

,

where the ﬁrst term in the O(·) in the ﬁrst row accounts for the total number of instances of Fλ,w(z) needs to be solved,
given by the bound in Lemma 5, and the second term in the ﬁrst row accounts for the communication-complexity of solving
each such instance according to Lemma 7. Additionally, we have used Lemma 5 to lower bound λ − ˆλ1 = Ω(ˆδ), and ˜()
is as prescribed in Algorithm 1. Finally, we have upper-bounded ln((cid:107)w(cid:107)), in all instances of Fλ,w(z) solved throughout
the run of the algorithm, by noticing that in all of them it holds that

(cid:16)

(cid:16)

λ(s) − ˆλ1)− max{m1,m2}(cid:17)(cid:17)

ln((cid:107)w(cid:107)) = O

ln

(cid:18)

= O

ln δ−1 ln

(cid:18) d

(cid:19)(cid:19)

p

,

where m1, m2 are as prescribed in Algorithm 1, and we have used Lemma 5 again to lower bound λ(s) − ˆλ1 = Ω(δ).

Finally, using Lemma 6, we can set µ =
by

. Thus, the overall number of communication rounds is upper-bound

(cid:115)(cid:112)ln(d/p)

√

δ

n

O

√
ln(3d/p)

n

(cid:18) d

(cid:19)

ln

p2

ln

(cid:32)(cid:112)ln(d/p)

(cid:33)

√

δ2

n

(cid:18) d

(cid:19)

p2

(cid:19)(cid:33) .

(cid:18) 1

δ

+ ln2

ln

√

4

(cid:32)

Communication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

C. Additional Experimental Results

Figure 2. Estimation error vs. the per-machine sample size n for uniform sampling-based distribution.

D. Proof of the Davis-Kahan sinθ Theorem
We prove Theorem 2 in greater generality. In particular, Theorem 2 follows from setting k = 1 in the next theorem.
Theorem 7 (Davis-Kahan sinθ theorem). Let X, Y be symmetric real d × d matrices and ﬁx k ∈ [d]. Let VX and VY
denote d×k matrix whose columns are the top k eigenvectors of X and the matrix whose columns are the top k eigenvectors
of Y, respectively. Also, suppose that δk(X) := λk(X) − λk+1(X) > 0. Then it holds that

(cid:107)VXV(cid:62)

X − VYV(cid:62)

Y(cid:107)F ≤ 2

(cid:107)X − Y(cid:107)
δk(X)

.

Proof. Throughout the proof we denote the projection matrices:

X, P⊥

PX := VXV(cid:62)

X, PY := VYV(cid:62)
i.e., PX is the projection matrix onto the top k eigenvectors of X and P⊥
eigenvectors, and same goes for PY, P⊥
We can write PY as

Y, P⊥
X is the projection matrix onto the lower d − k
Y. We also let A • B denote the standard inner products between matrices A, B.

Y := I − VYV(cid:62)
Y,

X := I − VXV(cid:62)

PY = PXPYPX + P⊥

XPYPX + PXPYP⊥

X + P⊥

XPYP⊥
X.

XXPX

XX(cid:1) = Tr(cid:0)PYP⊥

(cid:1) = 0,
X • X = Tr (PXPYPXX) + Tr(cid:0)P⊥
XX(cid:1) ≤ Tr (PYPXX) + Tr(cid:0)P⊥
(cid:1) ,

XP⊥

XPY

XX(cid:1)
(cid:1) · λ1(P⊥

XPYP⊥
XPYP⊥

X

XX)

(32)

(33)

(34)

where the second equality follows from the cyclic property of the trace, and the last equality follows since P⊥
0d×d. Using Eq. (32) and (33) we have that

XXPX =

Observe that

X • X = Tr(cid:0)PXPYP⊥

PXPYP⊥

PY • X = PXPYPX • X + P⊥

= Tr (PYPXX) + Tr(cid:0)P⊥
= Tr (PYPXX) + λk+1(X) · Tr(cid:0)P⊥

XPYP⊥
XPYP⊥

0100200300400500600n00.10.20.30.40.50.60.70.80.9avg. errorcentralized ERMavg. of ERMssign-fix avg. of ERMs.projection avg.avg. machine lossCommunication-efﬁcient Algorithms for Distributed Stochastic Principal Component Analysis

where the inequality follows since for any two positive semideﬁnite matrices A, B it holds that Tr(AB) ≤ Tr(A) · λ1(B)
and the fact that P⊥
XX) = λk+1(X). It further holds that

XX is positive semideﬁnite. The last equality follows since λ1(P⊥

PY • Y ≥ PX • Y = Tr(PXX) + PX • (Y − X).

Subtracting Eq. (35) from Eq. (34) we have that

Tr (PYPXX) + λk+1(X) · Tr(cid:0)P⊥

XPY

(cid:1) − Tr(PXX) − PX • (Y − X) ≥ PY • X − PY • Y.

Rearranging we have that

Tr ((I − PY)PXX) − λk+1(X) · Tr(cid:0)P⊥

XPY

(cid:1) ≤ (Y − X) • (PY − PX)

≤ (cid:107)X − Y(cid:107) · (cid:107)PX − PY(cid:107)F .

It holds that

Tr ((I − PY)PXX) = Tr (PX(I − PY)PXPXX)

≥ Tr (PX(I − PY)PX) · λk(PXX)
= Tr (PX − PYPX)) · λk(X)
= (k − PX • PY) · λk(X)

=

λk(X)

2

(cid:107)PX − PY(cid:107)2
F .

Furthermore, it holds that

Tr(cid:0)P⊥

XPY

(cid:1) = Tr ((I − PX)PY) = k − PX • PY =

(cid:107)PX − PY(cid:107)2
F .

1
2

Plugging Eq. (37) and (38) into Eq. (36), we have that

(cid:107)PX − PY(cid:107)2

F · (λk(X) − λk+1(X)) ≤ (cid:107)X − Y(cid:107) · (cid:107)PX − PY(cid:107)F ,

1
2

which completes the proof.

(35)

(36)

(37)

(38)

(39)

