Appendices

Minimax Regret Bounds for Reinforcement Learning

We begin by introducing some notation in Sect. B and Sect. A. We then provide the full analysis of UCBVI in Sect. C.

A. Table of Notation

Symbol
S
A
πk
P
R
S
A
H
T and Tk
K
Nk(x, a)
V ∗
T
Vk,h
Qk,h
b
L
Nk(x, a, y)
N(cid:48)
k,h(x, a)
N(cid:48)
k,h(x)

(cid:98)Pk(y|x, a)
(cid:98)Vk,h(x, a)
(cid:98)V∗

V∗

h

Mt
εk,h and ¯εk,h
c1(v, n), c2(p, n) and c3(n)
Ck,h
Bk,h
E
Ω
Ht

h(x, a)
k,h(x, a)
Vπ
h(x, a)
b(cid:48)
i,j(x)
[(x, a)]k
[k]typ
[y]k,x,a
Regret(k)
(cid:94)Regret(k)
Regret(k, x, h)
(cid:94)Regret(k, x, h)
∆k,h

(cid:101)∆k,h
(cid:101)∆typ,k,h

Explanation
The state space
The action space
The policy at episode k
The transition distribution
The reward function
Size of state space
Size of action space
The horizon length
The total number of steps and number of steps up to episode k
The total number of episodes
Number of visits to state-action pair (x, a) up to episode k
Optimal value function V ∗
Bellman operator
The estimate of value function at step h of episode k
The estimate of action value function at step h of episode k
The exploration bonus
ln(5SAT /δ)
Number of transitions from x to y upon taking action a up to episode k
Number of visits to state-action pair (x, a) at step h up to episode k
Number of visits to state x at step h up to episode k
The empirical transition distribution from x to y upon taking action a up to episode k
The empirical next-state variance of Vk,h for every (x, a)
The next-state variance of V ∗ for every state-action pair (x, a)
The empirical next-state variance of V ∗
The next-state variance of V π
min
Set of typical state-action pairs
Set of typical episodes
Set of typical next states at every episode k for every (x, a)
The regret after k episodes
The upper-bound regret after k episodes
The regret upon encountering state x at step h after k episodes
The regret upon encountering state x at step h after k episodes
One step regret at step h of episode k
One step upper-bound regret at step h of episode k
One step upper-bound regret at step h of episode k for typical episodes
The martingale operator
Martingale difference terms
The conﬁdence intervals for the value function and transition distribution
Sum of conﬁdence intervals c1 up to step h of episode k
Sum of exploration bonuses b up to step h of episode k
The high probability event under which the concentration inequality holds
The high probability event under which the estimates Vk,h are ucbs
The history of all random events up to time step t

h for every state-action pair (x, a) at episode k

(cid:16) 1002S2H 2AL2

h for every state-action pair (x, a)

, H 2(cid:17)

N(cid:48)

i,j (x)

Minimax Regret Bounds for Reinforcement Learning

B. Notation
Let denote the total number of times that we visit state x while taking action a at step h of all episodes up to episode k by
N(cid:48)
k,h(x, a). We also use the notation N(cid:48)
k,h(x, a) for the total number of visits to state x at time step h

up to episode k. Also deﬁne the empirical next-state variance(cid:98)Vk,h(x, a), the next-state variance of optimal value function
h(x, a) the next-state empirical variance of optimal value function(cid:98)V∗

k,h(x, a) and the next-state variance of V π

k,h(x) =(cid:80)

a∈A N(cid:48)

h as

V∗

(cid:98)Vk,h(x, a)
(cid:98)V∗

V∗
h(x, a)

k,h(x, a)
Vπ

h(x, a)

def= Vary∼(cid:98)Pk(·|x,a)(Vk,h+1(y)),
def= Vary∼P (·|x,a)(V ∗
def= Vary∼(cid:98)Pk(·|x,a)(V ∗
def= Vary∼P (·|x,a)(V π

h (y)),

h (y)),

h (y)).

for every (x, a) ∈ S × A and k ∈ [K] and h ∈ [H]. We further introduce some short-hand notation: we use the lower case
to denote the functions evaluated at the current state-action pair, e.g., we write nk,h for Nk(xk,h, πk(xk,h, h)) and vk,h for
k,h(xk,h, πk(xk,h, h)) for every k ∈ [K] and
Vk,h(xk,h). Let also denote V∗
h ∈ [H]. Also deﬁne b(cid:48)

k,h(xk,h, πk(xk,h, h)) and Vπk
N(cid:48)

(cid:16) 1002S2H 2AL2

for every x ∈ S.

, H 2(cid:17)

k,h = Vπk

k,h = V∗

i,j(x) = min

i,j+1(x)

B.1. “Typical” state-actions and steps

In our analysis we split the episodes into 2 sets: the set of “typical” episodes in which the number of visits to the encoun-
tered state-actions are large and the rest of the episodes. We then prove a tight regret bound for the typical episodes. As the
total count of other episodes is bounded this technique provides us with the desired result. The set of typical state-actions
pairs for every episode k is deﬁned as follows

[(x, a)]k

def= {(x, a) : (x, a) ∈ S × A, Nh(x, a) ≥ H, N(cid:48)

k,h(x) ≥ H}.

Based on the deﬁnition of [(x, a)]typ we deﬁne the set of typical episodes and the set of typical state-dependent episodes as
follow

[k]typ

[k]typ,x

def= {i : i ∈ [k],∀h ∈ [H], (xi,h, πi(xi,h, h)) ∈ [(x, a)]k, i ≥ 250HS2AL},
def= {i : i ∈ [k],∀h ∈ [H], (xi,h, πi(xi,h, h)) ∈ [(x, a)]k, N(cid:48)

k,h(x) ≥ 250HS2AL}.

Also for every (x, a) ∈ S × A the set of typical next states at every episode k is deﬁned as follows

[y]k,x,a

def= {y : y ∈ S, Nk(x, a)P (y|x, a) ≥ 2H 2L}.

Finally let denote [y]k,h = [y]k,xk,h,πk(xxk,h ) for every k ∈ [K] and h ∈ [H].
B.2. Surrogate regrets

surrogate regrets. Let (cid:101)∆k,h(x) def= Vk,h(x) − V πk

Our ultimate goal is to prove bound on the regret Regret(k). However in our analysis we mostly focus on bounding the
h (x) for every x ∈ S, h ∈ [H] and k ∈ [K]. Then the upper-bound regret

(cid:94)Regret deﬁned as follows

(cid:94)Regret(k)

def=

k(cid:88)

i=1

(cid:101)δi,1.

Minimax Regret Bounds for Reinforcement Learning

(cid:94)Regret(k) is useful in our analysis since it provides an upperbound on the true regret Regret(k). So one can bound
(cid:94)Regret(k) as a surrogate for Regret(k).
We also deﬁne the corresponding per state-step regret and upper-bound regret for every state x ∈ X and step h ∈ [H],
respectively, as follows

Regret(k, x, h)

def=

(cid:94)Regret(k, x, h)

def=

k(cid:88)
k(cid:88)

i=1

i=1

I(xi,h = x)δi,h,

I(xi,h = x)(cid:101)δi,h.

B.3. Martingale difference sequences

In our analysis we rely heavily on the theory of martingale sequences to prove bound on the regret incurred due to encoun-
tering a random sequence of states. We now provide some deﬁnitions and notation in that regard.
We deﬁne the following martingale operator for every k ∈ [K], h ∈ [H] and F : S → (cid:60). Also let t = (k − 1)H + h
denote the time stamp at step h of episode k then

MtF def= P πk

h F − F (xk,h+1).

Let Ht be the history of all random events up to (and including) step h of episode k then we have that E(MtF|Ht) = 0.
Thus MtF is a martingale difference w.r.t. Ht. Also let G be a real-value function depends on Ht+s for some integer
s > 0. Then we generalize our deﬁnition of operator Mt as

MtG def= E ( G(Ht+s)|Ht) − G(Ht+s),

where E is over the randomization of the sequence of states generated by the sequence of policies πk, πk+1, . . . . Here also
MtG is a martingale difference w.r.t. Ht.
Let deﬁne ∆typ,k,h : S → (cid:60) as follows for every k ∈ [K] and h ∈ [H] and y ∈ S

(cid:101)∆typ,k,h+1(y)

def=

(cid:115) Ik,h(y)

(cid:101)∆k,h+1(y),

where the function pk,h : S → [0, 1] is deﬁned as pk,h(y) = P πk
for every y ∈ X . We also deﬁne the following martingale differences which we use frequently

nk,hpk,h(y)
h (y|xk,h) and Ik,h(y) writes for Ik,h(y) = I(y ∈ [y]k,h)

def= Mt(cid:101)∆k,h+1,
def= Mt(cid:101)∆typ,k,h+1.

εk,h

¯εk,h

B.4. High probability events

We now introduce the high probability events E and Ωk,h under which the regret is small.

(cid:1). Also for every v > 0, p ∈ [0, 1] and n > 0 let deﬁne the conﬁdence

Let use the shorthand notation L def= ln(cid:0) 5SAT

δ
intervals c1, c2 and c3, respectively, as follow

Minimax Regret Bounds for Reinforcement Learning

(cid:114)
(cid:114)
(cid:114)

c1(v, n)

c2(p, n)

c3(n)

def= 2

def= 2

def= 2

14HL

+

vL
n
3n
p(1 − p)L

,

+

2L
3n

,

n

SL
n

.

Let P be the set of all probability distributions on S. Deﬁne the following conﬁdence set for every k = 1, . . . , K, n > 0
and (x, a) ∈ S × A

P(k, h, n, x, a, y)

def=

(cid:110)(cid:101)P (·|x, a) ∈ P : |((cid:101)P − P )V ∗
(cid:16)
h (x, a)| ≤ min
|(cid:101)P (y|x, a) − P (y|x, a)| ≤ c2 (P (y|x, a), n) ,
(cid:111)
(cid:107)(cid:101)P (·|x, a) − P (·|x, a)(cid:107)1 ≤ c3(n)

.

(cid:16)(cid:98)V∗

(cid:17)(cid:17)

c1 (V∗

h(x, a), n) , c1

k,h(x, a), n

We now deﬁne the random event E(cid:98)P as follows

(cid:110)(cid:98)Pk(y|x, a) ∈ P(k, h, Nk(x, a), x, a, y),∀k ∈ [K],∀h ∈ [H],∀(y, x, a) ∈ S × S × A(cid:111)

.

E(cid:98)P

def=

Let t be a positive integer. Let F = {fs}s∈[t] be a set of real-value functions on Ht+s, for some integer s > 0. We now
deﬁne the following random events for every ¯w > 0 and ¯u > 0 and ¯c > 0:

Eaz(F, ¯u, ¯c)

Efr(F, ¯w, ¯u, ¯c)

def=

def=

(cid:40) t(cid:88)
(cid:40) t(cid:88)

s=1

s=1

(cid:41)

√
Msfs ≤ 2
√
Msfs ≤ 4

t¯u2¯c

,

¯wc +

14¯u¯c

3

(cid:41)

.

We also use the short-hand notation Eaz(F, ¯u) and Efr(F, ¯w, ¯u) for Eaz(F, ¯u, L) and Efr(F, ¯w, ¯u, L), respectively.
Now let deﬁne the following sets of random variables for every k ∈ [K] and h ∈ [H]:

,

def=

def=

def=

def=

,

(cid:110)(cid:101)∆i,j : i ∈ [k], h < j ∈ [H − 1]
(cid:111)
(cid:110)(cid:101)∆typ,i,j : i ∈ [k], h < j ∈ [H]
(cid:111)
(cid:110)(cid:101)∆i,jI(xi,h = x) : i ∈ [k], h < j ∈ [H]
(cid:111)
(cid:110)(cid:101)∆typ,i,jI(xi,h = x) : i ∈ [k], h < j ∈ [H]
 H(cid:88)
 ,
 H(cid:88)
i,j : i ∈ [k], h < j ∈ [H − 1](cid:9) ,
def= (cid:8)b(cid:48)
def= (cid:8)b(cid:48)
I(xi,h = x) : i ∈ [k], h < j ∈ [H](cid:9) .

: i ∈ [k], h < j ∈ [H]

Vπi
j

Vπi
j

j=h+1

j=h+1

def=

def=

i,j

I(xi,h = x) : i ∈ [k], h < j ∈ [H]

,

(cid:111)

,

 ,

F(cid:101)∆,k,h
F(cid:48)(cid:101)∆,k,h
F(cid:101)∆,k,h,x
F(cid:48)(cid:101)∆,k,h,x
GV,k,h

GV,k,h,x

Fb(cid:48),k,h
Fb(cid:48),k,h,x

Minimax Regret Bounds for Reinforcement Learning

We now deﬁne the high probability event E as follows

E

(cid:20)
(cid:92) (cid:92)
def= E(cid:98)P
Eaz(F(cid:101)∆,k,h, H)
(cid:92)Efr(GV,k,h, H 4T, H 3)

(cid:92)Eaz(F(cid:48)(cid:101)∆,k,h
(cid:92)Eaz(GV,k,h,x, H 5N(cid:48)

k∈[K]
h∈[H]
x∈S

√
, 1/

(cid:92)Eaz(F(cid:101)∆,k,h,x, H)

(cid:92)Eaz(F(cid:48)(cid:101)∆,k,x,h

L)

√

L)

, 1/

(cid:92)Eaz(Fb(cid:48),k,h, H 2)

(cid:92)Eaz(Fb(cid:48),k,h,x, H 2)

(cid:21)

.

k,h(x), H 3)

The following lemma shows that the event E holds with high probability:
Lemma 1. Let δ > 0 be a real scalar. Then the event E holds w.p. at least 1 − δ.

Proof. To prove this result we need to show that a set of concentration inequalities with regard to the empirical model (cid:98)Pk

holds simultaneously. For every h ∈ [H] the Bernstein inequality combined with a union bound argument, to take into
account that Nk(x, a) ∈ [T ] is a random number, leads to the following inequality w.p. 1 − δ (see, e.g., Cesa-Bianchi &
Lugosi, 2006; Bubeck & Cesa-Bianchi, 2012, for the statement of the Bernstein inequality and the application of the union
bound in similar cases, respectively.)

(cid:12)(cid:12)(cid:12)(cid:104)

(cid:105)

(P − (cid:98)Pk)V ∗

h

(cid:12)(cid:12)(cid:12) ≤

(x, a)

(cid:115)

h(x, a) ln(cid:0) 2T

δ

(cid:1)

2V∗

Nk(x, a)

2H ln(cid:0) 2T

δ

(cid:1)

3Nk(x, a)

+

,

(9)

where we rely on the fact that V ∗
Bernstein inequality (see, e.g., Maurer & Pontil, 2009), for Nk(x, a) > 1, leads to

h is uniformly bounded by H. Using the same argument but this time with the Empirical

(cid:12)(cid:12)(cid:12)(cid:104)

(cid:105)

(P − (cid:98)Pk)V ∗

h

(x, a)

(cid:115)

k,h(x, a) ln(cid:0) 2T
2(cid:98)V∗

δ

(cid:1)

Nk(x, a)

7H ln(cid:0) 2T

δ

(cid:1)

3Nk(x, a)

.

(10)

+

The Bernstein inequality combined with a union bound argument on Nk(x, a) also implies the following bound w.p. 1 − δ

(cid:12)(cid:12)(cid:12) ≤
(cid:115)

(cid:18) 2T

(cid:19)

2 ln(cid:0) 2T

δ

(cid:1)

,

+

δ

3

|Nk(y, x, a) − Nk(x, a)P (y|x, a)| ≤

which implies the following bound w.p. 1 − δ:

2Nk(x, a)Varz∼P (·|x,a)(1(z = y)) ln

(cid:12)(cid:12)(cid:12)(cid:98)Pk(y|x, a) − P (y|x, a)
(cid:12)(cid:12)(cid:12) ≤

(cid:115)

P (y|x, a)(1 − P (y|x, a)) ln(cid:0) 2T

δ

(cid:1)

Nk(x, a)

2 ln(cid:0) 2T

δ

(cid:1)

3Nk(x, a)

.

+

(11)

A similar result holds on (cid:96)1-normed estimation error of the transition distribution. The result of (Weissman et al., 2003)
combined with a union bound on Nk(x, a) ∈ [T ] implies w.p. 1 − δ

(cid:13)(cid:13)(cid:13)(cid:98)Pk(·|x, a) − P (·|x, a)
(cid:13)(cid:13)(cid:13)1

≤

(cid:115)

(cid:1)

2S ln(cid:0) 2T

δ
Nk(x, a)

.

(12)

We now focus on bounding the sequence of martingales. Let n > 0 be an integer and u, δ > 0 be some real scalars. Let the
sequence of random variables {X1, X2, . . . , Xn} be a sequence of martingale differences w.r.t. to some ﬁltration Fn. Let

Minimax Regret Bounds for Reinforcement Learning

this sequence be uniformly bounded from above and below by u. Then the Azuma’s inequality (see, e.g., Cesa-Bianchi &
Lugosi, 2006) implies that w.p. 1 − δ

n(cid:88)

i=1

Xi ≤

(cid:115)

2nu ln

(cid:18) 1

(cid:19)

.

δ

When the sum of the variances(cid:80)n

(1975) holds w.p. 1 − δ

i=1 Var(Xi|Fi) ≤ w for some w > 0 then the following sharper bound due to Freedman

n(cid:88)

i=1

Xi ≤

(cid:115)

2w ln

(cid:18) 1

(cid:19)

δ

+

2u ln(cid:0) 1

δ

(cid:1)

3

.

(14)

Let k ∈ [K], h ∈ [H] and x ∈ X . Then the inequality of Eq. 13 immediately implies that the following events holds w.p.
1 − δ:

(cid:17)
(cid:16)F(cid:101)∆,k,h, H, ln (1/δ)
(cid:16)F(cid:48)(cid:101)∆,k,h
(cid:0)Fb(cid:48),k,h, H 2, ln (1/δ)(cid:1) .

√
, 1/

L, ln (1/δ)

,

(cid:17)

Eaz
Eaz
Eaz

,

(15)

(16)

(17)
k,h(x) ∈ [T ] (see, e.g., Bubeck et al., 2011, for the full
Also Eq. 13 combined with a union bound argument over all N(cid:48)
description of the application of union bound argument in the case of martigale process with random stopping time) implies
that the following events hold w.p. 1 − δ

(13)

(18)

(19)

(20)

(21)
(22)

(23)

(24)

Similarly the inequality of Eq. 14 leads to the following events hold w.p. 1 − δ

where ¯wk,h and ¯wk,h,x are upper bounds on Wk,h and Wk,h,x, respectively, deﬁned as

Eaz
Eaz
Eaz

,

,

(cid:17)

L, ln (T /δ)

√
, 1/

(cid:16)F(cid:101)∆,k,h,x, H, ln (T /δ)
(cid:17)
(cid:16)F(cid:48)(cid:101)∆,k,h,x
(cid:0)Fb(cid:48),k,h,x, H 2, ln (T /δ)(cid:1) .
(cid:0)GV,k,h, ¯wk,h, , H 3, ln (T /δ)(cid:1) ,
(cid:0)GV,k,h,x, ¯wk,h,x, , H 3, ln (1/δ)(cid:1) ,
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Hi,1
 ,
 H−1(cid:88)

 H−1(cid:88)

k(cid:88)
k(cid:88)

I(xi,h = x)E

Var

Vπ

i,j+1

Vπ

i,j+1

j=h

i=1

i=1

j=h

Efr
Efr

 .

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Hi,1

Wk,h

def=

Wk,h,x

def=

So to establish a value for ¯wk,h and ¯wk,h,x we need to prove bound on Wk,h and Wk,h,x. Here we only prove this bound
for Wk,h as the proof techniques to bound Wk,h,x is identical to the way we bound Wk,h.

Minimax Regret Bounds for Reinforcement Learning

Wk,h ≤ k(cid:88)

E

i=1

 H−1(cid:88)

j=h

2

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Hk

 H−1(cid:88)

j=h

k(cid:88)

i=1

E

≤ H 3

Vπk

i,j+1

 .

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Hk

Vπk

i,j+1

(25)

Now let the sequence {x1, x2, . . . , xH} be the sequence of states encountered by following some policy π throughout an
episode k. Then the recursive application of LTV leads to (see e.g., Munos & Moore, 1999; Lattimore & Hutter, 2012, for
the proof.)

H−1(cid:88)

E

 .

rπ(xj)

 = Var

H−1(cid:88)

Vπ(xj, π(xj, j))

j=h

j=h

By combining Eq. 26 into Eq. 25 we deduce

Wk,h ≤ H 3

k(cid:88)

i=1

Var

 H−1(cid:88)

j=h

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Hk

rk,h

 ≤ H 5k = H 4Tk.

Similarly the following bound holds on Wk,h,x

Wk,h,x ≤ H 5Nk,h(x).

(26)

(27)

(28)

(29)
(30)

Plugging the bounds of Eq. 27 and Eq. 28 in to the bounds of Eq. 21 and Eq. 22 and a union bound over all Nk,h(x) ∈ [T ]
leads to the following events hold w.p. 1 − δ:

(cid:0)GV,k,h, H 4T, H 3, ln (1/δ)(cid:1) ,
(cid:0)GV,k,h,x, H 5Nk,h(x), H 3, ln (T /δ)(cid:1) .

Efr
Efr

Combining the results of Eq. 9, Eq. 10, Eq. 11, Eq. 12, Eq. 15, Eq. 16 Eq. 17, Eq. 18, Eq. 19, Eq. 20, Eq. 29 and Eq. 30
and taking a union bound over these random events as well as all possible k ∈ [K], h ∈ [H] and (s, a) ∈ S ×A proves the
result.

B.4.1. UCB EVENTS
Let k ∈ [K] and h ∈ [H]. Denote the set of steps for which the value functions are obtained before Vk,h as

[k, h]hist = {(i, j) : i ∈ [K], j ∈ [H], i < k ∨ (i = k ∧ j > h)}.

Let Ωk,h = {Vi,j ≥ V ∗
h ,∀(i, j) ∈ [k, h]hist} be the event under which Vi,j prior to Vk,h computation are upper bounds on
the optimal value functions. Using backward induction on h (and standard concentration inequalities) we will prove that
Ωk,h holds under the event E (see Lem. 19).

Minimax Regret Bounds for Reinforcement Learning

B.5. Other useful notation

Here we deﬁne some other notation that we use throughout the proof. We denote the total count of steps up to episode
k ∈ [K] by Tk

def= H(k − 1). We ﬁrst deﬁne c4,k,h, for every h ∈ [H] and k ∈ [K], as follow

c4,k,h =

4H 2SAL

nk,h

.

for every k ∈ [K] , h ∈ [H] and x ∈ [x] we also introduce the following notation which we use later when we sum up the
regret:

Ck,h

def=

Bk,h

def=

Ck,h,x

def=

Bk,h,x

def=

i=1

k(cid:88)
k(cid:88)
k(cid:88)
k(cid:88)

i=1

i=1

i=1

I(i ∈ [k]typ)

I(i ∈ [k]typ)

H−1(cid:88)
H−1(cid:88)

j=h

j=h

c1,i,j,

bi,j,

I(i ∈ [k]typ,x, xk,h = x)

I(i ∈ [k]typ,x, xk,h = x)

H−1(cid:88)
H−1(cid:88)

j=h

j=h

c1,i,j,

bi,j,

where c1,k,h is the shorthand-notation for c1(v∗
, h ∈ [H] and x ∈ S as follows, respectively

k(cid:88)
k(cid:88)

i=1

Uk,h

def= e

Uk,h,x

def= e

j=h

k,h, nk,h). We also deﬁne the upper bound Uk,h and Uk,h,x for every k ∈ [K]
H−1(cid:88)
H−1(cid:88)

(cid:112)
[bi,j + c1,i,j + c4,i,j] + (H + 1)3/2(cid:113)

[bi,j + c1,i,j + c4,i,j] + (H + 1)

N(cid:48)
k,h(x)L,

TkL,

i=1

j=h

C. Proof of the Regret Bounds
Before we start the main analysis we state the following useful lemma that will be used frequently in the analysis:
Lemma 2. let X ∈ R and Y ∈ R be two random variables. Then following bound holds for their variances

Var(X) ≤ 2 [Var(Y ) + Var(X − Y )] .

Proof. The following sequence of inequalities hold

Var(X) = E(X − Y − E(X − Y ) + Y − E(Y ))2 ≤ 2E(X − Y − E(X − Y ))2 + 2E(Y − E(Y ))2.

The result follows from the deﬁnition of variance.

We proceed by proving the following key lemma which shows that proves bound on ∆k,h under the assumption that Vk,h
is UCB w.r.t. V ∗
h .

Lemma 3. Let k ∈ [K] and h ∈ [H]. Let the events E and Ωk,h hold. Then the following bound holds on δk,h and(cid:101)δk,h:

Minimax Regret Bounds for Reinforcement Learning

δk,h ≤(cid:101)δk,h ≤ e

H−1(cid:88)

(cid:104)

i=h

√

εk,i + 2

L¯εk,i + c1,k,i + bk,i + c4,k,i

.

(31)

(cid:105)

Proof. For the ease of exposition we abuse the notation and drop the dependencies on k, e.g., we write x1, π and V1 for

xk,1, πk and Vk,1, respectively. We proceed by bounding(cid:101)δh under the event E at every step 0 < h < H:

h V π

(cid:101)δh = ThVh+1(xh) − T π
h − P π
h )V ∗

= [(cid:98)P π
h+1(xh)
= bh + [((cid:98)P π
h Vh+1](xh) + bh − [P π
h V π
≤ (cid:101)δh+1 + εh + bh + c1,h + [((cid:98)P π
(cid:124)

h+1](xh) + [((cid:98)P π
h − P π

h+1](xh)

h+1)](xh) + [P π

h (Vh+1 − V π

h+1)](xh)

h − P π
h )(Vh+1 − V ∗
(cid:123)(cid:122)
h )(Vh+1 − V ∗

h+1)](xh)
,

(cid:125)

(32)

where the last inequality follows from the fact that under the event E we have that [((cid:98)P π

(a)

h − P π

h )V ∗

h+1](xh) ≤ c1,h. We now

bound (a):

h+1(y))

 ∆h+1(y)

(a) =

h (y|xh))(Vh+1(y) − V ∗

(cid:88)
(I)≤ (cid:88)

y∈S

y∈S
√

≤ 2

L

nh

(cid:115)

((cid:98)P π
h (y|xh) − P π
2
(cid:115)
(cid:88)
(cid:124)
(cid:115)

(cid:123)(cid:122)

ph(y)

y∈S

nh

(b)

ph(y)(1 − ph(y))L

+

4L
3nh

+

4SHL

3nh

,

(cid:101)∆h+1(y)
(cid:125)

(b) =

(cid:88)
(cid:124)

y∈[y]h

(cid:101)∆h+1(y)
(cid:125)

(cid:88)
(cid:124)

y /∈[y]h

+

ph(y)

nh

(cid:123)(cid:122)

(c)

(cid:115)

(cid:101)∆h+1(y)
(cid:125)

.

ph(y)

nh

(cid:123)(cid:122)

(d)

where (I) holds under the event E. We proceed by bounding (b):

(cid:115)

(cid:101)∆h+1(y) = ¯εh +

I(xh+1 ∈ [y]h)(cid:101)δh+1

1

nhph(xh+1)

1

nhph(y)

The term (c) can be bounded as follows

(cid:88)

(c) =

y∈[y]h
≤ ¯εh +

h (y|xh)
P π

(cid:115)
(cid:114) 1
4LH 2(cid:101)δh+1,
(cid:88)

(d) =

y /∈[y]h

(cid:115)

where in the last line we rely on the deﬁnition of [y]h. We now bound (d):

(cid:101)∆h+1(y) ≤ SH

√

4LH 2
nh

.

ph(y)nh

n2
h

(33)

(34)

(35)

Minimax Regret Bounds for Reinforcement Learning

By combining Eq. 34 and Eq. 35 into Eq. 33 we deduce

(b) ≤ SH

√

4LH 2
nh

+

By combining Eq. 36 and Eq. 33 into Eq. C we deduce

(36)

(cid:114) 1
4LH 2(cid:101)δh+1 + ¯εh.
(cid:19)(cid:101)δh+1.
(cid:105)

(cid:18)

1
H

1 +

√

εi + 2

L¯εi + c1,i + c4,i + bi

.

(cid:101)δh ≤ εh + 2

√

L¯εh + bh + c1,h + c4,h +

(cid:101)δh ≤(cid:80)H−1

i=h γi−h

(cid:104)

Let denote γh = (1 + 1/H)h. The previous bound combined with an induction argument implies that

The inequality ln(1 + x) ≤ x for every x > −1 leads to γh ≤ γH ≤ e for every h ∈ [H]. This combined with the
assumption that vh ≥ v∗
Lemma 4. Let k ∈ [k] and h ∈ [H]. Let the events E and Ωk,h hold. Then

h under the event Ωh completes the proof.

k−1(cid:88)

i=1

δi,h ≤ k−1(cid:88)

(cid:101)δi,h ≤ e

k−1(cid:88)

H−1(cid:88)

i=1

i=1

j=h

(cid:104)

√

L¯εi,j + bi,j + c1,i,j + c4,i,j

εi,j + 2

(cid:105)

.

Proof. The proof follows by summing up the bounds of Lem. 3 and taking into acoount the fact if Ωk,h holds then Ωi,j for
all (i, j) ∈ [k, h]hist hold.

To simplify the bound of Lem. 4 we prove bound on sum of the martingales εk,h and ¯εk,h
Lemma 5. Let k ∈ [k] and h ∈ [H]. Let the events E and Ωk,h hold. Then the following bound holds

k(cid:88)
k(cid:88)

i=1

H−1(cid:88)
H−1(cid:88)

j=h

i=1

j=h

εi,j ≤ H(cid:112)(H − h)kL ≤ H
¯εi,j ≤ (cid:112)(H − h)k ≤(cid:112)

Tk.

(cid:112)

TkL,

Also the following bounds holds for every x ∈ X and h ∈ H:

k(cid:88)
k(cid:88)

i=1

i=1

I(xi,h = x)

I(xi,h = x)

H−1(cid:88)
H−1(cid:88)

j=h

j=h

εi,j ≤ H

(cid:113)
¯εi,j ≤ (cid:113)

(H − h)N(cid:48)

k,h(x)L,

(H − h)N(cid:48)

k,h(x).

(37)

(38)

(39)

(40)

Proof. The fact that the event E holds implies that the events Eaz(F(cid:101)∆,k,h, H), Eaz(F(cid:48)(cid:101)∆,k,h
Eaz(F(cid:48)(cid:101)∆,x,k,h
(H − h)k ≤ Tk completes the proof.

, 1√
L

) hold. Under these events the inequalities of the statement hold. This combined with the fact that

) , Eaz(F(cid:101)∆,k,h,x, H) and

, 1√
L

Minimax Regret Bounds for Reinforcement Learning

We now bound the sum of δs in terms of the upper-bound U:
Lemma 6. Let k ∈ [K] and h ∈ [H]. Let the events E and Ωk,h holds. Then the following bounds hold for every h ∈ [H]
x ∈ S

k(cid:88)

δi,h ≤ k(cid:88)
I(xi,h = x)δi,h ≤ k(cid:88)

i=1

i=1

i=1

k(cid:88)

i=1

(cid:101)δi,h ≤ Uk,h ≤ Uk,1,
I(xi,h = x)(cid:101)δi,h ≤ Uk,h,x. ≤ Uk,1,x.

Proof. The proof follows by incorporating the result of Lem. 5 into Lem. 4 and taking into account that for every h ∈ [H]
the term Uk,h (Uk,h,x) is a summation of non-negative terms which are also contained in Uk,1 (U1,h,x).
Lemma 7. Let k ∈ [K] and h ∈ [H]. Let the events E and Ωk,h holds. Then the following bounds hold for every x ∈ S

k(cid:88)

i=1

I(xi,h = x)

H(cid:88)
H(cid:88)

j=h

j=h

k(cid:88)

i=1

H(cid:88)

j=h

δi,j ≤ k(cid:88)
δi,j ≤ k(cid:88)

i=1

(cid:101)δi,j ≤ HUk,1,
H(cid:88)

I(xi,h = x)

i=1

j=h

(cid:101)δi,j ≤ HUk,1,x.

Proof. The proof follows by summing up the bounds of Lem. 6.

We now focus on bounding the terms Ck,h (Ck,h,x) and Bk,h (Bk,h,x) in Lem. 11 and Lem. 12, respectively. Before we
proceed with the proof of Lem. 11 and Lem. 12. we prove the following key result which bounds sum of the variances of
k,h using an LTV argument:
V π
Lemma 8. Let k ∈ [K] and h ∈ [H]. Then under the events E and Ωk,h the following hold for every x ∈ S

k(cid:88)

i=1

I(xi,h = x)

H−1(cid:88)
H−1(cid:88)

j=h

j=h

k(cid:88)

i=1

(cid:112)

(cid:113)

i,j+1 ≤ TkH + 2
Vπ

H 4TkL +

4H 3L

3

,

i,j+1 ≤ N(cid:48)
Vπ

k,h(x)H 2 + 2

H 5N(cid:48)

k,h(x)L +

4H 3L

3

.

Proof. Under E the events Efr(GV,k,h, H 4Tk, H 3) and Efr(GV,k,h,x, H 5Nk,h(x), H 3) hold which then imply:

k(cid:88)

i=1

I(xi,h = x)

H−1(cid:88)
H−1(cid:88)

j=h

j=h

k(cid:88)

i=1

i,j+1 ≤ k(cid:88)
i,j+1 ≤ k(cid:88)

i=1

Vπ

Vπ

 H−1(cid:88)

E

Vπ

i,j+1

j=h

I(xi,h = x)E

i=1

j=h

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Hk,h
 H−1(cid:88)

 + 2
(cid:112)
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Hk,h

i,j+1

Vπ

4H 3L

3

H 4TkL +

 + 2

(cid:113)

,

(41)

H 5N(cid:48)

k,hL +

4H 3L

3

.

(42)

The LTV argument of Eq. 26 then leads to

Minimax Regret Bounds for Reinforcement Learning

k(cid:88)

i=1

E

I(xi,h = x)E

k(cid:88)

i=1

 H−1(cid:88)
 H−1(cid:88)

j=h

j=h

 =
 =

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Hi,h
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)Hi,h

k(cid:88)
k(cid:88)

i=1

i=1

Vπ

i,j+1

Vπ

i,j+1

 H(cid:88)

Var

rπ
i,j

j=h+1

I(xi,h = x)Var

 ≤ KH 2 = T H,
 ≤ N(cid:48)
 H(cid:88)

rπ
i,j

j=h+1

k,h(x)H 2.

Eq. 41 and Eq. 42 combined with Eq. 43 and Eq. 44, respectively, complete the proof.

Lemma 9. Let k ∈ [K] and h ∈ [H]. Then under the events E and Ωk,h the following hold for every x ∈ S

k(cid:88)

i=1

I(xi,h = x)

H−1(cid:88)
H−1(cid:88)

j=h

j=h

k(cid:88)

i=1

(cid:0)V∗
(cid:0)V∗

i,j+1 − Vπ

i,j+1

i,j+1 − Vπ

i,j+1

(cid:1) ≤ 2H 2Uk,h + 4H 2(cid:112)
(cid:1) ≤ 2H 2Uk,h,x + 4H 2(cid:113)

TkL,

HN(cid:48)

k,h(x, a)L.

Proof. We begin by the following sequence of inequalities:

(43)

(44)

(45)

(46)

k(cid:88)

H−1(cid:88)

i=1

j=h

i,j+1 − Vπ
V∗

i,j+1

(I)≤

=

k(cid:88)
k(cid:88)

i=1

i=1

≤ 2H

j+1(y)(V ∗

j+1(y) + V πi

j+1(y))(cid:3)

i,j+1(y)(cid:1)2(cid:105)

Ey∼pi,j

Ey∼pi,j

j=h

H−1(cid:88)
H−1(cid:88)
H−1(cid:88)
k(cid:88)
(cid:124)

j=h

j=h

i=1

(cid:104)(cid:0)V ∗
i,j+1(y)(cid:1)2 −(cid:0)V πi
(cid:2)(V ∗
(cid:0)V ∗
j+1(y) − V πi
(cid:123)(cid:122)

j+1(y) − V πi

(a)

Ey∼pi,j

j+1(y)(cid:1)
(cid:125)
i,j ≥ V π

,

(47)

where (I) is obtained from the deﬁnition of the variance as well as the fact that V ∗
the fact that V πk ≤ V ∗
Using an identical argument we can also prove the following bound for state-dependent difference:

k,h. The last line also follows from

i,j+1 − Vπ
V∗

i,j+1 ≤ 2H

H−1(cid:88)

j=h

I(xi,h = x)

k(cid:88)
(cid:124)

i=1

Ey∼pi,j

(cid:123)(cid:122)

(b)

(cid:0)V ∗

j+1(y)(cid:1)
(cid:125)

,

j+1(y) − V πi

(48)

h ≤ H.
H−1(cid:88)

j=h

I(xi,h = x)

k(cid:88)

i=1

To bound (a) we use the fact that under the event E the event Eaz(F(cid:101)∆,k,h, H) also holds. This combined with the fact that
under the event Ωk,h the inequality δk,h ≤(cid:101)δk,h holds implies that

(a) ≤ k(cid:88)

H−1(cid:88)

(cid:101)δi,j+1 + 2H

(cid:112)

TkL

(cid:112)

i=1

j=h

≤ HU1,h + 2H

TkL,

(49)

Minimax Regret Bounds for Reinforcement Learning

where in the last line we rely on the result of Lem. 7. Similarly we can prove the following bound for (b) under the events

Ωk,h and Eaz(F(cid:101)∆,k,h,x, H):

(b) ≤ k(cid:88)
H−1(cid:88)
≤ HUk,h,x + 2H 1.5(cid:113)

I(xi,h = x)

j=h

i=1

(cid:101)∆i,j+1 + 2H 1.5(cid:113)

N(cid:48)
k,h(x)L

N(cid:48)
k,h(x)L.

The result then follows by incorporating the results of Eq. 49 and Eq. 50 into Eq. 47 and Eq. 48, respectively.

Lemma 10. Let k ∈ [K] and h ∈ [H]. Then under the events E and Ωk,h the following hold for every x ∈ S

k(cid:88)

i=1

I(xi,h = x)

H−1(cid:88)
H−1(cid:88)

j=h

j=h

k(cid:88)

i=1

(cid:98)Vi,j+1 − Vπ
(cid:98)Vi,j+1 − Vπ

i,j+1 ≤ 2H 2Uk,1 + 15H 2S

ATkL,

i,j+1 ≤ 2H 2Uk,h,x + 15H 2S

HAN(cid:48)

k,h(x)L.

(cid:112)
(cid:113)

(50)

(51)

(52)

Proof. Here we only prove the bound on Eq. 51. The proof for the bound of Eq. 52 can be done in a very similar manner,
as it is shown in the previous lemmas (the only difference is that HN(cid:48)
k,h(x) and Uk,h,x replace Tk and Uk,1, respectively).
The following sequence of inequalities hold:

k(cid:88)

H−1(cid:88)

i=1

j=h

(cid:98)Vi,j+1 − Vπ

i,j+1

(I)≤

where (I) holds due to the fact that under Ωk,h, Vi,j ≥ V ∗
We now bound (a):

and (II) holds under the event E.

+

i=1

j=h

i=1

j=h

H−1(cid:88)
k(cid:88)
(cid:0)V πi
j+1(y)(cid:1)2
Ey∼(cid:98)pi,j (Vi,j+1(y))2 − Ey∼pi,j
k(cid:88)
H−1(cid:88)
(cid:0)Ey∼pi,j V ∗
j+1(y)(cid:1)2
j+1(y)(cid:1)2 −(cid:0)Ey∼(cid:98)pi,j V ∗
Ey∼(cid:98)pi,j (Vi,j+1(y))2 − H−1(cid:88)
H−1(cid:88)
(II)≤ k(cid:88)
(cid:124)
H−1(cid:88)
k(cid:88)
(Vi,j+1(y))2 −(cid:0)V πi
(cid:123)(cid:122)
(cid:124)
H−1(cid:88)
k(cid:88)
(cid:124)

j+1(y)(cid:1)2(cid:105)
(cid:125)

Ey∼pi,j

L
nk,h

(cid:115)

(cid:123)(cid:122)

(cid:104)

(cid:125)

i=1

j=h

i=1

j=h

i=1

j=h

+

+

j=1

(b)

,

(a)

4H 2

(cid:123)(cid:122)
j ≥ V πi

(c)

j

Ey∼pi,j (Vi,j+1(y))2

(cid:125)

(53)

Minimax Regret Bounds for Reinforcement Learning

(a)

(I)≤

k(cid:88)

i=1

(cid:115)

SL
nk,h

2H 2

H−1(cid:88)
(cid:112)

j=h

(II)≤ 3H 2S

ATkL,

where (I) holds under the event E and (II) holds due to the pigeon-hole argument (see, e.g., Jaksch et al., 2010, for the
proof).
Using an identical analysis to the one in Lem. 10 and taking into account that Vi,j ≥ V ∗
can bound (b)

 k(cid:88)

H−1(cid:88)

(cid:101)δi,j+1 + 2H

(cid:112)

TkL)

 ≤ 2H 2(cid:16)

(I)≤ 2H

(b)

i=1

j=h

j under the event Ωk,h and E we
(cid:112)

(cid:17)

Uk,1 + 2

TkL

,

where (I) holds since under the event E the event Eaz(F(cid:101)∆,k,h, H) holds. Another application of pigeon-hole principle

SATkL on (c). We then combine this with the bounds on (a) and (b) to bound Eq. 53, which

√

leads to a bound of 6H 2
proves the result.

We now bound Ck,h and Ck,h,x:
Lemma 11. Let k ∈ [K] and h ∈ [H]. Then under the events E and Ωk,h the following hold for every x ∈ S

(cid:112)
(cid:113)

Ck,h ≤ 4
Ck,h,x ≤ 4

(cid:113)

(cid:113)

HSATk + 4

H 2Uk,1SAL2,

H 2SANk,h(x) + 4

H 2Uk,h,xSAL2.

Proof. Here we only prove the bound on Eq. 54. The proof for the bound of Eq. 55 can be done in a very similar manner,
as it is shown in the previous lemmas (the only difference is that HN(cid:48)
k,h(x) and Uk,h,x replace Tk and Uk,1, respectively).
The Cauchy–Schwarz inequality leads to the following sequence of inequalities:

(54)

(55)

(56)

(57)


H−1(cid:88)

j=h

i,j+1L
ni,j

+

4HL
3ni,j

I(i ∈ [k]typ)

(cid:123)(cid:122)

(b)

1
ni,j

(cid:125)

k(cid:88)

i=1

Ck,h =

√

≤ 2

L

I(j ∈ [k]typ)

(cid:118)(cid:117)(cid:117)(cid:117)(cid:117)(cid:117)(cid:116) k(cid:88)
(cid:124)

i=1

H−1(cid:88)
(cid:123)(cid:122)

j=h

(a)

2

j=h

i,j+1

V∗

H−1(cid:88)
(cid:115)V∗
(cid:118)(cid:117)(cid:117)(cid:117)(cid:117)(cid:117)(cid:116) k(cid:88)
(cid:124)
(cid:125)
H−1(cid:88)
(cid:123)(cid:122)

k(cid:88)
(cid:124)

Vπ

j=h

i=1

i=1

(c)

i,j+1

(a) =

H−1(cid:88)

j=h

k(cid:88)
(cid:124)

i=1

+

(cid:125)

i,j+1 − Vπ
V∗
(cid:123)(cid:122)

(d)

i,j+1

.

(cid:125)

We now prove bounds on (a) and (b) respectively

k(cid:88)

i=1

+

I(i ∈ [k]typ)

H−1(cid:88)

h=j

4HL
3ni,j

(c) and (d) can be bounded under the events E and Ωk,h using the results of Lem. 8 and Lem.9. We then deduce

Minimax Regret Bounds for Reinforcement Learning

(a) ≤ HTk + 2H 2Uk,1 + 6H 2(cid:112)

≤ 2HTk + 2H 2Uk,1,

TkL +

4H 2L

3

where the last line follows by the fact that for the typical episodes Tk ≥ 250H 2S2AL2. Thus if Tk ≤ H 2L the term Ck,h
trivially equals to 0 otherwise the higher order terms are bounded by O(HTk).
We now bound (b) using a pigeon-hole argument

(cid:88)

Nk(x,a)(cid:88)

(x,a)∈S×A

n=1

(b) ≤ 2

T(cid:88)

n=1

1
n

≤ 2SA

1
n

≤ 2SA ln(3T ).

Plugging the bound on (a) and (b) into Eq. 56 and taking in to account that for the typical episodes [k]typ we have that
T ≥ H 2L completes the proof.

We now bound Bk,h:
Lemma 12. Let k ∈ [K] and h ∈ [H]. Let the bonus is deﬁned according to Algo. 4. Then under the events E and Ωk,h
the following hold for every x ∈ S,

(cid:112)
(cid:113)

Bk,h ≤ 11L
Bk,h,x ≤ 11L

(cid:113)

TkHSA + 12
N(cid:48)
k,h(x)HSA + 12

(cid:113)

H 2SAL2Uk,1 + 570H 2S2AL2,

H 2SAL2Uk,h,x + 570H 2S2AL2,

(58)

(59)

Proof. Here we only prove the bound on Eq. 58. The proof for the bound of Eq. 59 can be done in a very similar manner,
as it is shown in the previous lemmas (the only difference is that HN(cid:48)
k,h(x) and Uk,h,x replace Tk and Uk,1, respectively).
We ﬁrst notice that the following holds:

Bk,h ≤ k(cid:88)
(cid:124)

i=1

I(i ∈ [k]typ)

H−1(cid:88)
(cid:123)(cid:122)

j=h

(a)

(cid:115)

8(cid:98)Vi,j+1L
(cid:125)

ni,j

k(cid:88)
(cid:124)

i=1

+L

I(i ∈ [k]typ)

H−1(cid:88)

j=h

(cid:118)(cid:117)(cid:117)(cid:116) 8

ni,j

(cid:32)

(cid:88)

y∈S

(cid:98)pi,j(y) min
(cid:123)(cid:122)

(b)

1002S2H 2AL2

N(cid:48)
i,j+1(y)

, H 2

(cid:33)
(cid:125)

.

h+1 is replaced by(cid:98)Vi,j+1. So in our proof we ﬁrst focus on dealing with this difference.

We ﬁrst note that the bound on Bk,h is similar to the bound on Ck,h. The main difference (beside the difference in H.O.Ts)
is that here V∗
The Cauchy–Schwarz inequality leads to:

√

(a) ≤

8L

(cid:118)(cid:117)(cid:117)(cid:117)(cid:117)(cid:117)(cid:116) k(cid:88)
(cid:124)

i=1

H−1(cid:88)

j=h

(cid:118)(cid:117)(cid:117)(cid:117)(cid:117)(cid:117)(cid:116) k(cid:88)
(cid:98)Vi,j+1(x, a)
(cid:124)
(cid:125)
(cid:123)(cid:122)

i=1

(c)

H−1(cid:88)

j=h

I(k ∈ [k]typ)

(cid:123)(cid:122)

(d)

1
ni,j

,

(cid:125)

The bound on (d) is identical to the corresponding bound in Lem. 11. So we only focus on bounding (c):

Minimax Regret Bounds for Reinforcement Learning

(c) =

k(cid:88)
(cid:124)

i=1

H−1(cid:88)
(cid:123)(cid:122)

j=h

(e)

Vπ

i,j+1

+

(cid:125)

H−1(cid:88)

j=h

k(cid:88)
(cid:124)

i=1

(cid:98)Vi,j+1 − Vπ
(cid:123)(cid:122)

(f )

(cid:125)

.

(60)

i,j+1

(e) and (f ) can be bounded in high probability using the results of Lem. 8 and Lem.10. This implies

(c) ≤ HTk + 3H 2Uk,1 + 15H 2S

≤ 2HTk + 3H 2Uk,1,

ATkL +

4H 2L

3

(cid:112)

where the last line follows by the fact that for the typical episodes Tk ≥ 250H 2S2AL. Thus if Tk ≤ 250H 2S2L then
Bk,h trivially equals to 0 otherwise the higher order terms are bounded by O(HT ). Combining the bound on (b) and (c)
leads to the following bound on (a):

(cid:112)
HSATk + 12HL(cid:112)SAUk,1.

(a) ≤ 8L

To bound (b) we make use of Cauchy-Schwarz inequality again.

(cid:118)(cid:117)(cid:117)(cid:117)(cid:117)(cid:117)(cid:116)8

(b) ≤

k(cid:88)
(cid:124)

i=1

I(i ∈ [k]typ)

H−1(cid:88)
(cid:88)
(cid:123)(cid:122)

y∈S

j=h

(g)

(cid:98)pi,j(y)b(cid:48)

i,j+1(y)

H−1(cid:88)

j=h

k(cid:88)
(cid:124)

i=1

(cid:125)

I(i ∈ [k]typ)

ni,j

(cid:123)(cid:122)

(h)

.

(cid:125)

The term (h) bounded by 2SAL using a pigeon-hole argument (see Lem. 11). We proceed by bounding (g):

(g) ≤ k(cid:88)
(cid:124)

i=1

H−1(cid:88)
((cid:98)pi,j − pi,j)b(cid:48)
(cid:123)(cid:122)

j=h

(i)

H−1(cid:88)

j=h

k(cid:88)
(cid:124)

i=1

+

(cid:125)

i,j+1

(pi,jb(cid:48)V − b(cid:48)

i,j+1(xi,j+1))

(cid:123)(cid:122)

(j)

k(cid:88)
(cid:124)

i=1

+

I(i ∈ [k]typ)

H−1(cid:88)
(cid:123)(cid:122)

j=h

k

b(cid:48)
i,j+1(xi,j+1)

(cid:125)

.

(cid:125)

√
Given that the event E holds the term (i) bounded by 2
ALTk by using the pigeon-hole argument. Under the
event E the event Eaz(Fb(cid:48),k,h, H 2) holds. This implies that the term (j) is also bounded by 2H 2
TkL as it is sum of the
martingale differences. The term (k) is also bounded by 20000H 3S3A3L3 using the pigeon-hole argument. Combining
all these bounds together leads to the following bound on (b)

2H 2S

√

√

(cid:113)

2H 2S2(cid:112)

√

32

(b) ≤

(cid:112)

TkAL3 + 32H 2SA

TkL3 + 320000S4H 4A2L3.

Combining this with the bound on (a) and taking into account the fact that we only bound the Bk,h for the typical episodes,
in which Tk ≥ 250H 2S2AL2, completes the proof.

Lemma 13. Let the bonus is deﬁned according to Algo. 4. Then under the events E and ΩK,1 the following hold

√
Regret(K) ≤ (cid:94)Regret(K) ≤ UK,1 ≤ 15L

HSAT + 16HL(cid:112)SAUk,1 + 820H 2S2A2L + 2H

√

T L.

(61)

Minimax Regret Bounds for Reinforcement Learning

regret due to Bk,h and Ck,h from Lem. 11 and Lem. 12. We also bound the sum(cid:80)K

Proof. We ﬁrst notice that Regret(K) and Regret(K) are bounded by Uk,1 due to Lem.6. To bound Uk,1 we sum up the
h=1 c4,k,h by 2HSAL using
a pigeon hole argument. We also note that Bk,h and Ck,h only account for the regret of typical episodes in which T ≥
H 2S2A2L. The regret of those episodes which do not belong to the typical set [k]typ, can be bounded by O(H 2S2A2L2),
trivially.

(cid:80)H

k=1

The following lemma establishes an explicit bound on the regret:
Lemma 14. Let the bonus is deﬁned according to Algo. 4. Then under the events E and ΩK,1 the following hold

√
Regret(K) ≤ (cid:94)Regret(K) ≤ UK,1 ≤ 30L

HSAT + 2500H 2S2AL2 + 4H

√

T L.

(62)

Proof. The proof follows by solving the bound of Lem. 13 in terms of Uk,1. which only contributes to the additional regret
of O(H 2L2SA).

Lemma 15. Let the bonus is deﬁned according to Algo. 3. Then under the events E and ΩK,1 the following holds

Regret(K) ≤ (cid:94)Regret(K) ≤ UK,1 ≤ 20HL

√

SAT + 250H 2S2AL2.

(63)

(cid:113) SAL

Proof. The proof up to Lem. 11 is identical to the proof of Lem. 14. The main difference is to prove bound on Ck,h and
) for both exploration bonus bk,h and the conﬁdence interval c1,k,h and
Bk,h here we use a loose bound of O(H
then sum these terms using a pigeon-hole argument (The proof is provided in Jaksch et al., 2010) which leads to a bound
of O(H
SAT L) on both BK,1 and CK,1. Plugging these results into the bound of Lem. 7 combined with the regret of
non-typical episodes complete the proof

√

nk,h

Lemma 16. Let the bonus is deﬁned according to Algo. 4. Let k ∈ [K] and h ∈ [H]. Then under the events E and Ωk,h
the following hold for every x ∈ S,

Regret(k, x, h) ≤ (cid:94)Regret(k, x, h) ≤ 30HL

SAN(cid:48)

(cid:113)

k,h(x) + 2500H 2S2AL2 + 4H 1.5(cid:113)

N(cid:48)
k,h(x)L

(cid:113)

≤ 100H 1.5SL

AN(cid:48)

k,h(s).

Proof. The proof is similar to the proof of total regret. Here also we use Lem. 12, Lem. 11 and a pigeon-hole argument to
bound the regrets due to Bk,h, Ck,h and c4,k,h. We then incorporate these terms into Lem.6 to bound the regret in terms of
Uk,h,x. The result follows by solving the bound w.r.t. the upper bound Uk,h,x.
Lemma 17. Let the bonus b is deﬁned according to Algo. 4. Let k ∈ [K] and h ∈ [H]. Then under the events E and Ωk,h
the following hold for every x ∈ S

Vk,h(x) − V ∗

h (x) ≤ 100

Proof. From Lem. 16 we have that

(cid:115)

H 3S2AL2
N(cid:48)
k,h(s)

.

Minimax Regret Bounds for Reinforcement Learning

(cid:113)

100H 1.5SL

AN(cid:48)

k,h(s)

≥ k(cid:88)

I(xi,h = x)(Vi,h(x) − V πi

h (x))

i=1

≥ (Vk,h(x) − V ∗

h (x))

k(cid:88)

I(xi,h = x) = N(cid:48)

k,h(x)(Vk,h(x) − V ∗

h (x)),

where the last inequality holds due to the fact that Vk,h by deﬁnition is monotonically non-increasing in k. The proof then
follows by collecting terms.

i=1

Lemma 18. Let the bonus b is deﬁned according to Algo. 3. Then under the event E the set of events {Ωk,h}k∈[K],h∈H
hold.

Proof. We prove this result by induction. First we notice that for h = H by deﬁnition Vk,h = V ∗
Vk,h ≥ V ∗
holds for h it also holds for h − 1 for every h < H:

h thus the inequality
h trivially holds. Thus to prove this result for h < H we only need to show that if the inequality Vk,h ≥ V ∗

h

Vk,h(x) − V ∗

h (x) = TkVk,h−1(x) − T V ∗(x) ≥ bk(x, π∗
h(x)) + ((cid:98)P π∗
k,h(Vk,h+1 − V ∗

h(x)) + (cid:98)P π∗

≥ bk(x, π∗

= bk(x, π∗

h(x)) + (cid:98)P π∗
h+1)(x) + ((cid:98)P π∗
k,h − P π∗

h )V ∗

k,hVk,h+1(x) − P π∗
h )V ∗

k,h − P π∗
h+1(x),

h Vk,h+1(x)
h+1(x)

h − (cid:98)P π∗

where the last line follows by the induction condition that Vk,h+1 ≥ V ∗
(P π∗

h+1(x) ≤ c1(Nk(x, π∗

h(x))) ≤ bk(x, π∗

k,h)V ∗

h(x)), which completes the proof.

h+1. The fact that the event E hols implies that

Lemma 19. Let the bonus b is deﬁned according to Algo. 4. Then under the event E the set of events {Ωk,h}k∈[K],h∈H
hold.

Proof. We prove this result by induction. We ﬁrst notice that in the case of the ﬁrst episode V1,h = H ≥ V ∗
h .
To prove this result by induction in the case of 1 < k ∈ [K] we need to show that in the case of h ∈ [H − 1] if Ωk,h+1
holds then Ωk,h also holds.
If Ωk,h−1 holds then Vi,j ≥ V ∗

j for every (i, j) ∈ [k, h]hist. We can then invoke the result of Lem. 17 which implies

Vk,h+1(x) − V ∗

√
h+1(x) ≤ 100H 1.5SL

(cid:113)

A

.

N(cid:48)
k,h+1(x)

Using this result which guarantees that Vk,h+1 is close to V ∗

h+1 we prove that Vk,h − V ∗

h ≥ 0, that is the event Ωk,h holds.

If Vk−1,h ≤ Tk,hVi,j+1 the result Vk,h − V ∗
holds. So we only need to consider the case that Tk,hVi,j+1 ≤ Vk−1,h ≤ H in that case we have w

h ≥ 0 holds trivially. Also if Vk−1,h ≥ H the result trivially

Vk,h − V ∗

h = min(Vk−1,h,Tk,hVi,j+1, H) − V ∗
h = Vk−1,h − V ∗

h

Minimax Regret Bounds for Reinforcement Learning

Vk,h(x) − V ∗

h (x) ≥ Tk,hVi,j+1(x) − T V ∗
(I)≥ bk,h(x, π∗(x, h)) + (cid:98)P π∗
= bk,h(x, π∗(x, h)) + ((cid:98)P π∗
(II)≥ bk,h(x, π∗(x, h)) + ((cid:98)P π∗

h+1(x)
h Vi,j+1(x) − P π∗
h − P π∗
h − P π∗

h )V ∗
h )V ∗

h+1(x),

h V ∗

h+1(x) + (cid:98)P π∗

h+1(x)

h (Vi,j+1 − V ∗

h+1)(x)

where in (I) we rely on the fact that πk,h is the greedy policy w.r.t. Vk,h. Thus

bk,h(x, π∗(x, h)) + (cid:98)P π∗

h Vi,j+1(x) ≤ bk,h(x, πk(x, h)) + (cid:98)P πk

h Vi,j+1(x).

Also (II) follows from the induction assumption. Under the event E we have

Vk,h − V ∗

≥

h ≥ bk,h − c1((cid:98)V∗
(cid:115)
8(cid:98)Vk,hL
(cid:123)(cid:122)
(cid:124)
(cid:118)(cid:117)(cid:117)(cid:116)(cid:98)Pk
(cid:104)

Nk

(a)

+

8 min

h, Nk)
− 2

(cid:115)(cid:98)V∗
(cid:16) 1002H 3S2A2L2

− 14L
3Nk

hL
Nk

(cid:125)

k,h+1

N(cid:48)
Nk

, H 2(cid:17)(cid:105)

+

14L
3Nk

.

We now prove a lower bound on (a):

4(cid:98)V∗
k,h − 8(cid:98)Vk,h

(a) ≥

(cid:115)
−
k,h in terms of(cid:98)Vk,h from above:
(I)≤ 2(cid:98)Vk,h + 2Vary∼(cid:98)Pk

Nk

0

(Vk,h+1(y) − V ∗

We proceed by bounding(cid:98)V∗

(cid:98)V∗

k,h

(cid:98)Vk,h ≤ V∗,

otherwise.

h+1(y)) ≤ 2(cid:98)Vk,h + 2(cid:98)Pk(Vk,h+1 − V ∗

h+1)2

,

(cid:124)

(cid:123)(cid:122)

(b)

(cid:125)

where (I) is an application of Lem. 2. We now bound (b). Combining this result with the result of Eq. 64 leads to the
following bound on (a)

(cid:104)

(cid:118)(cid:117)(cid:117)(cid:116) 8(cid:98)Pk

−

0

(a) ≥

(cid:16) 1002H 3S2AL2

, H 2(cid:17)(cid:105)

min

k,h+1

N(cid:48)
Nk

(cid:98)Vk,h ≤(cid:98)V∗,

otherwise,

where the last inequality holds under the event E. The proof is completed by plugging (a) and (b) into Eq. 64 which proves
that Vk,h ≥ V ∗

h thus the event Ωk,hholds.

