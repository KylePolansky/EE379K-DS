Maximum Selection and Ranking under Noisy Comparisons

Moein Falahatgar 1 Alon Orlitsky 1 Venkatadheeraj Pichapati 1 Ananda Theertha Suresh 2

Abstract

We consider (, δ)-PAC maximum-selection and
ranking using pairwise comparisons for general
probabilistic models whose comparison proba-
bilities satisfy strong stochastic transitivity and
stochastic triangle inequality. Modifying the
popular knockout
tournament, we propose a
simple maximum-selection algorithm that uses

(cid:0)1 + log 1
(cid:1)(cid:1) comparisons, optimal up to a
O(cid:0) n
O(cid:0) n
2 log n(log log n)3(cid:1) comparisons for δ = 1

constant factor. We then derive a general frame-
work that uses noisy binary search to speed up
many ranking algorithms, and combine it with
merge sort to obtain a ranking algorithm that uses
n,

optimal up to a (log log n)3 factor.

2

δ

1. Introduction
1.1. Background

Maximum selection and sorting using pairwise compar-
isons are computer-science staples taught in most introduc-
tory classes and used in many applications. In fact, sorting,
also known as ranking, was once claimed to utilize 25% of
all computer cycles, e.g., (Mukherjee, 2011).
In many applications, the pairwise comparisons produce
only random outcomes. In sports, tournaments rank teams
based on pairwise matches whose outcomes are probabilis-
tic in nature. For example, Microsoft’s TrueSkill (Herbrich
et al., 2006) software matches and ranks thousands of Xbox
gamers based on individual game results. And in online ad-
vertising, out of a myriad of possible ads, each web page
may display only a few, and a user will typically select at
most one. Based on these random comparisons, ad compa-
nies such as Google, Microsoft, or Yahoo, rank the ads’ ap-
peal (Radlinski & Joachims, 2007; Radlinski et al., 2008).
These and related applications have brought about a resur-

1University of California, San Diego 2Google Research.
Venkatadheeraj Pichapati <dheera-

to:

Correspondence
jpv7@ucsd.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017
by the author(s).

gence of interest in maximum selection and ranking us-
ing noisy comparisons.
Several probabilistic models
were considered,
including the popular Bradley-Terry-
Luce (Bradley & Terry, 1952) and its Plackett-Luce (PL)
generalization (Plackett, 1975; Luce, 2005). Yet even for
such speciﬁc models, the number of pairwise comparisons
needed, or sample complexity, of maximum selection and
ranking was known only to within a log n factor. We con-
sider a signiﬁcantly broader class of models and yet pro-
pose algorithms that are optimal up to a constant factor for
maximum selection and up to (log log n)3 for ranking.

1.2. Notation

Noiseless comparison assumes an unknown underlying
ranking r(1), . . . ,r(n) of the elements in {1, . . . ,n} such
that if two elements are compared, the higher-ranked one
is selected. Similarly for noisy comparisons, we assume
an unknown ranking of the elements, but now if two el-
ements i and j are compared, i is chosen with some un-
known probability p(i, j) and j is chosen with probabil-
ity p(j, i) = 1 − p(i, j), where if i is higher-ranked, then
p(i, j) ≥ 1
2. Repeated comparisons are independent of
each other.
Let ˜p(i, j) = p(i, j) − 1
2 reﬂect the additional probability
by which i is preferable to j. Note that ˜p(j, i) = −˜p(i, j)
and ˜p(i, j) ≥ 0 if r(i) > r(j). |˜p(i, j)| can also be seen as
a measure of dissimilarity between i and j. Following (Yue
& Joachims, 2011), we assume that two natural proper-
ties, satisﬁed for example by the PL model, hold whenever
r(i) > r(j) > r(k): Strong Stochastic Transitivity (SST),
˜p(i, k) ≥ max(˜p(i, j), ˜p(j, k)), and Stochastic Triangle In-
equality (STI), ˜p(i, k) ≤ ˜p(i, j) + ˜p(j, k).
Two types of algorithms have been proposed for maxi-
mum selection and ranking under noisy comparisons: non-
adaptive or ofﬂine (Rajkumar & Agarwal, 2014; Negahban
et al., 2012; 2016; Jang et al., 2016) where the compar-
ison pairs are chosen in advance, and adaptive or online
where the comparison pairs are selected sequentially based
on previous comparison results. We focus on the latter.
We specify the desired output via the (, δ)-PAC
paradigm (Yue & Joachims, 2011; Sz¨or´enyi et al., 2015)
that requires the output to likely closely approximate the
intended outcome. Speciﬁcally, given , δ > 0, with prob-

Maximum Selection and Ranking under Noisy Comparisons

δ

δ

2

complexity.

2 (log n)3 log n

(cid:0)1 + log 1

• Maximum-selection algorithm with sample complex-

ity O(cid:0) n
(cid:1)(cid:1), optimal up to a constant factor.
(cid:1) sample
• Ranking algorithm with O(cid:0) n
rithm with sample complexity O(cid:0) n
(cid:1)
complexity O(cid:0) n
2 log n(log log n)x(cid:1).
2 log n(log log n)3(cid:1) for δ = 1
sample complexity O(cid:0) n
• An Ω(cid:0) n
(cid:1) lower bound on the sample complex-

• General framework that converts any ranking algo-
2 (log n)x log n
n has sample

• Using the above framework, a ranking algorithm with
n.

into a ranking algorithm that for δ ≥ 1

δ

2 log n

δ

ability ≥ 1 − δ, maximum selection must output an -
2 − .
maximum element i such that for all j, p(i, j) ≥ 1
Similarly, with probability ≥ 1 − δ, the ranking algorithm
must output an -ranking r(cid:48)(1), . . . ,r(cid:48)(n) such that when-
ever r(cid:48)(i) > r(cid:48)(j), p(i, j) ≥ 1

2 − .

1.3. Outline

In Section 2 we review past work and summarize our
contributions.
In Section 3 we describe and analyze our
maximum-selection algorithm.
In Section 4 we propose
and evaluate the ranking algorithm. In Section 5 we exper-
imentally compare our algorithms with existing ones. In
Section 6 we mention some future directions.

2. Old and new results
2.1. Related work

Several researchers studied algorithms that with probabil-
ity 1 − δ ﬁnd the exact maximum and ranking.
(Feige
et al., 1994) considered a simple model where the ele-
ments are ranked, and ˜p(i, j) =  whenever r(i) > r(j).
(Busa-Fekete et al., 2014a) considered comparison prob-
abilities p(i, j) satisfying the Mallows model (Mallows,
1957). And (Urvoy et al., 2013; Busa-Fekete et al., 2014b;
Heckel et al., 2016) considered general comparison proba-
bilities, without an underlying ranking assumption, and de-
rived rankings based on Copeland- and Borda-counts, and
random-walk procedures. As expected, when the compar-
ison probabilities approach half, the above algorithms re-
quire arbitrarily many comparisons.
To achieve ﬁnite complexity even with near-half compar-
ison probabilities, researchers adopted the PAC paradigm.
For the PAC model with SST and STI constraints, (Yue &
Joachims, 2011) derived a maximum-selection algorithm

with sample complexity O(cid:0) n

(cid:1) and used it to bound

the regret of the problem’s dueling-bandits variant. Related
results appeared in (Syrgkanis et al., 2016).
For the PL
model, (Sz¨or´enyi et al., 2015) derived a PAC ranking algo-
rithm with sample complexity O( n
Deterministic adversarial versions of the problem were
considered by (Ajtai et al., 2015), and by (Acharya et al.,
2014a; 2016) who were motivated by density estima-
tion (Acharya et al., 2014b).

2 log n log n

δ ).

2 log n

δ

2.2. New results

We consider (, δ)-PAC adaptive maximum selection and
ranking using pairwise comparisons under SST and STI
2 or δ ≥ 1 − 1/n for
constraints. Note that when  ≥ 1
maximum selection and δ ≥ 1 − 1/n2 for ranking, any
output is correct. We show for  < 1/4, δ < 1
2 and any n:

ity of any PAC ranking algorithm, matching our algo-
rithm’s sample complexity up to a (log log n)3 factor.

3. Maximum selection
3.1. Algorithm outline

algorithm,

(cid:0)1 + log 1

We propose a simple maximum-selection algorithm based
on Knockout tournaments. Knockout tournaments are used
to ﬁnd a maximum element under non-noisy comparisons.
Knockout tournament of n elements runs in (cid:100)log n(cid:101) rounds
where in each round it randomly pairs the remaining ele-
ments and proceeds the winners to next round.
Our

O(cid:0) n
O(cid:0) n
tion algorithm requires Ω(cid:0) n

(cid:1)(cid:1)
(cid:1) comparisons and O(n2) memory to ﬁnd an
(cid:1)(cid:1) comparisons,

-maximum. Hence we get log n-factor improvement in
the number of comparisons and also we use linear memory
compared to quadratic memory. From (Zhou & Chen,
2014) it can be inferred that the best PAC maximum selec-

KNOCKOUT
uses
and O(n) memory
(Yue & Joachims, 2011) uses

(cid:0)1 + log 1

given
in
comparisons

to ﬁnd an -maximum.

2 log n

2

δ

δ

hence up to constant factor, KNOCKOUT is optimal.
(Yue & Joachims, 2011; Sz¨or´enyi et al., 2015) eliminate el-
ements one by one until only -maximums are remaining.
Since they potentially need n − 1 eliminations, in order
to appply union bound they had to ensure that each elimi-
nated element is not an -maximum w.p. 1− δ/n, requiring
O(log(n/δ)) comparisons for each eliminated element and
hence a superlinear sample complexity O(n log(n/δ)).
In contrast, KNOCKOUT eliminates elements in log n
rounds. Since in Knockout tournaments, number of ele-
ments decrease exponentially with each round, we afford
to endure more error in the initial rounds and less error
in the latter rounds by repeating comparison between each
pair more times in latter rounds. Speciﬁcally, let bi be the
highest-ranked element (according to the unobserved un-
derlying ranking) at the beginning of round i. KNOCKOUT
2i , ˜p(bi, bi+1) ≤ i by repeating
makes sure that w.p. ≥ 1− δ

2

δ

(cid:17)
log 2i
(cid:1)(cid:1) and by
δ
2i/3 with c = 21/3−1, we make sure
times. Choosing i = c
union bound and STI, w.p. ≥ 1 − δ, ˜p(b1, b(cid:100)log n(cid:101)+1) ≤

comparison between each pair in round i for O(cid:16) 1
that comparison complexity is O(cid:0) n
(cid:0)1 + log 1
(cid:80)(cid:100)log n(cid:101)+1

2
i

2

c

δ

i=1

2i/3 ≤ .

For γ ≥ 1, a relaxed notion of SST, called γ-stochastic
transitivity (Yue & Joachims, 2011), requires that if r(i) >
r(j) > r(k), then max(˜p(i, j), ˜p(j, k)) ≤ γ · ˜p(i, k). Our
results apply to this general notion of γ-stochastic transi-
tivity and the analysis of KNOCKOUT is presented under
com-

this model. KNOCKOUT uses O(cid:16) nγ4

(cid:0)1 + log 1

(cid:1)(cid:17)

2

δ

parisons.
Remark 1.
(Yue & Joachims, 2011) considered a differ-
ent deﬁnition of -maximum as an element i that is at most
 dissimilar to true maximum i.e., for j with r(j) = n,
˜p(j, i) ≤ . Note that this deﬁnition is less restrictive than
ours, hence requires fewer comparisons. Under this deﬁ-
com-
parisons to ﬁnd an -maximum whereas a simple modiﬁca-
com-
parisons sufﬁce. Hence we also get a signiﬁcant improve-
ment in the exponent of γ.

nition, (Yue & Joachims, 2011) used O(cid:16) nγ6
tion of KNOCKOUT shows that O(cid:16) nγ2

(cid:0)1 + log 1

(cid:17)
(cid:1)(cid:17)

2 log n

2

δ

δ

To simplify the analysis, we assume that n is a power of
2, otherwise we can add 2(cid:100)log n(cid:101) − n dummy elements that
lose to every original element with probability 1. Note that
all -maximums will still be from the original set.

3.2. Algorithm

We start with a subroutine COMPARE that compares two
elements. It compares two elements i, j and maintains em-
pirical probability ˆpi, a proxy for p(i, j). It also maintains a
conﬁdence value ˆc s.t., w.h.p., ˆpi ∈ (p(i, j)− ˆc, p(i, j)+ ˆc).
COMPARE stops if it is conﬁdent about the winner or if it
reaches its comparison budget m. It outputs the element
with more wins breaking ties randomly.

Algorithm 1 COMPRARE
Input: element i, element j, bias , conﬁdence δ.
Initialize: ˆpi = 1
1. while (| ˆpi − 1

2, ˆc = 1
22 log 2
2| ≤ ˆc −  and r ≤ m)

2, m = 1

δ , r = 0, wi = 0.

(a) Compare i and j. if i wins wi = wi + 1.
2r log 4r2
δ .
(b) r = r + 1, ˆpi = wi

r , ˆc =

(cid:113) 1

if ˆpi ≤ 1

2 Output: j. else Output: i.

We show that COMPARE w.h.p., outputs the correct winner
if the elements are well seperated.

Maximum Selection and Ranking under Noisy Comparisons

Lemma 2. If ˜p(i, j) ≥ , then

P r(COMPARE(i, j, , δ) (cid:54)= i) ≤ δ.

Note that instead of using ﬁxed number of comparisons,
COMPARE stops the comparisons adaptively if it is con-
ﬁdent about the winner. If |˜p(i, j)| (cid:29) , COMPARE stops
δ and hence works
much before comparison budget 1
better in practice.
Now we present the subroutine KNOCKOUT-ROUND that
we use in main algorithm KNOCKOUT.

22 log 2

3.2.1. KNOCKOUT-ROUND

KNOCKOUT-ROUND takes a set S and outputs a set of size
|S|/2. It randomly pairs elements, compares each pair us-
ing COMPARE, and returns the set of winners. We will later
show that maximum element in the output set will be com-
parable to maximum element in the input set.

Algorithm 2 KNOCKOUT-ROUND
Input: Set S, bias , conﬁdence δ.
Initialize: Set O = ∅.
1. Pair elements in S randomly.

2. for every pair (i, j):

Add COMPARE(i, j, , δ) to O.

Output: O

Note that comparisons between each pair can be handled
by a different processor and hence this algorithm can be
easily parallelized.
S can have several maximum elements. Comparison prob-
abilities corresponding to all maximum elements will be
essentially same because of STI. We deﬁne max(S) to be
the maximum element with the least index, namely,

(cid:16)

min{i : ˜p(S(i), S(j)) ≥ 0 ∀j}(cid:17)
(cid:17)(cid:33)

KNOCKOUT-ROUND(S, , δ)

(cid:16)

.

≤ γ.

Lemma 3. KNOCKOUT-ROUND(S, , δ) uses |S|
comparisons and with probability ≥ 1 − δ,

42 log 2

δ

max(S) def= S

(cid:32)

˜p

max(S), max

3.2.2. KNOCKOUT

Now we present the main algorithm KNOCKOUT. KNOCK-
OUT takes an input set S and runs log n rounds of
KNOCKOUT-ROUND halving the size of S at the end of
each round. Recall that KNOCKOUT-ROUND makes sure
that maximum element in the output set is comparable to

Maximum Selection and Ranking under Noisy Comparisons

maximum element in the input set. Using this, KNOCK-
OUT makes sure that the output element is comparable to
maximum element in the input set.
Since the size of S gets halved after each round, KNOCK-
OUT compares each pair more times in the latter rounds.
Hence the bias between maximum element in input set and
maximum element in output set is small in latter rounds.

Algorithm 3 KNOCKOUT
Input: Set S, bias , conﬁdence δ, stochasticity γ.
Initialize: i = 1, S = set of all elements, c = 21/3 − 1.
while |S| > 1

(cid:16)

(cid:17)

.

1. S = KNOCKOUT-ROUND

2. i = i + 1.

S,

c

γ22i/3 , δ

2i

Output: the unique element in S.

Note that KNOCKOUT uses only memory of set S and
hence O(n) memory sufﬁces.
Theorem 4 shows that KNOCKOUT outputs an -maximum
with probability ≥ 1 − δ. It also bounds the number of
comparisons used by the algorithm.
Theorem

uses
KNOCKOUT(S, , δ)
comparisons and with proba-

(cid:0)1 + log 1

O(cid:16) γ4|S|

(cid:1)(cid:17)

4.

2

bility at least 1 − δ, outputs an -maximum.

δ

2

2

(cid:17)

4. Ranking
We propose a ranking algorithm that with probability at
least 1− 1
comparisons and out-
puts an -ranking.

n uses O(cid:16) n log n(log log n)3
(cid:17)
Notice that we use only ˜O(cid:16) n log n
(Sz¨or´enyi et al., 2015) uses O(cid:0)n(log n)2/2(cid:1)

comparisons for δ = 1
n

where as
comparisons even for constant error probability δ. Fur-
thermore (Sz¨or´enyi et al., 2015) provided these guarantees
only under Plackett-Luce model which is more restrictive
compared to ours. Also, their algorithm uses O(n2) mem-
ory compared to O(n) memory requirement of ours.
Our main algorithm BINARY-SEARCH-RANKING assumes
the existence of a ranking algorithm RANK-x that with

probability at least 1 − δ uses O(cid:0) n

(cid:1) com-

2 (log n)x log n

parisons and outputs an -ranking for any δ > 0,  > 0 and
some x > 1. We also present a RANK-x algorithm with
x = 3.
Observe that we need RANK-x algorithm to work for any
model that satisﬁes SST and STI. (Sz¨or´enyi et al., 2015)
showed that their algorithm works for Plackett-Luce model
but not for more general model. So we present a RANK-x

δ

n

n

algorithm that works for general model.
The main algorithm BINARY-SEARCH-RANKING ran-
(log n)x elements (anchors) and rank them
domly selects
using RANK-x . The algorithm has then effectively cre-
ated
(log n)x bins, each between two successively ranked
anchors. Then for each element, the algorithm identiﬁes
the bin it belongs to using a noisy binary search algorithm.
The algorithm then ranks the elements within each bin us-
ing RANK-x .
We ﬁrst present MERGE-RANK, a RANK-3 algorithm.

4.1. Merge Ranking

uses O(cid:16) n(log n)3

(cid:17)

2

log n
δ

We present a simple ranking algorithm MERGE-RANK that
comparisons, O(n) memory and
with probability ≥ 1 − δ outputs an -ranking. Thus
MERGE-RANK is a RANK-x algorithm for x = 3.
Similar to Merge Sort, MERGE-RANK divides the elements
into two sets of equal size, ranks them separately and com-
bines the sorted sets. Due to the noisy nature of com-
parisons, MERGE-RANK compares two elements i, j suf-
ﬁcient times, so that the comparison output is correct with
high probability when |˜p(i, j)| ≥ 
log n. Put differently,
MERGE-RANK is same as the typical Merge Sort, except it
uses COMPARE as the comparison function. Due to lack of
space, MERGE-RANK is presented in Appendix A.
Let’s deﬁne the error of an ordered set S as the maximum
distance between two wrongly ordered items in S, namely,

err(S) def= max

1≤i≤j≤|S| ˜p(S(i), S(j)).



We show that when we merge two ordered sets, the error of
log n more than the
the resulting ordered set will be at most
maximum of errors of individual ordered sets.
Observe that MERGE-RANK is a recursive algorithm and
the error of a singleton set is 0. Two singleton sets each
containing a unique element from the input set merge to
form a set with two elements with an error at most
log n,
then two sets with two elements merge to form a set with
log n and henceforth.
four elements with an error of at most
Thus the error of the output ordered set is bounded by .
Lemma 5 shows that MERGE-RANK can output an -
ranking of S with probability ≥ 1 − δ. It also bounds the
number of comparisons used by the algorithm.
Lemma
δ|S|2

5. MERGE-RANK

log |S| ,

takes

(cid:16)

(cid:17)

S,

3

2



2

log

comparisons and with probability
≥ 1 − δ, outputs an -ranking. Hence, MERGE-RANK is a
RANK-3 algorithm.

O(cid:16)|S|(log |S|)3

(cid:17)

|S|
δ

Now we present our main ranking algorithm.

Maximum Selection and Ranking under Noisy Comparisons

4.2. BINARY-SEARCH-RANKING

We ﬁrst sketch the algorithm outline below. We then pro-
vide a proof outline.

4.2.1. ALGORITHM OUTLINE

n

n

n

Our algorithm is stated in BINARY-SEARCH-RANKING. It
can be summarized in three major parts.
Creating anchors:
(Steps 1 to 3) BINARY-SEARCH-
RANKING ﬁrst selects a set S(cid:48) of
(log n)x random elements
(anchors) and ranks them using RANK-x . At the end of
(log n)x ranked anchors. Equivalently,
this part, there are
(log n)x − 1 bins, each bin between
the algorithm creates
two successively ranked anchors.
Coarse ranking: (Step 4) After forming the bins, the al-
gorithm uses a random walk on a binary search tree, to ﬁnd
which bin each element belongs to. INTERVAL-BINARY-
SEARCH is similar to the noisy binary search algorithm
in (Feige et al., 1994). It builds a binary search tree with the
bins as the leaves and it does a random walk over this tree.
Due to lack of space the algorithm INTERVAL-BINARY-
SEARCH is presented in Appendix B but more intuition is
given later in this section.
Ranking within each bin:
(Step 5) For each bin, we
show that the number of elements far from both anchors
is bounded. The algorithm checks elements inside a bin
whether they are close to any of the bin’s anchors. For
the elements that are close to anchors, the algorithm ranks
them close to the anchor. And for the elements that are
away from both anchors the algorithm ranks them using
RANK-x and outputs the resulting ranking.

n

(cid:106)

(cid:107)

(log n)x

+ 2.

1. Form a set S(cid:48) with

Algorithm 4 BINARY-SEARCH-RANKING
Input: Set S, bias .
Initialize: (cid:48) = /16, (cid:48)(cid:48) = /15, and So = ∅. Sj = ∅,

Cj = ∅ and Bj = ∅, for 1 ≤ j ≤(cid:106)
(cid:107)
2. Rank S(cid:48) using RANK-x(cid:0)S(cid:48), (cid:48), 1
p(a, e) = 0 ∀e ∈ S(cid:83) S(cid:48). Add dummy element b at
the end of S(cid:48) such that p(b, e) = 1 ∀e ∈ S(cid:83) S(cid:48).

3. Add dummy element a at the beginning of S(cid:48) such that

S. Remove these elements from S.

random elements from

(cid:1).

(log n)x

n6

n

4. for e ∈ S:

n

(cid:106)

(log n)x

+ 2:

(a) for e ∈ Sj:

(a) k = INTERVAL-BINARY-SEARCH(S(cid:48), e, (cid:48)(cid:48)).
(b) Insert e in Sk.

(cid:107)
(cid:2) 1
2 + 6(cid:48)(cid:48)(cid:3) , insert e in Cj.
2 − 6(cid:48)(cid:48), 1
1), 10(cid:48)(cid:48)−2 log n) ∈ (cid:2) 1
if
(b) Rank Bj using RANK-x(cid:0)Bj, (cid:48)(cid:48), 1
(cid:1).

i. if COMPARE2(e, S(cid:48)(j), 10(cid:48)(cid:48)−2 log n)

COMPARE2(e, S(cid:48)(j
2 − 6(cid:48)(cid:48), 1

iii. else insert e in Bj.

then insert e in Cj+1.

ii. else

n4

(c) Append S(cid:48)(j), Cj, Bj in order at the end of So.

∈

2 + 6(cid:48)(cid:48)(cid:3),

+

5. for j = 1 to

Output: So

4.2.2. ANALYSIS OF BINARY-SEARCH-RANKING
Creating anchors In Step 1 of the algorithm we select
n/(log n)x random elements. Since these are chosen uni-
formly random, they lie nearly uniformly in the set S. This
intuition is formalized in the next lemma.
Lemma 6. Consider a set S of n elements. If we select
(log n)x elements uniformly randomly from S and build an
ordered set S(cid:48) s.t. ˜p(S(cid:48)(i), S(cid:48)(j)) ≥ 0 ∀i > j , then with
probability ≥ 1 − 1
|{e ∈ S : ˜p(e, S(cid:48)(k)) > , ˜p(S(cid:48)(k+1), e) > }| ≤ 5(log n)x+1.

n4 , for any  > 0 and all k,

n

anchor and a right anchor . We say that an element belongs
to a bin if it wins over the bin’s left anchor with probability
≥ 1
2 and wins over the bin’s right anchor with probabil-
ity ≤ 1
2. Notice that some elements might win over S(cid:48)(1)
with probability < 1
2 and thus not belong to any bin. So in
Step 3, we add a dummy element a at the beginning of S(cid:48)

where a loses to every element in S(cid:83) S(cid:48) with probability
end of S(cid:48) where every element in S(cid:83) S(cid:48) loses to b with

1. For similar reasons we add a dummy element b to the

probability 1.
Coarse Ranking Note that S(cid:48)(i) and S(cid:48)(i + 1) are respec-
tively the left and right anchors of the bin Si.

In Step 2, we use RANK-x to rank S(cid:48). Lemma 7 shows the
guarantee of ranking S(cid:48).
Lemma 7. After Step 2 of
RANKING with probability ≥ 1 − 1

the BINARY-SEARCH-
n6 , S(cid:48) is (cid:48)-ranked.

(log n)x − 1 bins, each be-
At the end of Step 2, we have
tween two successively ranked anchors. Each bin has a left

n

Algorithm 5 COMPARE2
Input: element i, element j, number of comparisons m.

1. Compare i and j for m times and return the fraction

of times i wins over j.

Maximum Selection and Ranking under Noisy Comparisons

1

2 − (cid:48)(cid:48) for some (cid:48)(cid:48) > (cid:48).

Since S(cid:48) is (cid:48)-ranked and the comparisons are noisy, it is
hard to ﬁnd a bin Si for an element e such that p(e, S(cid:48)(i)) ≥
2 and p(S(cid:48)(i + 1), e) ≥ 1
2. We call a bin Si a (cid:48)(cid:48)−nearly
2 − (cid:48)(cid:48) and
correct bin for an element e if p(e, S(cid:48)(i)) ≥ 1
p(S(cid:48)(i + 1), e) ≥ 1
In Step 4, for each element we ﬁnd an (cid:48)(cid:48)-nearly correct bin
using INTERVAL-BINARY-SEARCH . Next we describe an
outline of INTERVAL-BINARY-SEARCH.
INTERVAL-BINARY-SEARCH ﬁrst builds a binary search
tree of intervals (see Appendix B) as follows: the root node
is the entire interval between the ﬁrst and the last elements
in S(cid:48). Each non-leaf node interval I has two children corre-
sponding to the left and right halves of I. The leaves of the
tree are the bins between two successively ranked anchors.
To ﬁnd an (cid:48)(cid:48)-nearly correct bin for an element e, the algo-
rithm starts at the root of the binary search tree and at every
non-leaf node corresponding to interval I, it checks if e be-
longs to I or not by comparing e with I’s left and right
anchors. If e loses to left anchor or wins against the right
anchor, the algorithm backtracks to current node’s parent.
If e wins against I’s left anchor and loses to its right one,
the algorithm checks if e belongs to the left or right child
by comparing e with the middle element of I and moves
accordingly.
When at a leaf node, the algorithm checks if e belongs to
the bin by maintaining a counter. If e wins against the bin’s
left anchor and loses to the bin’s right anchor, it increases
the counter by one or otherwise it decreases the counter by
one. If the counter is less than 0 the algorithm backtracks
to the bin’s parent. By repeating each comparison several
times, the algorithm makes a correct decision with proba-
bility ≥ 19
20.
Note that there could be several (cid:48)(cid:48)-nearly correct bins for
e and even though at each step the algorithm moves in the
direction of one of them, it could end up moving in a loop
and never reaching one of them. We thus run the algorithm
for 30 log n steps and terminate.
If the algorithm is at a leaf node by 30 log n steps and the
counter is more than 10 log n we show that the leaf node bin
is a (cid:48)(cid:48)-nearly correct bin for e and the algorithm outputs
the leaf node. If not, the algorithm puts in a set Q all the
anchors visited so far and orders Q according to S(cid:48).
We select 30 log n steps to ensure that if there is only one
nearly correct bin, then the algorithm outputs that bin w.p.
≥ 1 − 1
n6 . Also we do not want too many steps so as to
bound the size of Q.
By doing a simple binary search in Q using BINARY-
SEARCH (see Appendix B) we ﬁnd an anchor f ∈ Q
such that |˜p(e, f )| ≤ 4(cid:48)(cid:48). Since INTERVAL-BINARY-

2 − 3(cid:48)(cid:48) and 1

SEARCH ran for at most 30 log n steps, Q can have at most
60 log n elements and hence BINARY-SEARCH can search
effectively by repeating each comparison O(log n) times
to maintain high conﬁdence. Next paragraph explains how
BINARY-SEARCH ﬁnds such an element f.
BINARY-SEARCH ﬁrst compares e with the middle el-
ement m of Q for O(log n) times.
If the fraction of
2 − 3(cid:48)(cid:48) and 1
2 + 3(cid:48)(cid:48), then w.h.p.
wins for e is between 1
|˜p(e, m)| ≤ 4(cid:48)(cid:48) and hence BINARY-SEARCH outputs m.
2 − 3(cid:48)(cid:48), then w.h.p.
If the fraction of wins for e is less than 1
˜p(e, m) ≤ −2(cid:48)(cid:48) and hence it eliminates all elements to the
right of m in Q. If the fraction of wins for e is more than
2 + 3(cid:48)(cid:48), then w.h.p. ˜p(e, m) ≥ 2(cid:48)(cid:48) and hence it eliminates
1
all elements to the left of m in Q. It continues this process
until it ﬁnds an element f such that the fraction of wins for
2 + 3(cid:48)(cid:48).
e is between 1
INTERVAL-BINARY-
In next Lemma, we show that
SEARCH achieves to ﬁnd a 5(cid:48)(cid:48)-nearly correct bin for every
element.
Lemma 8. For any element e ∈ S, Step 4 of
BINARY-SEARCH-RANKING places e in bin Sl such that
˜p(e, S(cid:48)(l)) > −5(cid:48)(cid:48) and ˜p(S(cid:48)(l + 1), e) > −5(cid:48)(cid:48) with prob-
ability ≥ 1 − 1
n5 .
Ranking within each bin Once we have identiﬁed the bins,
we rank the elements inside each bin. By Lemma 6, inside
each bin all elements are close to the bin’s anchors except
at most 5(log n)x+1 of them.
The algorithm ﬁnds the elements close to anchors in Step
5a by comparing each element in the bin with the bin’s
anchors. If an element in bin Sj is close to bin’s anchors
S(cid:48)(j) or S(cid:48)(j + 1) , the algorithm moves it to the set Cj
or Cj+1 accordingly and if it is far away from both, the al-
gorithm moves it to the set Bj. The following two lemmas
state that this separating process happens accurately with
high probability. The proofs of these results follow from
the Chernoff bound and hence omitted.
Lemma 9. At the end of Step 5a, for all j, ∀e ∈ Cj,
|˜p(e, S(cid:48)(j))| < 7(cid:48)(cid:48) with probability ≥ 1 − 1
n3 .
Lemma 10. At the end of Step 5a, for all j, ∀e ∈ Bj,
min(˜p(e, S(cid:48)(j)), ˜p(S(cid:48)(j + 1), e)) > 5(cid:48)(cid:48) with probability
≥ 1 − 1
n3 .
Combining Lemmas 6, 7 and 10 next lemma shows that the
size of Bj is bounded for all j.
Lemma 11. At the end of Step 5a, |Bj| ≤ 5(log n)x+1 for
all j, with probability ≥ 1 − 3
n3 .
Since all the elements in Cj are already close to an an-
chor, they need not be ranked. By Lemma 11 with proba-
bility ≥ 1 − 3
n3 the number of elements in Bj is at most
5(log n)x+1. We use RANK-x to rank each Bj and output
the ﬁnal ranking.

Maximum Selection and Ranking under Noisy Comparisons

Lemma 12 shows that all Bj’s are (cid:48)(cid:48)-ranked at the end of
Step 5b. Proof follows from properties of RANK-x and
union bound.
Lemma 12. At the end of Step 5b, all Bjs are (cid:48)(cid:48)-ranked
with probability ≥ 1 − 1
n3 .

(cid:16) n log n(log log n)x

Combining the above set of results yields our main result.
Theorem 13. Given access
to RANK-x, BINARY-
SEARCH-RANKING with probability ≥ 1 − 1
n , uses
comparisons and outputs an -
O
ranking.

(cid:17)

2

(cid:16) n log n(log log n)3

Using MERGE-RANK as a RANK-x algorithm with x = 3
leads to the following corollary.
Corollary
O
ranking with probability ≥ 1 − 1
n .

uses
comparisons and outputs an -

BINARY-SEARCH-RANKING

(cid:17)

14.

2

Using PALPAC-AMPRR (Sz¨or´enyi et al., 2015) as a
RANK-x algorithm with x = 1 leads to the following corol-
lary over PL model.
Corollary 15. Over PL model, BINARY-SEARCH-
probability ≥
RANKING with
uses
comparisons and outputs an -ranking.
O

(cid:16) n log n log log n
We show that under the noisy model, Ω(cid:0) n

It is well known that to rank a set of n values under the
noiseless setting, Ω(n log n) comparisons are necessary.

(cid:1) samples

1 − 1

(cid:17)

2

n

are necessary to output an -ranking and hence our algo-
rithm is near-optimal.
Theorem 16. For  ≤ 1
2 , there exists a noisy model
that satisﬁes SST and STI such that to output an -ranking

with probability ≥ 1 − δ, Ω(cid:0) n

(cid:1) comparisons are

4 , δ ≤ 1

2 log n

δ

necessary.

2 log n

δ

5. Experiments
We compare the performance of our algorithms with that
of others over simulated data. Similar to (Yue & Joachims,
2011), we consider the stochastic model where p(i, j) =
0.6 ∀i < j. Note that this model satisﬁes both SST and STI.
We ﬁnd 0.05-maximum with error probability δ = 0.1. Ob-
serve that i = 1 is the only 0.05-maximum. We compare
the sample complexity of KNOCKOUT with that of BTM-
PAC (Yue & Joachims, 2011), MallowsMPI (Busa-Fekete
et al., 2014a), and AR (Heckel et al., 2016). BTM-PAC is
an (, δ)-PAC algorithm for the same model considered in
this paper. MallowsMPI ﬁnds a Condorcet winner which
exists under our general model. AR ﬁnds the maximum ac-
cording to Borda scores. We also tried PLPAC (Sz¨or´enyi
et al., 2015), developed originally for PL model but the al-
gorithm could not meet guarantees of δ = 0.1 under this

Figure 1. Comparison of sample complexity for small input sizes,
with  = 0.05, and δ = 0.1

Figure 2. Comparison of sample complexity for large input size,
with  = 0.05, and δ = 0.1

model and hence omitted. Note that in all the experiments
the reported numbers are averaged over 100 runs.
In Figure 1, we compare the sample complexity of algo-
rithms when there are 7, 10 and 15 elements. Our al-
gorithm outperforms all the others. BTM-PAC performs
much worse in comparison to others because of high con-
stants in the algorithm. Further BTM-PAC allows com-
paring an element with itself since the main objective in
(Yue & Joachims, 2011) is to reduce the regret. We exclude
BTM-PAC for further experiments with higher number of
elements.
In Figure 2, we compare the algorithms when there are 50,
100, 200 and 500 elements. Our algorithm outperforms
others for higher number of elements too. Performance of
AR gets worse as the number of elements increases since
Borda scores of the elements get closer to each other and
hence AR takes more comparisons to eliminate an element.
Notice that number of comparisons is in logarithmic scale
and hence the performance of MallowsMPI appears to be
close to that of ours.
As noted in (Sz¨or´enyi et al., 2015), sample complexity of
MallowsMPI gets worse as ˜p(i, j) gets close to 0. To

n=7n=10n=15Number of elements103104105106Sample complexity KNOCKOUTMalllowsMPIARBTM-PACn = 50n = 100n = 200n = 500Number of elements104105106107108109Sample complexityKNOCKOUTMallowsMPIARMaximum Selection and Ranking under Noisy Comparisons

Figure 3. Sample complexity of KNOCKOUT and MallowsMPI
for different values of ˜q, with  = 0.05 and δ = 0.1

Figure 4. Sample complexity of KNOCKOUT and MallowsMPI
under Mallows model for various values of φ

show the pronounced effect, we use the stochastic model
p(1, j) = 0.6 ∀j > 1, p(i, j) = 0.5 + ˜q ∀j > i, i > 1
where ˜q < 0.1, and the number of elements is 15. Here too
we ﬁnd 0.05-maximum with δ = 0.1. Note that i = 1 is
the only 0.05-maximum in this stochastic model. In Fig-
ure 3, we compare the algorithms for different values of
˜q: 0.01, 0.005 and 0.001. As discussed above, the per-
formance of MallowsMPI gets much worse whereas our
algorithm’s performance stays unchanged. The reason is
that MallowsMPI ﬁnds the Condorcet winner using suc-
cessive elimination technique and as ˜q gets closer to 0,
MallowsMPI takes more comparisons for each elimina-
tion. Our algorithm tries to ﬁnd an alternative which de-
feats Condorcet winner with probability ≥ 0.5 − 0.05 and
hence for alternatives that are very close to each other, our
algorithm declares either one of them as winner after com-
paring them for certain number of times.
Next we evaluate KNOCKOUT on Mallows model which
does not satisfy STI. Mallows is a parametric model which
is speciﬁed by single parameter φ. As in (Busa-Fekete
et al., 2014a), we consider n = 10 elements and various
values for φ: 0.03, 0.1, 0.3, 0.5, 0.7, 0.8, 0.9, 0.95 and 0.99.
Here again we seek to ﬁnd 0.05-maximum with δ = 0.05.

Figure 5. Sample complexity of MERGE-RANK for different 

As we can see in Figure 4, sample complexity of KNOCK-
OUT and MallowsMPI is essentially same under small val-
ues of φ but KNOCKOUT outperforms MallowsMPI as φ
gets close to 1 since comparison probabilities grow closer
to 1/2. Surprisingly, for all values of φ except for 0.99,
KNOCKOUT returned Condorcet winner in all runs. For
φ = 0.99, KNOCKOUT returned second best element in 10
runs out of 100. Note that ˜p(1, 2) = 0.0025 and hence
KNOCKOUT still outputed a 0.05-maximum. Even though
we could not show theoretical guarantees of KNOCKOUT
under Mallows model, our simulations suggest that it can
perform well even under this model.
For the stochastic model p(i, j) = 0.6 ∀i < j, we run
our MERGE-RANK algorithm to ﬁnd an -ranking with
δ = 0.1. Figure 5 shows that sample complexity does not
increase a lot with decreasing . We attribute this to the
subroutine COMPARE that ﬁnds the winner faster when the
elements are more dissimilar.
Some more experiments are provided in Appendix G.

6. Conclusion
We studied maximum selection and ranking using noisy
comparisons for broad comparison models satisfying SST
and STI. For maximum selection we presented a simple
algorithm with linear, hence optimal, sample complexity.
For ranking we presented a framework that improves the
performance of many ranking algorithms and applied it to
merge ranking to derive a near-optimal algorithm.
We conducted several experiments showing that our algo-
rithms perform well and out-perform existing algorithms
on simulated data.
The maximum-selection experiments suggest that our al-
gorithm performs well even without STI. It would be of in-
terest to extend our theoretical guarantees to this case. For
ranking, it would be interesting to close the (log log n)3 ra-
tio between the upper- and lower- complexity bounds.

0.010.0050.001105106107108Sample complexityKNOCKOUTMallowsMPI00.20.40.60.81102104106108Sample complexityMallowsMPIKNOCKOUT02004006008001000Number of elements02468101214Sample complexity1060.090.050.010.0010.00010.00001Maximum Selection and Ranking under Noisy Comparisons

Negahban, Sahand, Oh, Sewoong, and Shah, Devavrat. Iterative
ranking from pair-wise comparisons. In NIPS, pp. 2474–2482,
2012.

Negahban, Sahand, Oh, Sewoong, and Shah, Devavrat. Rank cen-
trality: Ranking from pairwise comparisons. Operations Re-
search, 2016.

Plackett, Robin L. The analysis of permutations. Applied Statis-

tics, pp. 193–202, 1975.

Radlinski, Filip and Joachims, Thorsten. Active exploration for
In Proceedings of

learning rankings from clickthrough data.
the 13th ACM SIGKDD, pp. 570–579. ACM, 2007.

Radlinski, Filip, Kurup, Madhu, and Joachims, Thorsten. How
In Proceed-
does clickthrough data reﬂect retrieval quality?
ings of the 17th ACM conference on Information and knowl-
edge management, pp. 43–52. ACM, 2008.

Rajkumar, Arun and Agarwal, Shivani. A statistical convergence
perspective of algorithms for rank aggregation from pairwise
data. In Proc. of the ICML, pp. 118–126, 2014.

Syrgkanis, Vasilis, Krishnamurthy, Akshay, and Schapire,
Robert E. Efﬁcient algorithms for adversarial contextual learn-
ing. arXiv preprint arXiv:1602.02454, 2016.

Sz¨or´enyi, Bal´azs, Busa-Fekete, R´obert, Paul, Adil,

and
H¨ullermeier, Eyke. Online rank elicitation for plackett-luce:
A dueling bandits approach. In NIPS, pp. 604–612, 2015.

Urvoy, Tanguy, Clerot, Fabrice, F´eraud, Raphael, and Naamane,
In

Sami. Generic exploration and k-armed voting bandits.
Proc. of the ICML, pp. 91–99, 2013.

Yue, Yisong and Joachims, Thorsten. Beat the mean bandit. In

Proc. of the ICML, pp. 241–248, 2011.

Zhou, Yuan and Chen, Xi. Optimal pac multiple arm identiﬁcation

with applications to crowdsourcing. 2014.

7. Acknowledgements
We thank Yi Hao and Vaishakh Ravindrakumar for very
helpful discussions and suggestions, and NSF for sup-
porting this work through grants CIF-1564355 and CIF-
1619448.

References
Acharya, Jayadev, Jafarpour, Ashkan, Orlitsky, Alon, and Suresh,
Ananda Theertha. Sorting with adversarial comparators and
In ISIT, pp. 1682–1686.
application to density estimation.
IEEE, 2014a.

Acharya, Jayadev, Jafarpour, Ashkan, Orlitsky, Alon, and Suresh,
Ananda Theertha. Near-optimal-sample estimators for spheri-
cal gaussian mixtures. NIPS, 2014b.

Acharya, Jayadev, Falahatgar, Moein, Jafarpour, Ashkan, Orlit-
sky, Alon, and Suresh, Ananda Theertha. Maximum selection
and sorting with adversarial comparators and an application to
density estimation. arXiv preprint arXiv:1606.02786, 2016.

Ajtai, Mikl´os, Feldman, Vitaly, Hassidim, Avinatan, and Nel-
son, Jelani. Sorting and selection with imprecise comparisons.
ACM Transactions on Algorithms (TALG), 12(2):19, 2015.

Bradley, Ralph Allan and Terry, Milton E. Rank analysis of in-
complete block designs: I. the method of paired comparisons.
Biometrika, 39(3/4):324–345, 1952.

Busa-Fekete, R´obert, H¨ullermeier, Eyke, and Sz¨or´enyi, Bal´azs.
Preference-based rank elicitation using statistical models: The
case of mallows. In Proc. of the ICML, pp. 1071–1079, 2014a.

Busa-Fekete, R´obert, Sz¨or´enyi, Bal´azs, and H¨ullermeier, Eyke.
Pac rank elicitation through adaptive sampling of stochastic
pairwise preferences. In AAAI, 2014b.

Feige, Uriel, Raghavan, Prabhakar, Peleg, David, and Upfal, Eli.
Computing with noisy information. SIAM Journal on Comput-
ing, 23(5):1001–1018, 1994.

Heckel, Reinhard, Shah, Nihar B, Ramchandran, Kannan, and
Wainwright, Martin J. Active ranking from pairwise com-
parisons and when parametric assumptions don’t help. arXiv
preprint arXiv:1606.08842, 2016.

Herbrich, Ralf, Minka, Tom, and Graepel, Thore. Trueskill: a
In Proceedings of the 19th In-
bayesian skill rating system.
ternational Conference on Neural Information Processing Sys-
tems, pp. 569–576. MIT Press, 2006.

Jang, Minje, Kim, Sunghyun, Suh, Changho, and Oh, Sewoong.
Top-k ranking from pairwise comparisons: When spectral
ranking is optimal. arXiv preprint arXiv:1603.04153, 2016.

Luce, R Duncan. Individual choice behavior: A theoretical anal-

ysis. Courier Corporation, 2005.

Mallows, Colin L. Non-null ranking models. i. Biometrika, 44

(1/2):114–130, 1957.

Mukherjee, Sudipta. Data structures using C: 1000 problems and

solutions. McGraw Hill Education, 2011.

Maximum Selection and Ranking under Noisy Comparisons

A. Merge Ranking
We ﬁrst introduce a subroutine that is used by MERGE-RANK. It
merges two ordered sets in the presence of noisy comparisons.

A.1. MERGE
MERGE takes two ordered sets S1 and S2 and outputs an ordered
set Q by merging them. MERGE starts by comparing the ﬁrst ele-
ments in each set S1 and S2 and places the loser in the ﬁrst posi-
tion of Q. It compares the two elements sufﬁcient times to make
sure that output is near-accurate. Then it compares the winner
and the element right to loser in the corresponding set. It contin-
ues this process until we run out of one of the sets and then adds
the remaining elements to the end of Q and outputs Q.

Algorithm 6 MERGE
Input: Sets S1, S2, bias , conﬁdence δ.
Initialize: i = 1, j = 1 and O = ∅.
1. while i ≤ |S1| and j ≤ |S2|.

(a) if S1(i) = COMPARE(S1(i), S2(j), , δ), then

append S2(j) at the end of O and j = j + 1.

(b) else append S1(i) at the end of O and i = i + 1.
2. if i ≤ |S1|, then append S1(i : |S1|) at the end of O.
3. if j ≤ |S2|, then append S2(j : |S2|) at the end of O.
Output: O.

We show that when we merge two ordered sets using MERGE, the
error of resulting ordered set is not high compared to the maxi-
mum of errors of individual ordered sets.
Lemma 17. With probability ≥ 1 − (|S1| + |S2|)δ, error of
MERGE(S1, S2, , δ) is at most  more than the maximum of er-
rors of S1 and S2. Namely, with probability ≥ 1− (|S1| +|S2|)δ,

err(MERGE(S1, S2, , δ)) ≤ max (err(S1), err(S2)) + .

A.2. MERGE-RANK
Now we present the algorithm MERGE-RANK. MERGE-RANK
partitions the input set S into two sets S1 and S2 each of size
|S|/2.
It then orders S1 and S2 separately using MERGE-
RANK and combines the ordered sets using MERGE. Notice that
MERGE-RANK is a recursive algorithm. The singleton sets each
containing an unique element in S are merged ﬁrst. Two single-
ton sets are merged to form a set with two elements, then the sets
with two elements are merged to form a set with four elements
and henceforth. By Lemma 17, each merge with bound parameter
(cid:48) adds at most (cid:48) to the error. Since error of singleton sets is 0
and each element takes part in log n merges, the error of the out-
put set is at most (cid:48) log n. Hence with bound parameter / log n,
the error of the output set is less than .

Algorithm 7 MERGE-RANK
Input: Set S, bias , conﬁdence δ.
1. S1 = MERGE-RANK(S(1 : (cid:98)|S|/2(cid:99)), , δ).
2. S2 = MERGE-RANK(S((cid:98)|S|/2(cid:99) + 1 : |S|), , δ).
Output: MERGE(S1, S2, , δ).

B. Algorithms for Ranking

Algorithm 8 INTERVAL-BINARY-SEARCH
Input: Ordered array S, search element e, bias 
1. T = BUILD-BINARY-SEARCH-TREE(|S|).
2. Initialize set Q = ∅, node α = root(T ), and count

c = 0.

3. repeat for 30 log n times

go to the left child,α = left(α).

• else go to the right child, α = right(α).

2

2 ) > 1/2

(b) else

i. if COMPARE2(e, S(α1), 10

2 ) > 1/2 and

COMPARE2(S(α2), e, 10

2 ) > 1/2,

c = c + 1.

ii. else

A. if c = 0, α = parent(α).
B. else c = c − 1.

4.

(a) if c > 10 log n, Output: α1.
(b) else

i. Sort Q.
ii. Output: BINARY-SEARCH(S, Q, e, ).

(cid:7) to Q.

(a) if α2 − α1 > 1,

ii. if COMPARE2(S(α1), e, 10

i. Add α1, α2 and(cid:6) α1+α2
• if COMPARE2(S((cid:6) α1+α2

PARE2(e, S(α2), 10
the parent, α = parent(α).

iii. else

2

(cid:7)), e, 10

2 ) > 1/2 or COM-
2 ) > 1/2 then go back to

Maximum Selection and Ranking under Noisy Comparisons

Algorithm 9 BUILD-BINARY -SEARCH-TREE
Input: size n.
// Recall that each node m in the tree is an interval between
left end m1 and right end m2.
1. Initialize set T (cid:48) = ∅.
2. Initialize the tree T with the root node (1, n).

m = (1, n)

where m1 = 1 and m2 = n,

root(T ) = m

3. Add m to T (cid:48).
4. while T (cid:48) is not empty

(a) Consider a node i in T (cid:48).
(b) if i2 − i1 > 1, create a left child and right child

to i and set their parents as i.

(cid:18)

(cid:24) i1 + i2

(cid:25)(cid:19)

α =

i1,

,

β =

(cid:18)(cid:24) i1 + i2

(cid:25)

(cid:19)

, i2

,

2

left(i) = α,
parent(α) = i,

2
right(i) = β,
parent(β) = i.

and add nodes α and β to T (cid:48).

(c) Remove node i from T (cid:48).

Output: T .

Algorithm 10 BINARY-SEARCH
Input: Ordered array S, ordered array Q, search item e,
bias .
Initialize: l = 1, h = |Q|.
1. while h − l > 0

(a) t = COMPARE2
(b) if

Q((cid:6) l+h

(cid:7)).

∈

t

2

(c) else if t < 1

(d) else move to the right.

then Output:

(cid:17)

.

2

2

(cid:7)), 10 log n

(cid:16)
e, S(Q((cid:6) l+h
(cid:2) 1
2 + 3(cid:3),
2 − 3, 1
(cid:25)
(cid:24) l + h
2 − 3, then move to the left.
(cid:25)
(cid:24) l + h

h =

2

.

l =

.

2

C. Some tools for proving lemmas
We ﬁrst prove an auxilliary result that we use in the future analy-
sis.
Lemma 18. Let W = COMPARE(i, j, , δ) and L be the other
element. Then with probability ≥ 1 − δ,

p(W, L) ≥ 1
2

− .

Proof. Note that if |˜p(i, j)| < , then p(i, j) > 1
p(j, i) > 1
If |˜p(i, j)| ≥ , without loss of generality, assume that i is a better
element i.e., ˜p(i, j) ≥ . By Lemma 2, with probability atleast
1 − δ, W = i. Hence

2 − . Hence, p(W, L) ≥ 1
(cid:18)

2 −  and

2 − .

(cid:19)

− 

= P r(W = i) ≥ 1 − δ.

P r

p(W, L) ≥ 1
2

We now prove a Lemma that follows from SST and STI that we
will use in future analysis.
Lemma 19. If ˜p(i, j) ≤ 1, ˜p(j, k) ≤ 2, then ˜p(i, k) ≤ 1 + 2.

Proof. We will divide the proof into four cases based on whether
˜p(i, j) > 0 and ˜p(j, k) > 0.
If ˜p(i, j) ≤ 0 and ˜p(j, k) ≤ 0, then by SST, ˜p(i, k) ≤ 0 ≤
1 + 2.
If 0 < ˜p(i, j) ≤ 1 and 0 < ˜p(j, k) ≤ 2, then by STI, ˜p(i, k) ≤
1 + 2.
If ˜p(i, j) < 0 and 0 < ˜p(j, k) ≤ 2, then by SST, ˜p(i, k) ≤ 2 ≤
1 + 2.
If 0 < ˜p(i, j) ≤ 1 and ˜p(j, k) < 0, then by SST, ˜p(i, k) ≤ 1 ≤
1 + 2.

D. Proofs of Section 3
Proof of Lemma 2
Proof. Let ˆpr
i and ˆcr denote ˆpi and ˆc respectively after r number
of comparisons. Output of COMPARE(i, j, , δ) will not be i only
if ˆpr
2 for
r = m. We will show that the probability of each of these events
happening is bounded by δ
2 . Hence by union bound, Lemma fol-
lows.
After r comparisons, by Hoeffding’s inequality,

2 +  − ˆcr for any r < m = 1

δ or if ˆpi < 1

22 log 2

i < 1

P r(ˆpr

i <

1
2

+  − ˆcr) ≤ e

−2r(ˆcr )2

Using union bound,

− log 4r2

δ =

= e

δ
4r2 .

P r(∃r s.t. ˆpr

i ≤ 1
2

+  − ˆcr) ≤ δ
2

After m = 1

22 log 2

δ rounds, by Hoeffding’s inequality,

Output: Q(h).

P r(ˆpm

i <

) ≤ e

−2m2

=

1
2

δ
2

.

Maximum Selection and Ranking under Noisy Comparisons

1

2 pairs is compared at most

Proof of Lemma 3
Proof. Each of the |S|
22 log 2
times, hence the total comparisons is ≤ |S|
δ . Let k∗ =
max(KNOCKOUT-ROUND(S, , δ)) and s∗ = max(S). Let a be
the element paired with s∗. There are two cases: ˜p(s∗, a) ≥ 
and ˜p(s∗, a) < .
If ˜p(s∗, a) ≥ , by Lemma 2 with probability ≥ 1 − δ, s∗ will
win and hence by deﬁnitions of s∗ and k∗, ˜p(s∗, k∗) = 0 ≤ γ.
Alternatively, if ˜p(s∗, a) < , let winner(s∗, a) denote the winner
between s∗ and a. Then,

42 log 2

δ

(a)≤ r(winner(s
∗

, a))

(b)≤ r(k

∗

)

(c)≤ r(s
∗

)

r(a)

where (a) follows from r(a) ≤ r(s∗), (b) and (c) follow from the
deﬁnitions of s∗ and k∗ respectively. From stochastic tranisitivity
on a, k∗ and s∗, ˜p(s∗, k∗) ≤ γ ˜p(s∗, a) ≤ γ.

Proof of Theorem 4
Proof. We ﬁrst bound the number of comparisons. Let ni =
|S|
2i−1 be the number of elements in the set at the beginning of
round i. The number of comparisons at round i is

≤ ni
2

· γ422i/3
2c22

· log

2i+1

δ

.

Hence the number of comparisons in all rounds is

log |S|(cid:88)

i=1

|S|
2i

· γ422i/3
2c22

· log

2i+1

δ

(cid:19)

i + log

(cid:18)

1

2i/3

∞(cid:88)
≤ |S|γ4
(cid:18) 21/3
2c22
|S|γ4
(cid:18)
(cid:18)|S|γ4
2c22

i=1

=

= O

2

c2 +

1
c

log

1 + log

1
δ

2
δ

(cid:19)
(cid:19)(cid:19)

2
δ

.

We now show that with probability ≥ 1−δ, the output of KNOCK-
OUT is an -maximum. Let i = c/(γ2i/3) and δi = δ/2i. Note
that i and δi are bias and conﬁdence values used in round i. Let
bi be a maximum element in the set S before round i. Then by
Lemma 3, with probability ≥ 1 − δi,

˜p(bi, bi+1) ≤ i

(1)

By union bound, the probability that Equation 1 does not hold for
some round 1 ≤ i ≤ log |S| is

≤ log |S|(cid:88)

log |S|(cid:88)

δi =

i=1

i=1

δ

2i ≤ δ.

With probability ≥ 1 − δ, Equation 1 holds for all i and by
stochastic triangle inequality,

˜p(b1, blog |S|+1) ≤ log |S|(cid:88)

∞(cid:88)

˜p(bi, bi+1) ≤

c
γ2i/3 = /γ.

i=1

i=1

We now show that if ˜p(b1, e) ≤ /γ, e is an -maximum, namely
˜p(f, e) ≤  ∀f ∈ S. Note that b1 is a maximum element in the
original set S and hence r(b1) = n. If r(f ) ≥ r(e), then by
γ-stochastic transitivity, ˜p(f, e) ≤ γ ˜p(b1, e) ≤  and if r(f ) ≤
r(e), then ˜p(f, e) ≤ 0 ≤ .

E. Proofs of Section 4.1
Proof of Lemma 17
Proof. Let Q = MERGE(S1, S2, , δ). We will show that for ev-
ery k, w.p. ≥ 1− δ, ˜p(Q(k), Q(l)) ≤ max(err(S1), err(S2)) +
 ∀l > k. Note that if this property is true for every element
then err(Q) ≤ max(err(S1), err(S2)) + . Since there are
|S1| + |S2| elements in the ﬁnal merged set, the Lemma follows
by union bound.
If S1(i) and S2(j) are compared in MERGE algorithm, without
loss of generality, assume that S1(i) loses i.e., S1(i) appears be-
fore S2(j) in T . The elements that appear to the right of S1(i) in
We will show that w.p. ≥ 1 − δ, ∀e ∈ Q≥S1(i), ˜p(S1(i), e) ≤
max (err(S1), err(S2)) + .
By deﬁnition of error of an ordered set,

Q belong to set Q≥S1(i) = {S1(k) : k > i}(cid:83){S2(k) : k ≥ j}.

˜p(S1(i), S1(k)) ≤ err(S1) ∀k > i
˜p(S2(j), S2(k)) ≤ err(S2) ∀k ≥ j.

(2)
(3)

By Lemma 18, w.p. ≥ 1 − δ,

˜p(S1(i), S2(j)) ≤ .

(4)
Hence by Equations 3, 4 and Lemma 19, w.p. ≥ 1 − δ,
˜p(S1(i), S2(k)) ≤  + err(S2) ∀k ≥ j.

Proof of Lemma 5
Proof. We ﬁrst bound the total comparisons. Let C(Q, (cid:48), δ(cid:48)) be
the number of comparisons that the MERGE-RANK uses on a set
Q. Since MERGE-RANK is a recursive algorithm,

C(Q, 

(cid:48)

(cid:48)

, δ

) ≤C(Q[1 : (cid:98)|Q|/2(cid:99)], 

)
+ C(Q[(cid:98)|Q|/2(cid:99) : |Q|], 

, δ

(cid:48)

(cid:48)

(cid:48)

(cid:48)

, δ

) +

|Q|
2(cid:48)2 log

2
δ(cid:48) .

From this

O(cid:16) |S| log |S|
(cid:18)

(cid:48)2

|S|,

C

(cid:17)

one
log 1
δ(cid:48)

can
. Hence,

(cid:19)



log |S| ,

δ
|S|2

= O

obtain

that C(S, (cid:48), δ(cid:48))

(cid:18)|S| log3 |S|

2

|S|2
δ

log

=

(cid:19)

.

Now we bound the error. By Lemma 17, with probability ≥ 1 −
|Q|δ,
err(MERGE-RANK(Q, 

max{err(cid:0)MERGE-RANK(cid:0)Q[1 : (cid:98)|Q|/2(cid:99)], 
err(cid:0)MERGE-RANK(cid:0)T [(cid:98)|Q|/2(cid:99) + 1 : |Q|], 

(cid:48)(cid:1)(cid:1),
(cid:48)(cid:1)(cid:1)} + 

(cid:48)

.

(5)

)) ≤

, δ

, δ

, δ

(cid:48)
(cid:48)

(cid:48)

(cid:48)

We can bound the total times MERGE is called in a single in-
stance of MERGE-RANK(S, (cid:48), δ(cid:48)). MERGE combines the sin-
gleton sets and forms the sets with two elements, it combines
the sets with two elements and forms the sets with four ele-
ments and henceforth. Hence the total times MERGE is called
2i ≤ |S|. Therefore, the probability that Equation 5
|S|
holds every time when two ordered sets are merged in MERGE-
RANK(S, (cid:48), δ(cid:48)) is ≤ |S| · |S|δ(cid:48) = |S|2δ(cid:48).
If Equation 5 holds every time MERGE is called, then error of
(cid:48) ≤ (cid:48) log |S|. This

is(cid:80)log |S|
MERGE-RANK(S, (cid:48), δ(cid:48)) is at most(cid:80)log |S|

i=1

i=1

Maximum Selection and Ranking under Noisy Comparisons

is because err(S) is 0 if S has only one element. And a singleton
set participates in log n merges before becoming the ﬁnal output
set.
Therefore, w.p. ≥ 1 − |S|2δ(cid:48),

err(MERGE-RANK(S, 

(cid:48)

(cid:48)

)) ≤ log |S|

(cid:48)

.

, δ

Hence with probability ≥ 1 − δ,

(cid:18)

(cid:18)

err

MERGE-RANK

S,



log |S| ,

δ
|S|2

(cid:19)(cid:19)

≤ .

F. Proofs for Section 4.2
Proof of Lemma 6
Proof. Let set S be ordered s.t. ˜p(S(i), S(j)) ≥ 0 ∀i > j. Let
k = {S(l) : k ≤ l ≤ k + 5(log n)x+1 − 1). The probability
S(cid:48)(cid:48)
that none of the elements in S(cid:48)(cid:48)

k is selected for a given k is

(cid:18)

(cid:19)n/(log n)x

≤

1 − 5(log n)x+1

n

<

1
n5 .

Therefore by union bound, the probability that none of the ele-
ments in S(cid:48)(cid:48)

k is selected for any k is

≤ n · 1

n5 =

1
n4 .

Proof of Lemma 8
We prove Lemma 8 by dividing it into smaller lemmas. We refer
to |˜p(e, f )| as a measure of distance between elements e and f.
We divide all elements in S into two sets based on distance from
anchors. First set contains all elements that are far away from all
anchors and the second set contains all elements which are close
to atleast one of the anchors. INTERVAL-BINARY-SEARCH acts
differently on both sets.
We ﬁrst show that for elements in the ﬁrst set, INTERVAL-
BINARY-SEARCH places them in between the right anchors by
using just the random walk subroutine.
For elements in the second set, INTERVAL-BINARY-SEARCH
might fail to ﬁnd the right anchors just by using the random walk
subroutine. But we show that INTERVAL-BINARY-SEARCH visits
a close anchor during random walk and BINARY-SEARCH ﬁnds a
close anchor from the set of visited anchors using simple binary
search.
We ﬁrst prove Lemma 8 for the elements of ﬁrst set.
Lemma 20. For (cid:48)(cid:48) > (cid:48), consider an (cid:48)-ranked S(cid:48). If an element
e is such that |˜p(e, S(cid:48)(j))| > (cid:48)(cid:48) ∀j, then with probability ≥ 1 −
n6 step 4a of INTERVAL-BINARY-SEARCH(S(cid:48), e, (cid:48)(cid:48)) outputs
the index y such that ˜p(e, S(cid:48)(y)) > (cid:48)(cid:48) and ˜p(S(cid:48)(y + 1), e) > (cid:48)(cid:48).

1

Proof. We ﬁrst show that there is an unique y s.t. ˜p(e, S(cid:48)(y)) >
(cid:48)(cid:48) and ˜p(S(cid:48)(y + 1), e) > (cid:48)(cid:48).
Let i be the largest index such that ˜p(e, S(cid:48)(i)) > (cid:48)(cid:48). By
Lemma 19, ˜p(e, S(cid:48)(j)) > (cid:48)(cid:48) − (cid:48) > 0 ∀j < i. Hence by
the assumption on e, ˜p(e, S(cid:48)(j)) > (cid:48)(cid:48) ∀j < i. Let k be the

smallest index such that ˜p(S(cid:48)(k), e) > (cid:48)(cid:48). By a similar argument
as previously, we can show that ˜p(S(cid:48)(j), e) > (cid:48)(cid:48) ∀j > k.
Hence by the above arguments and the fact that |˜p(e, S(cid:48)(j))| >
(cid:48)(cid:48) ∀j, there exists only one y such that ˜p(e, S(cid:48)(y)) > (cid:48)(cid:48) and
˜p(S(cid:48)(y + 1), e) > (cid:48)(cid:48).
Thus in the tree T , there is only one leaf node w such that
˜p(e, S(cid:48)(w1)) > (cid:48)(cid:48) and ˜p(S(cid:48)(w2), e) > (cid:48)(cid:48).
Consider some node m which is not an ancestor of w. Then either
˜p(S(cid:48)(m1), e) > (cid:48)(cid:48) or ˜p(S(cid:48)(m2), e) < −(cid:48)(cid:48). Since we compare
e with S(cid:48)(m1) and S(cid:48)(m2) 10
(cid:48)(cid:48)2 times, we move to the parent of
m with probability atleast 19
20 .
Consider some node m which is an ancestor of w.
˜p(S(cid:48)(m1), e) < −(cid:48)(cid:48)
˜p(S(cid:48)(m2), e) > (cid:48)(cid:48),

(cid:7)), e)| > (cid:48)(cid:48). Therefore we move in direction of

|˜p(S(cid:48)((cid:6) m1+m2

Then
and

,

w with probability atleast 19
20 .
Therefore if we are not at w, then we move towards w with proba-
20 and if we are at w then the count c increases with
bility atleast 19
probability atleast 19
20 .
Since we start at most log n away from w if we move towards
w for 21 log n then the algorithm will output y. The proba-
bility that we will move towards w less than 21 log n times is
≤ e−30 log nD( 21

30 || 19

20 ) ≤ 1
n6 .

2

To prove Lemma 8 for the elements of the second set, we ﬁrst
show that the random walk subroutine of algorithm INTERVAL-
BINARY-SEARCH placing an element in wrong bin is highly un-
likely.
Lemma 21. For (cid:48)(cid:48) > (cid:48), consider an (cid:48)-ranked set S(cid:48). Now
consider an element e and y such that either ˜p(S(cid:48)(y), e) > (cid:48)(cid:48)
or ˜p(S(cid:48)(y + 1), e) < −(cid:48)(cid:48), then step 4a of INTERVAL-BINARY-
SEARCH(S(cid:48), e, (cid:48)(cid:48)) will not output y with probability ≥ 1 − 1
n7 .

Proof. Recall that step 4a of INTERVAL-BINARY-SEARCH out-
puts y if we are at the leaf node (y, y +1) and the count c is atleast
10 log n.
Since either ˜p(S(cid:48)(y), e) > (cid:48)(cid:48) or ˜p(S(cid:48)(y + 1), e) < −(cid:48)(cid:48),
when we are at leaf node (y, y + 1), the count decreases with
20 . Hence the probability that INTERVAL-
probability atleast 19
is greater
BINARY-SEARCH is at (y, y + 1) and the count
i=10 log n e−i·D( i−10 log n
|| 19
20 ) <
20 ) ≤ 1
n7 .

than 10 log n is at most (cid:80)30 log n

20 log ne−10 log nD( 1

3 || 19

2i

We now show that for an element of the second set, the random
walk subroutine either places it in correct bin or visits a close
anchor.
Lemma 22. For (cid:48)(cid:48) > (cid:48), consider an (cid:48)-ranked set S(cid:48). Now
consider an element e that is close to an element in S(cid:48) i.e.,
∃g :
n6 , step
4a of INTERVAL-BINARY-SEARCH(S(cid:48), e, (cid:48)(cid:48)) will either output
the right index y such that ˜p(S(cid:48)(y), e) < (cid:48)(cid:48) and ˜p(S(cid:48)(y +
1), e) > −(cid:48)(cid:48) or INTERVAL-BINARY-SEARCH visits S(cid:48)(h) such
that |˜p(S(cid:48)(h), e)| < 2(cid:48)(cid:48).

|˜p(S(cid:48)(g), e)| < (cid:48)(cid:48). With probability ≥ 1 − 1

Proof. By Lemma 21, step 4a of INTERVAL-BINARY-SEARCH
does not output a wrong interval with probability 1 − 1
n7 . Hence
we just need to show that w.h.p., e visits a close anchor.

Maximum Selection and Ranking under Noisy Comparisons

v

u

<

for

such

Let i be the largest index such that ˜p(e, S(cid:48)(i)) > 2(cid:48)(cid:48). Then
∀j < i, by Lemma 19, ˜p(e, S(j)) > 2(cid:48)(cid:48) − (cid:48) > (cid:48)(cid:48) .
Let k be the smallest index such that ˜p(S(cid:48)(k), e) > 2(cid:48)(cid:48). Then
∀j > k, by Lemma 19, ˜p(S(cid:48)(j), e) > (cid:48)(cid:48) .
that
Therefore
min(|˜p(S(cid:48)(u), e)|,|˜p(S(cid:48)(v), e)|) ≥ 2(cid:48)(cid:48) only one of three
sets {x : x < u},{x : u < x < v} and {x : x > v} contains an
index z such that |˜p(S(cid:48)(z), e)| < (cid:48)(cid:48).
Let a node α be s.t.
|˜p(S(cid:48)(c), e)| ≤ 2(cid:48)(cid:48).
such a node α then we are done.
So assume that INTERVAL-BINARY-SEARCH is at a node β s.t.
∀c ∈ {β1, β2,(cid:100) β1+β2
(cid:101)}, |˜p(S(cid:48)(c), e)| > 2(cid:48)(cid:48). Note that only
one of three sets {x : x < β1 or x > β2}, {x : β1 < x <
(cid:100) β1+β2
(cid:101) < x < β2} contains an index z
such that |˜p(S(cid:48)(z), e)| < (cid:48)(cid:48) and INTERVAL-BINARY-SEARCH
moves towards that set with probability 19
20 . Hence the probability
that we never visit an anchor that is less than 2(cid:48)(cid:48) away is at most
e−30 log nD( 15.5

(cid:101)},
If INTERVAL-BINARY-SEARCH reaches

for some c ∈ {α1, α2,(cid:100) α1+α2

(cid:101)} and {x : (cid:100) β1+β2

30 || 19

20 ) ≤ 1
n7 .

2

2

2

2

We now complete the proof by showing that for an element e from
the second set, if Q contains an index y of an anchor that is close
to e, BINARY-SEARCH will output one such index.
Lemma 23. For (cid:48)(cid:48) > (cid:48), consider ordered sets S(cid:48), Q s.t.
2 − (cid:48) ∀i > j. For an element e s.t.,
p(S(cid:48)(Q(i)), S(cid:48)(Q(j))) > 1
∃g :
|˜p(S(cid:48)(Q(g)), e)| < 2(cid:48)(cid:48), BINARY-SEARCH(S(cid:48), Q, e, (cid:48)(cid:48))
will return y such that |˜p(S(cid:48)(Q(y)), e)| < 4(cid:48)(cid:48) with probability
≥ 1 − 1
n6 .

(cid:48)(cid:48)2

(cid:48)(cid:48)2

− 10 log n

(cid:48)(cid:48)2 ≤ 1

(cid:48)(cid:48)2 ≤ 1

2 is less than e

2 − 3(cid:48)(cid:48) and 1

Proof. At any stage of BINARY-SEARCH, there are three possibil-
ities that can happen . Consider the case when we are comparing
e with S(cid:48)(Q(i)).
1.
wins for e is not between 1
− 10 log n

|˜p(S(cid:48)(Q(i)), e)| < 2(cid:48)(cid:48). Probability that the fraction of
2 + 3(cid:48)(cid:48) is less than

2 − 3(cid:48)(cid:48) is less than e

n10 . Hence BINARY-SEARCH outputs Q(i).

e
2. ˜p(S(cid:48)(Q(i)), e) > 2(cid:48)(cid:48). Probability that the fraction of wins for
e is more than 1
n10 . So BINARY-
SEARCH will not move right. Also notice that ˜p(S(cid:48)(Q(j)), e) >
2(cid:48)(cid:48) − (cid:48) > (cid:48)(cid:48) ∀j > i.
˜p(S(cid:48)(Q(i)), e) > 4(cid:48)(cid:48). Probability that the fraction of
3.
(cid:48)(cid:48)2 ≤
wins for e is more than 1
n10 . Hence BINARY-SEARCH will move left. Also notice that
1
˜p(S(cid:48)(Q(j)), e) > 4(cid:48)(cid:48) − (cid:48) > (cid:48)(cid:48) ∀j > i.
We can show similar results for ˜p(S(cid:48)(Q(i)), e) < −2(cid:48)(cid:48) and
˜p(S(cid:48)(Q(i)), e) < −4(cid:48)(cid:48). Hence if |˜p(S(cid:48)(Q(i)), e)| < 2(cid:48)(cid:48) then
BINARY-SEARCH outputs Q(i), and if 2(cid:48)(cid:48) < |˜p(S(cid:48)(Q(i)), e)| <
4(cid:48)(cid:48) then either BINARY-SEARCH outputs Q(i) or moves in the
correct direction and if |˜p(S(cid:48)(Q(i)), e)| > 4(cid:48)(cid:48), then BINARY-
SEARCH moves in the correct direction.
Lemma 24. INTERVAL-BINARY-SEARCH(S, e, ) terminates in
O( log n log log n

) comparisons for any set S of size O(n).

− 10 log n

(cid:48)(cid:48)2

2

Proof. Step 3 of INTERVAL-BINARY-SEARCH runs for 30 log n
iterations. In each iteration, INTERVAL-BINARY-SEARCH com-
pares e with at most 3 anchors and repeats each comparison for

10/2. So total comparisons in step 3 is O(log n/2). The
size of Q is upper bounded by 90 log n and BINARY-SEARCH
does a simple binary search over Q by repeating each comparison
10 log n/2. Hence total comparisons used by BINARY-SEARCH
is O(log n log log n/2)

Combining Lemmas 7, 21, 22, 23, 24 yields the result.

Proof of Lemma 11
Proof. Combining Lemmas 7, 10 and using union bound, at the
end of step 5a ,w.p. ≥ 1 − 2
n3 , S(cid:48) is (cid:48)-ranked and ∀j, e ∈ Bj,
min(˜p(e, S(cid:48)(j)), ˜p(S(cid:48)(j + 1), e)) > 5(cid:48)(cid:48). Hence by Lemma 19,
∀j, k < j, e ∈ Bj, ˜p(e, S(cid:48)(k)) > 5(cid:48)(cid:48) − (cid:48) > 4(cid:48)(cid:48). Similarly,
∀j, k > j, e ∈ Bj, ˜p(S(cid:48)(k), e) > 5(cid:48)(cid:48) − (cid:48) > 4(cid:48)(cid:48).
If |Bj| > 0, then ˜p(e, S(cid:48)(k)) > 4(cid:48)(cid:48) for e ∈ Bj, k ≤ j,
˜p(S(cid:48)(l), e) > 4(cid:48)(cid:48) for e ∈ Bj, l ≥ j + 1. Hence by stochas-
tic transitivity, ˜p(S(cid:48)(l), S(cid:48)(k)) > 4(cid:48)(cid:48) for l > j ≥ k. Therefore
there exists k, l s.t. ˜p(S(cid:48)(l), f ) > 0 ∀f ∈ {S(cid:48)(y) : y ≤ j},
˜p(S(cid:48)(k), S(cid:48)(l)) > 0 and ˜p(f, S(cid:48)(k)) > 0 ∀f ∈ {S(cid:48)(y) : y > j}.
Now by Lemma 6, w.p. ≥ 1 − 1
n4 , size of all such sets Bj is less
than 5(log n)x+1.
Lemma follows by union bound.

Proof of Theorem 13
We ﬁrst bound the running time of BINARY-SEARCH-RANKING
algorithm.
Theorem 25. BINARY-SEARCH-RANKING terminates after
O( n(log log n)x

log n) comparisons with probability ≥ 1 − 1
n2 .

2

2 log n)

n6 ) terminates after O( n

Proof. Step 2 RANK-x(S(cid:48), (cid:48), 1
comparisons with probability ≥ 1 − 1
n6 .
By Lemma
the step 4a INTERVAL-
8, for each element e,
BINARY-SEARCH(S(cid:48), e, (cid:48)(cid:48)) terminates after O( log n log log n
)
comparisons. Hence step 4 takes at most O( n log n log log n
) com-
parisons.
Comparing each element with the anchors in steps 5a takes at
most O( log n
2 ) comparisons.
With probability ≥ 1 − 1
terminates after O(|Bi| (log |Bi|)x
Lemma
1 − 3
n3 .

n4 step 5b RANK-X(Bi, (cid:48)(cid:48), 1
n4 )
By
≥
total comparisons
log n) ≤

11,
Hence, w.p. ≥ 1 − 3
n3 ,

|Bi| ≤ 5(log n)x+1 for all

log n) comparisons.
i w.p.

2

2

2

to rank all Bis is at most (cid:80)
(cid:80)
i O(
(cid:16) n log n(log log n)x

|Bi| log n(log(5(log n)x+1))x

(cid:17)

n2

2

i O(|Bi| (log |Bi|)x
) = O( n log n(log log n)x

2

).

2

Therefore, by summing comparisons over all steps, with
probability ≥ 1 − 1
is at most
O

total comparisons

.

2

Now we show that BINARY-SEARCH-RANKING outputs an -
ranking with high probability.
Theorem 26. BINARY-SEARCH-RANKING produces an -
ranking with probability at least 1 − 1
n2 .

Proof. By combining Lemmas 7, 9, 10, 12 and using union
bound, w.p. ≥ 1 − 1

n2 , at the end of step 5b,

Maximum Selection and Ranking under Noisy Comparisons

• S(cid:48) is (cid:48)-ranked.
• Each Ci has elements such that |˜p(Ci(j), S(i))| < 7(cid:48)(cid:48) for

all j.

• Each Bi has elements such that ˜p(S(cid:48)(i), Bi(j)) < −5(cid:48)(cid:48)

and ˜p(S(cid:48)(i + 1), Bi(j)) > 5(cid:48)(cid:48) for all j.

• All Bis are (cid:48)(cid:48)-ranked.

(cid:83) S(cid:48)(i)(cid:83) Ci, f ∈ S(cid:48)(j)(cid:83) Cj

(cid:83) Bj,

For j ≥ i, e ∈ Bi−1
˜p(e, f ) ≤ ˜p(e, S(cid:48)(i)) + ˜p(S(cid:48)(i), S(cid:48)(j)) + ˜p(S(cid:48)(j), f ) ≤ 7(cid:48)(cid:48) +
(cid:48) + 7(cid:48)(cid:48) < 15(cid:48)(cid:48) = . Combining the above results proves the
Theorem.

Combining Theorems 25, 26 yields the result.

Proof Sketch for Theorem 16
Proof sketch. Consider a stochastic model where there is an in-
herent ranking r and for any two consecutive elements p(i, i +
2 − 2. Suppose there is a genie that knows the true rank-
1) = 1
ing r up to the sets {r(2i − 1), r(2i)} for all i i.e., for each i,
genie knows {r(2i − 1), r(2i)} but it does not know the ranking
between these two elements. Since consecutive elements have
(i, i + 1) = 2 > , to ﬁnd an -ranking, the genie has to cor-
rectly identify the ranking within all the n/2 pairs. Using Fano’s
inequality from information theory, it can be shown that the genie
the consecutive elements with probability 1 − δ.

(cid:1) comparisons to identify the ranking of

needs at least Ω(cid:0) n

2 log n

δ

Figure 6. Sample complexity comparison of KNOCKOUT and
variations of BTM-PAC for different input sizes, with  = 0.05
and δ = 0.1

G. Additional Experiments
As we mentioned in Section 5, BTM-PAC allows comparison of
an element with itself. It is not beneﬁcial when the goal is to ﬁnd
-maximum. So we modify their algorithm by not allowing such
comparisons. We refer to this restricted version as R-BTM-PAC.
As seen in ﬁgure, performance of BTM-PAC does not increase
by much by restricting the comparisons.
We further reduce the constants in R-BTM-PAC. We change
Equations (7) and (8) in (Yue & Joachims, 2011) to cδ(t) =

(cid:113)

1

t log n3N

δ

and N = (cid:100) 1

2 log n3N

δ (cid:101), respectively.

We believe the same guarantees hold even with the updated con-
stants. We refer to this improved restricted version as IR-BTM-
PAC. Here too we consider the stochastic model where p(i, j) =
0.6∀ i < j and we ﬁnd 0.05-maximum with error probability
δ = 0.1.
In Figure 6 we compare the performance of KNOCKOUT and all
variations of BTM-PAC. As the ﬁgure suggests, the performance
of IR-BTM-PAC improves a lot but KNOCKOUT still outper-
forms it signiﬁcantly.
In Figure 7, we consider the stochastic model where p(i, j) =
0.6 ∀i < j and ﬁnd -maximum for different values of . Similar
to previous experiments, we use δ = 0.1. As we can see the
number of comparisons increases almost linearly with n. Further
the number of comparisons does not increase signiﬁcantly even
when  decreases. Also the number of comparisons seem to be
converging as  goes to 0. KNOCKOUT outperforms MallowsMPI
even for the very small  values. We attribute this to the subroutine
COMPARE that ﬁnds the winner faster when the distance between
elements are much larger than .

Figure 7. Sample complexity of KNOCKOUT for different values
of n and 

71015Number of elements103104105106107Sample complexityKNOCKOUTIR-BTM-PACR-BTM-PACBTM-PAC02004006008001000Number of elements02468101214Sample complexity105KNOCKOUT(0.09)KNOCKOUT(0.05)KNOCKOUT(0.01)KNOCKOUT(0.001)KNOCKOUT(0.0001)KNOCKOUT(0.00001)MallowsMPI