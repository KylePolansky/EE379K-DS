A. Proofs from Section 2
A.1. Proof of Proposition 1

Consistency Analysis for Binary Classiﬁcation Revisited

so that |B − B(cid:48)| ≤ 2|∆p|.

For the sake of readability, throughout the proof we ab-
breviate Φ = Φ(u, v, p), Φ(cid:48) = Φ(u(cid:48), v(cid:48), p(cid:48)), and denote
∆u = u − u(cid:48), ∆v = v − v(cid:48), ∆p = p − p(cid:48). In this notation,
proving p-Lipschitzness for metric Φ amounts to showing
that:

|Φ − Φ(cid:48)| ≤ Up|∆u| + Vp|∆v| + Pp|∆p|,

for constants Up, Vp, Pp, which may only depend on p.
The following fact is going to be very useful in prov-
ing p-Lipschitzness. If the metric is of the rational form:
Φ(u, v, p) = A(u,v,p)
B(u,v,p) + C, where C is some constant,
B(u, v, p) ≥ Gp for some positive constant Gp (which
may depend on p), and |Φ(u, v, p)| ≤ Φmax for some con-
stant Φmax, it sufﬁces to check p-Lipschitzness of numer-
ator and denominator separately. Indeed, using shorthand
notation A = A(u, v, p), A(cid:48) = A(u(cid:48), v(cid:48), p(cid:48)), and similarly
for B, B(cid:48):

Φ − Φ(cid:48) =

A − A(cid:48)
B(cid:48) B
B
A − A(cid:48)

=
A(cid:48)
B(cid:48)
|Φ − Φ(cid:48)| ≤ |A − A(cid:48)|

=

+

B

Gp

hence:

B(cid:48) B(cid:48) − A(cid:48)
B(cid:48) B

A − A(cid:48) + A(cid:48)
B
B − B(cid:48)

,

B

+

Φmax
Gp

|B(cid:48) − B|.

a) Accuracy Φ(u, v, p) = 1 − v − p + 2u. We have:

Φ − Φ(cid:48) ≤ 2∆u − ∆v − ∆p,

so that by triangle inequality:

|Φ − Φ(cid:48)| ≤ 2|∆u| + |∆v| + |∆p|.

Hence, the statement follows with Up = 2, Vp =
Pp = 1.

b) AM Φ(u, v, p) = 1 − vp−u

2p(1−p). We can use the re-
sult on the rational metric by noting that A(u, v, p) =
u − vp, B(u, v, p) = B(p) = 2p(1 − p), C = 1,
Φmax = 1, Gp = 2p(1 − p). We can now check the
p-Lipschitzness of A and B separately:

A − A(cid:48) = u − vp − u(cid:48) + v(cid:48)p(cid:48)

= ∆u + (vp(cid:48) − vp) + (v(cid:48)p(cid:48) − vp(cid:48))
= ∆u − v∆p − p(cid:48)∆v,

and since |v| ≤ 1, |p(cid:48)| ≤ 1, p-Lipschitzness follows
from triangle inequality. For the denominator,
B − B(cid:48) = 2p(1 − p) − 2p(cid:48)(1 − p(cid:48))
= 2(p − p(cid:48)) + 2(p(cid:48)2 − p2)
= 2(1 − p(cid:48) − p)(p − p(cid:48)),

c) Jaccard similarity Φ(u, v, p) = u

p+v−u. Follows from
the rational form of the metric, since A(u, v, p) = u,
B(u, v, p) = p + v − u, C = 0, Φmax = 1, Gp = p,
and the p-Lipschitzness of A(u, v, p) and B(u, v, p) is
trivial to show by the triangle inequality.

d) G-mean Φ(u, v, p) = u(1−v−p+u)

. Exploiting the
rational form of the metric, we have A(u, v, p) =
u(1 − v − p + u), B(u, v, p) = p(1 − p), C = 0,
Φmax = 1, Gp = p(1 − p). The p-Lipschitzness of B
was shown above for AM measure. As for A:

p(1−p)

A − A(cid:48) = (1 − v − p + u)(u − u(cid:48))
+ u(cid:48)(u − p − v − u(cid:48) − p(cid:48) − v(cid:48))
= (1 − v − p + u)∆u + u(cid:48)(∆u − ∆v − ∆p),

and hence the p-Lipschitzness follows by triangle in-
equality and the fact that |1 − v − p + u| ≤ 2 and
|u(cid:48)| ≤ 1.

e) AUC (v−u)(p−u)

p(1−p)

. Exploiting the rational form of the
metric, we have A(u, v, p) = (v − u)(p − u) and
B(u, v, p) = p(1 − p). The p-Lipschitzness of B was
shown above for AM measure; as for A:
A − A(cid:48) = (v − u)(p − u) − (v(cid:48) − u(cid:48))(p − u)

+ (v(cid:48) − u(cid:48))(p − u) − (v(cid:48) − u(cid:48))(p(cid:48) − u(cid:48))
= (∆v − ∆u)(p − u) + (v(cid:48) − u(cid:48))(∆p − ∆u),

and hence the p-Lipschitzness follows by triangle in-
equality and the fact that |p−u| ≤ 1 and |v(cid:48)−u(cid:48)| ≤ 1.

f) Linear-fractional metric of the form:

Φ(u, v, p) =

a1 + a2u + a3v + a4p
b1 + b2u + b3v + b4p

,

as long as the denominator is bounded from below by
some positive constant Gp. This follows immediately
from the rational form of the metric, as the numera-
tor A(u, v, p) and denominator B(u, v, p) are linear
functions of (u, v, p), so showing p-Lipschitzness of
A(u, v, p) and B(u, v, p) is straightforward.

B. Proofs from Section 3.1
B.1. Proof of Lemma 1

We ﬁx classiﬁer h and use a shorthand notation u, v,(cid:98)u,(cid:98)v
to denote u(h), v(h),(cid:98)u(h),(cid:98)v(h). Due to the Lipschitz as-
|Φ(u, v, p)−Φ((cid:98)u,(cid:98)v,(cid:98)p)| ≤ Up|u−(cid:98)u|+Vp|v−(cid:98)v|+Pp|p−(cid:98)p|.

sumption:

Consistency Analysis for Binary Classiﬁcation Revisited

Similarly, using standard Rademacher complexity argu-
ments (see, e.g. Mohri et al., 2012), we have, uniformly
over all h ∈ H, with probability 1 − δ/4,

and similarly, with probability 1 − δ/4,

(cid:2)Rn(H)(cid:3) +
|v −(cid:98)v| ≤ 2Ex
(cid:2)Rn(Hη)(cid:3) +
|u −(cid:101)u| ≤ 2Ex
(cid:20)
(cid:12)(cid:12)(cid:12) n(cid:88)

Rn(H) = Eσ

where Hη = {h · η : h ∈ H}, and:
1
n

sup
h∈H

i=1

(cid:115)
(cid:115)

log 4
δ
2n

,

log 4
δ
2n

,

(cid:12)(cid:12)(cid:12)(cid:21)

σih(xi)

is the Rademacher complexity6 of H. Furthermore, if we
let zi ∈ {−1, 1}, i = 1, . . . , n, with Pr(zi = 1) = 1+η(xi)
,
so that E [zi] = η(xi), we have:

2

n(cid:88)

σih(xi)η(xi) = Ez

σih(xi)zi

,

i=1

so that:

Rn(Hη) = Eσ

(cid:20)

1
n

sup
h∈H

(cid:20)

sup
h∈H

1
n

sup
h∈H

(cid:20)

≤ Eσ,z

= Eσ

Denote:

(cid:2)|p −(cid:98)p|(cid:3) .

Fixing x = (x1, . . . , xn) and taking expectation with re-
spect to y = (y1, . . . , yn) conditioned on x, we have:
Ey|x

i=1

1
n

η(xi),

≤ UpEy|x

n(cid:88)
n(cid:88)

(cid:2)|Φ(u, v, p) − Φ((cid:98)u,(cid:98)v,(cid:98)p)|(cid:3)
(cid:2)|u −(cid:98)u|(cid:3) + Vp|v −(cid:98)v| + PpEy|x
(cid:101)p = Ey|x [(cid:98)p] =
(cid:101)u = Ey|x [(cid:98)u] =
(cid:2)|p −(cid:101)p +(cid:101)p −(cid:98)p|(cid:3)
(cid:2)|p −(cid:98)p|(cid:3) = Ey|x
(cid:2)|(cid:101)p −(cid:98)p|(cid:3)
≤ |p −(cid:101)p| + Ey|x
(cid:104)(cid:112)((cid:101)p −(cid:98)p)2
(cid:105)
= |p −(cid:101)p| + Ey|x
(cid:113)Ey|x
(cid:2)((cid:101)p −(cid:98)p)2(cid:3)
≤ |p −(cid:101)p| +
(cid:113)
= |p −(cid:101)p| +
Vary|x((cid:98)p) ≤ |p −(cid:101)p| +

h(xi)η(xi)

1
n

i=1

We have:
Ey|x

Ey|x

(cid:114) 1

way, one can show that:

≤ |u −(cid:101)u| +

4n
where the second inequality follows from Jensen’s inequal-
x. In an analogous

Furthermore, using the convexity of the absolute value
function, Jensen’s inequality implies:

ity applied to a concave function x (cid:55)→ √
(cid:114) u
(cid:2)Φ((cid:98)u,(cid:98)v,(cid:98)p)(cid:3)(cid:12)(cid:12)(cid:12)
(cid:2)|Φ(u, v, p) − Φ((cid:98)u,(cid:98)v,(cid:98)p)|(cid:3) ,
(cid:2)Φ((cid:98)u,(cid:98)v,(cid:98)p)(cid:3)(cid:12)(cid:12)(cid:12) ≤ Up|u −(cid:101)u| + Vp|v −(cid:98)v|

(cid:2)|u −(cid:98)u|(cid:3) ≤ |u −(cid:101)u| +
(cid:12)(cid:12)(cid:12)Φ(u, v, p) − Ey|x
(cid:12)(cid:12)(cid:12)Φ(u, v, p) − Ey|x

≤ Ey|x

so that:

4n

4n

.

(cid:114) 1

+ Pp|p −(cid:101)p| +

Up + Vp

√
2

n

.

We will now show that under the class of thresholded func-
tions H speciﬁed in the statement of the theorem to which
h belongs, all the terms on the right-hand side are well con-
trolled. The rest of the proof follows in a straightforward
way from Hoeffding’s inequality and Vapnik-Chervonenkis
bounds, except for minor, technical details, which are in-
cluded for completeness.
We ﬁrst apply Hoeffding’s inequality to say that with prob-
ability at least 1 − δ/2,

(cid:115)

|p −(cid:101)p| ≤

log 4
δ
2n

.

,

(cid:105)
(cid:105)(cid:12)(cid:12)(cid:12)(cid:21)

i=1

(cid:104) n(cid:88)
(cid:12)(cid:12)(cid:12)Ez
(cid:104) n(cid:88)
(cid:12)(cid:12)(cid:12) n(cid:88)
(cid:12)(cid:12)(cid:12) n(cid:88)

i=1

i=1

1
n

i=1

σih(xi)zi

σih(xi)zi

(cid:12)(cid:12)(cid:12)(cid:21)

(cid:12)(cid:12)(cid:12)(cid:21)

σih(xi)

= Rn(H),

where the inequality is due to Jensen’s inequality applied to
convex functions |·| and sup{·}, and the second equality is
due to the fact that σizi and σi are distributed in the same
way.
Thus choosing Lp = max{Up, Vp, Pp}, with probability
1 − δ, uniformly over all h ∈ H,

(cid:2)Φ((cid:98)u,(cid:98)v,(cid:98)p)(cid:3)(cid:12)(cid:12)(cid:12) ≤ 4LpEx
(cid:115)

(cid:2)Rn(H)(cid:3)

(cid:12)(cid:12)(cid:12)Φ(u, v, p) − Ey|x

+ 3Lp

.
Now, if H is the class of threshold functions on η, its
growth function (Mohri et al., 2012) is equal to m + 1, and
thus we have7:

+

log 4
δ
2n

Lp√
n

(cid:114)

Rn(H) ≤

2 log(n + 1)

n

,

distributed according to P(σi = 1) = P(σi = −1) = 1
2 .

6Variables σi, i = 1, . . . , n, are i.i.d. Rademacher variables
7We could alternatively use the fact that VC-dimension of H
is 1, which would give a bound with log(n + 1) replaced by 1 +
log(n).

Consistency Analysis for Binary Classiﬁcation Revisited

n

so that with probability 1−δ, uniformly over all h ∈ H, we
get the bound in the statement of the theorem. The proof is
complete.

1
n

(cid:80)n

√
Lower bound. The dependence ˜O(1/
n) on the sam-
ple size stated in Lemma 1 cannot be improved in general.
To see this, take a metric Φ(u, v, p) = u, p-Lipschitzness
of which is trivial to show. Choose h(x) = 1 for all
i=1 yi. Hence,

x. Then, u(h) = p, while (cid:98)u(h) = 1
(cid:2)Φ((cid:98)u,(cid:98)v,(cid:98)p)(cid:3)(cid:12)(cid:12)(cid:12) = (cid:12)(cid:12)p − (cid:101)p(cid:12)(cid:12), where (cid:101)p =
(cid:12)(cid:12)(cid:12)Φ(u, v, p) − Ey|x
(cid:80)n
i=1 η(xi) and Ex [(cid:101)p] = p. Assume that η(x) follows
2. Denote |p − (cid:101)p| by Z. By Khinchine in-
equality, E [Z] ≥ 2c(cid:112)E [Z 2] = c/
n for some con-
stant c > 0. Furthermore, by Paley-Zygmund inequality
P(Z > E [Z] /2) ≥ (E[Z])2
4E[Z2] ≥ c2. Hence, with constant
probability,(cid:12)(cid:12)(cid:12)Φ(u, v, p) − Ey|x

a binomial distribution with P(η(x) = 1) = P(η(x) =
0) = 1

(cid:2)Φ((cid:98)u,(cid:98)v,(cid:98)p)(cid:3)(cid:12)(cid:12)(cid:12) ≥ c

√
2
n
√
for some c > 0, which shows that the rate ˜O(1/
be improved.

n) cannot

√

,

B.2. Proof of Theorem 1
First, note that for a given P, p-Lipschitzness implies
that Φ(u, v, p) is continuous as a function of (u, v). Let
H = {hη | hη = 1η(x)≥η, η ∈ [0, 1]} be the set of bi-
nary threshold functions on η(x). By Assumption 1, u(hη)
and v(hη) are continuous in the threshold η, and hence the
maximizer of Φ(u, v, p) over H exists due to compactness
of the domain of η. The existence of the maximizer, to-
gether with Assumption 1 and TP monotonicity implies by
PU ∈ H, i.e.
(Narasimhan et al., 2014a, Lemma 11) that h∗
the optimal PU classiﬁer is a threshold function.8.
For any given x = (x1, . . . , xn), let h∗
ETU(x) be the opti-
mal ETU classiﬁer. By TP monotonicity of Ψ, (Natarajan
et al., 2016, Theorem 1) implies that h∗
ETU(xi) = 0}
{η(xi) : h∗

{η(xi) : h∗
≤ min

ETU(x) satisﬁes:

ETU(xi) = 1}.

i=1,...,n

max

i=1,...,n

(cid:54)= η(xj) for all
However, by Assumption 1, η(xi)
i (cid:54)= j with probability one, so that the condition above
is satisﬁed with strict inequality, and hence there exists
ETU(xi) = 0} and
τ∗, which is between max{η(xi) : h∗
min{η(xi) : h∗
ETU(x)
8Lemma 11 of Narasimhan et al. (2014a) requires that the PU
maximizer within H is hη for some η ∈ (0, 1). However, we do
not impose this constraint here because the lemma can easily be
extended to the case η ∈ [0, 1] under our assumption that η(x)
has a density over [0, 1].

ETU(xi) = 1}. This means that h∗

is a threshold function on η(x) with threshold τ∗, i.e.
ETU ∈ H.
h∗
To conclude, with probability one, h∗

PU ∈ H.
2n + Lp√
Now, deﬁne /2 = 4Lp
n.
Then, with probability 1− δ (over the random choice of x),

(cid:113) 2 log(n+1)

(cid:113) log 4

ETU(x), h∗

+ 3Lp

n

δ

Φ(u(h∗

ETU(x)), v(h∗
PU), v(h∗

≤ Φ(u(h∗
≤ Ey|x
≤ Ey|x
≤ Φ(u(h∗

ETU(x)), p)
PU), p)

(cid:2)Φ((cid:98)u(h∗
(cid:2)Φ((cid:98)u(h∗

PU),(cid:98)v(h∗
ETU(x)),(cid:98)v(h∗
ETU(x)), v(h∗

PU),(cid:98)p)(cid:3) + /2
ETU(x)),(cid:98)p)(cid:3) + /2,

ETU(x)), p) + ,

where we used Lemma 1 twice in the second and fourth
inequality. Hence, with probability 1 − η,

(cid:12)(cid:12)(cid:12)Φ(u(h∗

ETU(x)), v(h∗

PU), p)

ETU(x)), p)

PU), v(h∗

− Φ(u(h∗

(cid:12)(cid:12)(cid:12) ≤ .
ETU(x)),(cid:98)p)(cid:3)
ETU(x)),(cid:98)v(h∗
PU),(cid:98)p)(cid:3)(cid:12)(cid:12)(cid:12) ≤ ,
(cid:2)Φ((cid:98)u(h∗
PU),(cid:98)v(h∗

(cid:12)(cid:12)(cid:12)Ey|x

(cid:2)Φ((cid:98)u(h∗

− Ey|x

Using analogous argument, one can show that with proba-
bility 1 − δ,

which ﬁnishes the proof.

B.3. Finite Sample Regime: Proof of Theorem 2

The PU-optimal classiﬁer is:

h∗
PU = argmax

h

Proposition 2.

ΦPrec(u(h), v(h), p) = argmax

u(h)

v(h) + α

.

h

(cid:40)

1, if x ∈ X1,
0, else .

h∗
PU(x) =

Proof. Note that for the deﬁned h∗
u(h∗

PU) = P(X1), and

PU) = v(h∗

PU classiﬁer, we have

ΦPrec(u(h∗

PU), v(h∗

PU), p) =

P(X1)

P(X1) + α

.

Firstly, observe that for any candidate optimal classiﬁer
h(cid:48), it must hold that h(cid:48)(x) = 0 for all x ∈ X3 (other-
wise the metric strictly decreases). Now, suppose there ex-
ists a classiﬁer h(cid:48)
PU which has strictly higher util-
ity than h∗
PU. Then, it must be that h(cid:48)(x) = 1 for all

(cid:54)= h∗

Consistency Analysis for Binary Classiﬁcation Revisited

x ∈ X2. We have, u(h(cid:48)) = P(X1) + P(X2)(1 − √
and v(h(cid:48)) = P(X1) + P(X2). So:

α)

ΦPrec(u(h(cid:48)), v(h(cid:48)), p) =

P(X1) + P(X2)(1 − √
P(X1) + P(X2) + α

α)

.

But for the chosen small value of α, we can show the con-
tradiction that:

ΦPrec(u(h(cid:48)), v(h(cid:48)), p) < ΦPrec(u(h∗

PU), v(h∗

PU), p).

Therefore, h∗

PU as stated is indeed optimal.

We see from the above constructed example that the PU
optimal classiﬁer assigns negative labels to 50% of the data
which are highly likely to belong to the positive class. PU
is sensitive to label noise if the metric is less stable as im-
plied by the high p-Lipschitz constant. Next, we show that
ETU is relatively more robust.
Given a set of instances x = {x1, x2, . . . , xn}, recall that
the ETU-optimal assignments can be computed as:

h∗
ETU(x) = s∗ := argmax
s∈{0,1}n

Ey∼P(.|x)ΦPrec(s, y) .

Proposition 3. On the subset of instances in x that have
deterministic labels, the ETU-optimal predictions satisfy:

(cid:40)

h∗
ETU(xj) = s∗

j =

1, if x ∈ X1,
0, if x ∈ X3 .

Note that the predictions coincide with that of h∗
indices.

PU on these

(cid:80)

Ey∼P(.|x)ΦPrec(s∗, y) =

Proof. Let Ii = {j : xj ∈ Xi}, for i = 1, 2, 3. Note that
the optimal value at the solution s∗ is given by:
s∗
j + ∆(s∗
I2
s∗
j∈I2

, yI2 )
s∗
j + αn
(2)
indicates the optimal assignments corresponding
, yI2) is a quantity that depends

where s∗
I2
to indices in I2 and ∆(s∗
I2
only on indices in I2, and is given by:

j +(cid:80)

j∈I1
j∈I1∪I3

(cid:80)

,

∆(s∗
I2

, yI2) =

P(yI2)(cid:104)yI2, s∗
I2

(cid:105)

(3)

(cid:88)

yI2∈{0,1}|I2|

ator term(cid:80)
(cid:80)

Fixing the optimal predictions for indices corresponding to
I2, the value (2) is maximized by maximizing the numer-
s∗
j and minimizing the denominator term
j∈I1
s∗
j . This is achieved precisely when the opti-
mal solution satisﬁes the statement in the proposition. The
proof is complete.

j∈I1∪I3

We know from Proposition 2 that h∗
PU sets the labels corre-
sponding to indices in the set I2 to 0. Now let us examine
what happens in the case of ETU, when labels have mild
, the label of an
noise (i.e. with some small probability
instance from X2 can be 0), at optimality. Consider a can-
didate optimal solution s(cid:48) that behaves exactly like h∗
PU, i.e.
j = 0 for all j ∈ I2, for some 1 ≤ k ≤ |I2|.
s(cid:48)
Then, ∆(s(cid:48)
I2

, yI2) = 0, so:

√

Ey∼P(.|x)ΦPrec(s(cid:48), y) =

|I1|

|I1| + αn

.

(4)

Now, consider another candidate solution s(cid:48)(cid:48) that is equal
to s(cid:48), but has a value of 1 corresponding to a subset of in-
dices j1, j2, . . . , jk ∈ I2. The value of this solution can be
shown to be:

Ey∼P(.|x)ΦPrec(s(cid:48)(cid:48), y) =

|I1| + k(1 − )
|I1| + k + αn
Comparing equations (4) and (5), we have that if:

.

 <

αn

|I1| + αn

,

(5)

(6)

then s(cid:48)(cid:48) is a strictly better solution than s(cid:48). In particular, as
(5) is mononotic in k, the optimal choice is k = |I2|. This
immediately leads to the following corollary.
Corollary 1.

1. If |I2| = 0, then
h∗
ETU(x) := s∗ = h∗

PU(x) .

2. Otherwise, if  < α

1+α , then

ETU(x) := s∗ (cid:54)= h∗
h∗
In particular, h∗
ETU assigns label 1 to all instances that
are overwhelmingly positive under P, corresponding
to indices I2, whereas h∗

PU assigns label 0.

PU(x) .

3. If |I1| = 0, but |I2| > 0 then for any 0 <  < 1,

ETU(x) := s∗ (cid:54)= h∗
h∗

PU(x) := 0 .

√

Note that  < α/(1 + α) does not hold for our choice of
α. However, case 3 in Corollary 1 is sufﬁcient to es-
 =
tablish the bound in Theorem 2, when P(X2) is very large.

C. Proofs for Section 4.1
Fix a binary classiﬁer h : X → {0, 1} and let the input
sample x = (x1, . . . , xn) be generated i.i.d. from P. For
the sake of clarity, abbreviate η(xi) = ηi and h(xi) = hi,
i = 1, . . . , n. In the proofs of Lemma 2 and Lemma 3 we
will use the following:

Consistency Analysis for Binary Classiﬁcation Revisited

where we used the independence of labels yi, i = 1, . . . , n.
Similarly, Ey|x
4n, which in total
gives:

(cid:2)((cid:98)p −(cid:101)p)2(cid:3) is at most 1
(cid:104)
((cid:98)z −(cid:101)z)(cid:62)∇2Φ((cid:101)z)((cid:98)z −(cid:101)z)

Using a lower bound −A on the second-order derivatives
and performing a similar chain of reasoning, one also gets:

.

n

(cid:105) ≤ A
(cid:105) ≥ − A

.

n

Ey|x

(cid:104)

Ey|x

((cid:98)z −(cid:101)z)(cid:62)∇2Φ((cid:101)z)((cid:98)z −(cid:101)z)
(cid:2)Φ((cid:98)z)(cid:3) − Φ((cid:101)z)(cid:107) ≤ A

,

2n

From that we have:
(cid:107)Ey|x

which is exactly what was to be shown.

C.2. Proof of Lemma 3

hiyi,(cid:98)v(h) =
n(cid:88)

• Empirical quantities:

(cid:98)u(h) =

1
n

n(cid:88)

i=1

n(cid:88)

i=1

1
n

hi,(cid:98)p =

n(cid:88)

i=1

1
n

yi,

• Semi-empirical quantities:

(cid:101)u(h) =
(we do not deﬁne(cid:101)v(h), as it would the same as(cid:98)v(h)).

and (cid:101)p =

hiηi,

1
n

1
n

i=1

i=1

ηi

n(cid:88)

(cid:2)(cid:98)u(h)(cid:3) ,

and (cid:101)p = Ey|x [(cid:98)p] .

Note that:(cid:101)u(h) = Ey|x
We will jointly denote (cid:98)z = ((cid:98)u(h),(cid:98)p), and similarly
(cid:101)z = ((cid:101)u(h),(cid:101)p). We will also abbreviate Φ((cid:98)z) =
Φ((cid:98)u(h),(cid:98)v(h),(cid:98)p) and similarly for Φ((cid:101)z).

C.1. Proof of Lemma 2

+

1
2

= 0.

that:

Ey|x

= ∇2

Furthermore, note that:

Assume Φ is two-times differentiable, with all partial
second-order derivatives bounded by A. Taylor expanding

for some z between(cid:98)z and(cid:101)z. Note that Ey|x [(cid:98)z] = (cid:101)z, so

Φ((cid:98)z) around point(cid:101)z up to the second order gives:
Φ((cid:98)z) = Φ((cid:101)z) + ∇Φ((cid:101)z)(cid:62)((cid:98)z −(cid:101)z)
((cid:98)z −(cid:101)z)(cid:62)∇2Φ(z)((cid:98)z −(cid:101)z)
(cid:104)∇Φ((cid:101)z)(cid:62)((cid:98)z −(cid:101)z)
(cid:105)
((cid:98)z −(cid:101)z)(cid:62)∇2Φ(z)((cid:98)z −(cid:101)z)
uu((cid:98)u −(cid:101)u)2 + 2∇2
up((cid:98)u −(cid:101)u)((cid:98)p −(cid:101)p) + ∇2
≤ A(cid:0)((cid:98)u −(cid:101)u)2 + 2|((cid:98)u −(cid:101)u)((cid:98)p −(cid:101)p)| + ((cid:98)p −(cid:101)p)2(cid:1)
≤ 2A(cid:0)((cid:98)u −(cid:101)u)2 + ((cid:98)p −(cid:101)p)2(cid:1),
where we used elementary inequality ab ≤ a2 + b2, and
uu,∇2
∇2
up,∇2
pp denote the second-order derivatives evalu-
(cid:104)
ated at some z = (u, p). Hence:
((cid:98)z −(cid:101)z)(cid:62)∇2Φ((cid:101)z)((cid:98)z −(cid:101)z)
((cid:98)p −(cid:101)p)2(cid:105)(cid:19)
(cid:104)
((cid:98)u −(cid:101)u)2(cid:105)
Since(cid:98)u is the empirical average over n labels and(cid:101)u is its
(cid:2)((cid:98)u −(cid:101)u)2(cid:3) is the vari-
ance of(cid:98)u, which is at most 1
4n, because(cid:98)u ∈ [0, 1]:
n(cid:88)
var((cid:98)u) =

expectation (over the labels), Ey|x

hiηi(1 − ηi) ≤ 1
4n

var(hiyi) ≤ 1
n

pp((cid:98)p −(cid:101)p)2

n(cid:88)

+ Ey|x

≤ 2A

Ey|x

Ey|x

(cid:18)

1
n2

(cid:104)

(cid:105)

,

.

i=1

i=1

Assume Φ is three-times differentiable, with all partial
third-order derivatives bounded by B. Taylor expanding

Φ((cid:98)z) around point(cid:101)z up to the third order gives:
Φ((cid:98)z) = Φ((cid:101)z) + ∇Φ((cid:101)z)(cid:62)((cid:98)z −(cid:101)z)
((cid:98)z −(cid:101)z)(cid:62)∇2Φ((cid:101)z)((cid:98)z −(cid:101)z)
2(cid:88)

((cid:98)zα −(cid:101)zα)((cid:98)zβ −(cid:101)zβ)((cid:98)zγ −(cid:101)zγ),
for some z between(cid:98)z and(cid:101)z. First note that Ey|x [(cid:98)z] =(cid:101)z,

∂zα∂zβ∂zγ

∂3Φ(z)

α,β,γ=1

1
2

1
6

+

+

so that:

Furthermore,

Ey|x

tr

= 0.

Ey|x

= Ey|x

(cid:104)∇Φ((cid:101)z)(cid:62)((cid:98)z −(cid:101)z)
(cid:105)
(cid:104)∇2((cid:98)z −(cid:101)z)(cid:62)Φ((cid:101)z)((cid:98)z −(cid:101)z)
(cid:105)
(cid:20)
(cid:16)∇2Φ((cid:101)z)((cid:98)z −(cid:101)z)((cid:98)z −(cid:101)z)(cid:62)(cid:17)(cid:21)
(cid:16)∇2Φ((cid:101)z)Σ
(cid:17)
(cid:2)((cid:98)z −(cid:101)z)((cid:98)z −(cid:101)z)(cid:62)(cid:3) is the covariance ma-
(cid:19)(cid:35)
(cid:34)(cid:18)hi(yi − ηi)2 hi(yi − ηi)2
(cid:18)hi hi

hi(yi − ηi)2

(yi − ηi)2

Eyi|xi

(cid:19)

= tr

,

ηi(1 − ηi)

,

hi

1

(cid:17)

= (∇2

uu + 2∇2

up)su + ∇2

ppsp,

i=1

1
n2

n(cid:88)
n(cid:88)
(cid:16)∇2Φ((cid:101)z)Σ

1
n2

i=1

Σ =

=

so that:

tr

where Σ = Ey|x

trix of(cid:98)z −(cid:101)z. By independence of examples,

where:

sp :=

su :=

1
n2

1
n2

n(cid:88)
n(cid:88)

i=1

ηi(1 − ηi),

hiηi(1 − ηi),

i=1

up,∇2

uu,∇2

and ∇2

pp denote be the second-order derivative

terms evaluated at ((cid:101)u,(cid:101)p). Thus, to ﬁnish the proof, we
only need to show that the ﬁrst order term is bounded by
3 n−3/2. To this end, note that for any numbers ai, bijk,
B
(cid:88)
such that |bijk| ≤ B, i, j, k = 1, . . . , 2:

|ai||aj||ak| = B(|a1| + |a2|)3.

bijkaiajak ≤ B

(cid:88)
(cid:18) 2(cid:88)

ijk

ijk

By H¨older’s inequality,

2(cid:88)

|ai| ≤

i=1

i=1

so that:
B(|a1| + |a2| + |a3|)3 ≤ 4B

Hence, if we bound:

|ai|3

22/3,

(cid:19)1/3
(cid:16)|a1|3 + |a2|3 + |a3|3(cid:17)

(cid:2)|(cid:98)p −(cid:101)p|3(cid:3). By
(cid:2)((cid:98)p −(cid:101)p)2(cid:3).

(cid:3). Using µ1 =

i

Ey|x

the third-order term 1
6

We now bound Ey|x
Cauchy-Schwarz inequality,

∂3Φ(z)

≤ B,

2B
3

∂zα∂zβ∂zγ

α,β,γ=1 . . . is bounded by:

(cid:80)2
(cid:16)|(cid:98)u −(cid:101)u|3 + |(cid:98)p −(cid:101)p|3(cid:17)
(cid:2)|(cid:98)u −(cid:101)u|3(cid:3) and Ey|x
(cid:2)((cid:98)p −(cid:101)p)4(cid:3)(cid:113)Ey|x
(cid:104)|(cid:98)p −(cid:101)p|3(cid:105) ≤(cid:113)Ey|x
(cid:104)
((cid:98)p −(cid:101)p)2(cid:105) ≤ 1
(cid:104)
((cid:98)p −(cid:101)p)4(cid:105)
(cid:88)
(cid:16)
(cid:2)((cid:98)p −(cid:101)p)4(cid:3) ≤ 3
4 and µ4 ≤ 1

nµ4 + 3n(n − 1)µ2
12, Ey|x

(cid:2)ak

aiajaka(cid:96)

Ey|x

Ey|x

1
n4

1
n4

(cid:17)

i,j,k,(cid:96)

=

=

2

.

.

4n
Denote ai = yi − ηi, and let µk = Ey|x
0, we have:

Before, we already showed that

Since µ2 ≤ 1
thus:

(cid:104)|(cid:98)p −(cid:101)p|3(cid:105) ≤

Ey|x

√

3
8

n−3/2 ≤ 1
4

n−3/2.

16n2 , and

Consistency Analysis for Binary Classiﬁcation Revisited

(cid:2)|(cid:98)u −(cid:101)u|3(cid:3), we conclude that

Using similar bound for Ey|x
3 n−3/2. Bounding the
the third-order term is bounded by B
third-order derivatives from below by −B, and using simi-
lar reasoning gives a lower bound of the same value. This
ﬁnishes the proof.

(cid:2)Φ((cid:98)u(h),(cid:98)v(h),(cid:98)p)(cid:3) and
(cid:123)(cid:122)

C.3. Proof of Theorem 3
Abbreviating Φ(h) = Ey|x
Φa(h) = Φappr(h):
(cid:125)
(cid:124)
ETU) − Φa(h∗
ETU) − Φ(h∗
a) = Φ(h∗
Φ(h∗
(cid:124)
(cid:123)(cid:122)
(cid:125)
(cid:123)(cid:122)
(cid:124)
a) − Φ(h∗
ETU) − Φa(h∗
a)
a)
≤ B

≤ B
+ Φa(h∗

≤ 2B
3n3/2

,

Φa(h∗

ETU)

(cid:125)

≤0

3n3/2

3n3/2

where the bounds shown in the inequalities are from
Lemma 3.

C.4. Derivation of the approximation algorithm for

Fβ-measure

.

Recall that Fβ(u, v, p) = (1+β2)u
derivatives with respect to u and p are:

β2p+v . The seconder order

=

−β2(1 + β2)
(β2p + v)2 ,

∂2Fβ
∂u∂p

∂2Fβ
2β4(1 + β2)u
∂u2 = 0,
(β2p + v)3 .
To optimize Φappr(h), we ﬁrst sort observations according
to η(xi). Then we precompute:

∂2Fβ
∂p2 =

Next, for each k = 0, 1, . . . , n, we precompute:

n(cid:88)

i=1

1
n2

i=1

1
n

n(cid:88)
(cid:101)p =
k(cid:88)
(cid:101)uk =
(1 + β2)(cid:101)uk
β2(cid:101)p + k

1
n

i=1

n

η(xi),

η(xi),(cid:98)vk =

(cid:101)pvar =
,(cid:101)uk
n )2(cid:101)uk
(β2(cid:101)p + k

k
n

− β2(1 + β2)

var +

var =

η(xi)(1 − η(xi)).

η(xi)(1−η(xi)).

1
n2

i=1

k(cid:88)
β4(1 + β2)(cid:101)uk
n )3 (cid:101)pvar,
(β2(cid:101)p + k

We then choose k for which the ETU approximation:

is maximized. The maximization can be done in time lin-
ear in O(n), so the most expensive operation is sorting the
instances.

D. Additional material to Section 4.2
Let x = (x1, . . . , xn) be the input sample (test set) of size

n generated i.i.d. from P. Given x and a function(cid:98)η : X →
[0, 1], let(cid:98)h = argmax
h∈(cid:98)H

(cid:2)Φ((cid:98)u(h),(cid:98)v(h),(cid:98)p)(cid:3)
(cid:123)(cid:122)
(cid:125)
=:(cid:98)ΦETU(h)

(cid:124)
Ey∼(cid:98)η(x)

.

Consistency Analysis for Binary Classiﬁcation Revisited

,

(cid:124)

=:ΦETU(h)

Ey∼η(x)

difference Ex

. By the deﬁnition

be the classiﬁer returned by the ETU procedure upon re-
ceiving the input sample x. Likewise, let:

(cid:2)Φ((cid:98)u(h),(cid:98)v(h),(cid:98)p)(cid:3)
(cid:123)(cid:122)
(cid:125)
h∗ = argmax
h∈(cid:98)H
be the optimal ETU classiﬁer in (cid:98)H. We want to bound the
(cid:104)|ΦETU((cid:98)h) − ΦETU(h∗)|(cid:105)
of h∗, ΦETU((cid:98)h) ≤ ΦETU(h∗) for any x, and thus:
(cid:104)|ΦETU((cid:98)h) − ΦETU(h∗)|(cid:105)
(cid:104)
(cid:105)
(cid:2)ΦETU(h∗)(cid:3) − Ex
ΦETU((cid:98)h)
(cid:104)(cid:98)ΦETU(h∗)
(cid:105)
(cid:2)ΦETU(h∗)(cid:3) − Ex
(cid:105) − Ex
(cid:104)(cid:98)ΦETU(h∗)
(cid:104)(cid:98)ΦETU((cid:98)h)
(cid:105)
(cid:123)(cid:122)
(cid:125)
(cid:124)
(cid:105)
(cid:104)
(cid:104)(cid:98)ΦETU((cid:98)h)
(cid:105) − Ex
ΦETU((cid:98)h)
(cid:12)(cid:12)(cid:12)Ex
(cid:105)(cid:12)(cid:12)(cid:12).
(cid:104)
ΦETU(h) −(cid:98)ΦETU(h)

= Ex
= Ex

+ Ex
≤ 2 sup
h∈(cid:98)H

+ Ex

Ex

(7)

≤0

n

notation, we have:

ΦETU(h) = Ey|x

Now, ﬁx some classiﬁer h and input sample x. We

let (cid:98)u(h),(cid:98)v(h),(cid:98)p denote the random variables generated
according to η (for ﬁxed x), while (cid:98)u(cid:48)(h),(cid:98)p(cid:48)(h) denote
random variables generated according to (cid:98)η; for instance,
(cid:80)n
i=1 h(xi)yi, where yi ∼ (cid:98)η(xi). Using this
(cid:98)u(cid:48)(h) = 1
(cid:98)ΦETU(h) = Ey|x
(cid:12)(cid:12)(cid:12)Ex
(cid:104)

(cid:2)Φ((cid:98)u(h),(cid:98)v(h),(cid:98)p)(cid:3) ,
(cid:2)Φ((cid:98)u(cid:48)(h),(cid:98)v(h),(cid:98)p(cid:48))(cid:3)
(note that (cid:98)v(h) does not depend on (cid:98)η or η, we (cid:98)v(cid:48)(h) =
(cid:98)v(h)). We now bound the term under sup in (7):
(cid:105)(cid:12)(cid:12)(cid:12)
ΦETU(h) −(cid:98)ΦETU(h)
≤ E(cid:104)(cid:12)(cid:12)Φ((cid:98)u,(cid:98)v,(cid:98)p) − Φ((cid:98)u(cid:48),(cid:98)v,(cid:98)p(cid:48))(cid:12)(cid:12)(cid:105)
≤ E(cid:104)(cid:12)(cid:12)Φ((cid:98)u,(cid:98)v,(cid:98)p) − Φ(u, v, p)(cid:12)(cid:12)(cid:105)
+ E(cid:104)(cid:12)(cid:12)Φ(u, v, p) − Φ((cid:98)u(cid:48),(cid:98)v,(cid:98)p(cid:48))(cid:12)(cid:12)(cid:105)

,

i=1

i=1

1
n

1
n

for some constant c. Moreover, using p-Lipschitzness of Φ,
we have:

Now,
shown in the proof of Lemma 1 to be at most
4n as the
expected deviation of the empirical average of [0, 1]-valued
random variable from its mean. Thus it remains to bound

E(cid:104)(cid:12)(cid:12)Φ(u, v, p) − Φ((cid:98)u(cid:48),(cid:98)v,(cid:98)p(cid:48))(cid:12)(cid:12)(cid:105) ≤ UpE(cid:2)|(cid:98)u(cid:48) − u|(cid:3)
+ VpE(cid:2)|(cid:98)v − v|(cid:3) + PpE(cid:2)|(cid:98)p(cid:48) − p|(cid:3) .
the term E(cid:2)|(cid:98)v − v|(cid:3) is well-controlled and was
(cid:113) 1
the terms E(cid:2)|(cid:98)p(cid:48) − p|(cid:3) and E(cid:2)|(cid:98)u(cid:48) − u|(cid:3). Deﬁne:
n(cid:88)
(cid:2)(cid:98)p(cid:48)(cid:3) =
(cid:98)η(xi),
n(cid:88)
(cid:2)(cid:98)u(cid:48)(cid:3) =
h(xi)(cid:98)η(xi),
(cid:2)(cid:101)p(cid:48)(cid:3) = E(cid:2)(cid:98)η(x)(cid:3) .
(cid:2)(cid:101)u(cid:48)(cid:3) = E(cid:2)h(x)(cid:98)η(x)(cid:3) .

(cid:101)p(cid:48) = Ey|x
(cid:101)u(cid:48) = Ey|x
p(cid:98)η = Ex
u(cid:98)η = Ex
|p −(cid:98)p(cid:48)| ≤ |p − p(cid:98)η| + |p(cid:98)η −(cid:101)p(cid:48)| + |(cid:101)p(cid:48) −(cid:98)p(cid:48)|
(cid:2)|p(cid:98)η −(cid:101)p(cid:48)|(cid:3), as well
(cid:2)|(cid:101)p(cid:48) −(cid:98)p(cid:48)|(cid:3) are both the expected deviations of the
(cid:113) 1
E(cid:2)|(cid:98)p(cid:48) − p|(cid:3) ≤ |p − p(cid:98)η| +
E(cid:2)|(cid:98)u(cid:48) − u|(cid:3) ≤ |u − u(cid:98)η| +
(cid:104)
ΦETU(h) −(cid:98)ΦETU(h)
(cid:114)

As before, we use the fact that Ex
as Ey|x
empirical averages of [0, 1]-valued random variables from
their means, and therefore are bounded by

Using analogous way of reasoning, one gets:

Putting it all together, we get:

We decompose:

(cid:12)(cid:12)(cid:12)Ex

4n. Hence:

(cid:105)(cid:12)(cid:12)(cid:12)

1√
n

1√
n

.

.

+ Up|u(h) − u(cid:98)η(h)| + Pp|p − p(cid:98)η|,

≤ c(cid:48)

log n

n

for some constant c(cid:48). Using (7), we ﬁnally get:

Ex

(cid:114)
(cid:104)(cid:12)(cid:12)ΦETU((cid:98)h) − ΦETU(h∗)(cid:12)(cid:12)(cid:105) ≤ c(cid:48)
h∈(cid:98)H

+ sup

log n

+ Pp|p − p(cid:98)η|
Up|u(h) − u(cid:98)η(h)|,

n

where the ﬁrst inequality is due to Jensen’s inequality ap-
plied to a convex function x (cid:55)→ |x|, the all expectations
except for the ﬁrst line are joint with respect to (x, y),
and for the sake of clarity we drop the dependence on h

in(cid:98)u(h),(cid:98)v(h),(cid:98)u(cid:48)(h). Now, it follow from Lemma 1 that:

(cid:114)
E(cid:104)(cid:12)(cid:12)Φ((cid:98)u,(cid:98)v,(cid:98)p) − Φ(u, v, p)(cid:12)(cid:12)(cid:105) ≤ c

log n

n

,

which was to be shown.

Consistency Analysis for Binary Classiﬁcation Revisited

E. Isotron Algorithm (Kalai & Sastry, 2009)
Here we include the Isotron Algorithm of (Kalai & Sas-
try, 2009) for completeness. The second update step is the
Pool of Adjacent Violators (PAV) routine, which solves the
isotonic regression problem:

u∗
1, u∗

2, . . . , u∗

n = arg

min

u1≤u2≤···≤un

(yi − ui)2,

n(cid:88)

i=1

where the instances are assumed to be sorted according to
their scores wT x (using w obtained in ﬁrst update step of
the iteration). This is a convex quadratic program and can
be solved efﬁciently. The output link function u of the Al-
gorithm is a piecewise linear estimate.

Algorithm 2 The Isotron algorithm (Kalai & Sastry, 2009).

i=1, iterations T

Input: Training data {(xi, yi)}n
Output: wT , uT
w0 ← 0
u0 ← z (cid:55)→ min(max(0, 2 · z + 1), 1)
for t = 1, 2, . . . , T do

wt ← wt−1 + 1
ut ← PAV({(cid:104)wt, xi(cid:105), yi})

n

(cid:80)n
i=1(yi − ut−1((cid:104)wt−1, xi(cid:105))) · xi

end for

