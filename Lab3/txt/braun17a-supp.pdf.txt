Lazifying Conditional Gradient Algorithms

G´abor Braun * 1 Sebastian Pokutta * 1 Daniel Zink * 1

Abstract

Conditional gradient algorithms (also often called
Frank-Wolfe algorithms) are popular due to their
simplicity of only requiring a linear optimization
oracle and more recently they also gained signif-
icant traction for online learning. While simple
in principle, in many cases the actual implemen-
tation of the linear optimization oracle is costly.
We show a general method to lazify various con-
ditional gradient algorithms, which in actual com-
putations leads to several orders of magnitude of
speedup in wall-clock time. This is achieved by
using a faster separation oracle instead of a linear
optimization oracle, relying only on few linear
optimization oracle calls.

1. Introduction
Convex optimization is an important technique both from
a theoretical and an applications perspective. Gradient de-
scent based methods are widely used due to their simplicity
and easy applicability to many real-world problems. We are
interested in solving constraint convex optimization prob-
lems of the form

min
x∈P

f (x),

(1)

where f is a smooth convex function and P is a polytope,
with access to f being limited to ﬁrst-order information,
i.e., we can obtain ∇f (v) and f (v) for a given v ∈ P and
access to P via a linear minimization oracle which returns
x = argminv∈P cx for a given linear objective c.
When solving Problem (1) using gradient descent ap-
proaches in order to maintain feasibility, typically a projec-
tion step is required. This projection back into the feasible
region P is potentially computationally expensive, espe-
cially for complex feasible regions in very large dimensions.
As such projection-free methods gained a lot of attention
recently, in particular the Frank-Wolfe algorithm (Frank

1ISyE, Georgia Institute of Technology, Atlanta, GA. Corre-

spondence to: Daniel Zink <daniel.zink@gatech.edu>.

Proceedings of the 34 th International Conference on Machine
Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by
the author(s).

P start vertex, LPP linear minimization oracle

Algorithm 1 Frank-Wolfe Algorithm (Frank & Wolfe,
1956)
Input: smooth convex f function with curvature C, x1 ∈
Output: xt points in P
1: for t = 1 to T − 1 do
2:
vt ← LPP (∇f (xt))
xt+1 ← (1 − γt)xt + γtvt with γt := 2
3:
4: end for

t+2

& Wolfe, 1956) (also known as conditional gradient de-
scent (Levitin & Polyak, 1966); see also (Jaggi, 2013) for
an overview) and its online version (Hazan & Kale, 2012)
due to their simplicity. We recall the basic Frank-Wolfe
algorithm in Algorithm 1. These methods eschew the pro-
jection step and rather use a linear optimization oracle to
stay within the feasible region. While convergence rates
and regret bounds are often suboptimal, in many cases the
gain due to only having to solve a single linear optimization
problem over the feasible region in every iteration still leads
to signiﬁcant computational advantages (see e.g., (Hazan
& Kale, 2012, Section 5)). This led to conditional gradi-
ents algorithms being used for e.g., online optimization
and more generally machine learning and the property that
these algorithms naturally generate sparse distributions over
the extreme points of the feasible region (sometimes also
refereed to as atoms) is often helpful. Further increasing
the relevance of these methods, it was shown recently that
conditional gradient methods can also achieve linear con-
vergence (see e.g., (Garber & Hazan, 2013; Lacoste-Julien
& Jaggi, 2015; Garber & Meshi, 2016)) as well as that the
number of total gradient evaluations can be reduced while
maintaining the optimal number of oracle calls as shown in
(Lan & Zhou, 2014).

Oracle 1 Weak Separation Oracle LPsepP (c, x, Φ, K)
Input: c ∈ Rn linear objective, x ∈ P point, K ≥ 1
accuracy, Φ > 0 objective value;
Output: Either (1) y ∈ P vertex with c(x− y) > Φ/K, or
(2) false: c(x − z) ≤ Φ for all z ∈ P .
Unfortunately, for complex feasible regions even solving the
linear optimization problem might be time-consuming and
as such the cost of solving the LP might be non-negligible.

Lazifying Conditional Gradient Algorithms

accuracy, Φ > 0 objective value;

(2) false: c(x − z) ≤ Φ for all z ∈ P .

Oracle 2 LPsepP (c, x, Φ, K) via LP oracle
Input: c ∈ Rn linear objective, x ∈ P point, K ≥ 1
Output: Either (1) y ∈ P vertex with c(x− y) > Φ/K, or
1: if y ∈ P cached with c(x − y) > Φ/K exists then
2:
3: else
4:
5:
6:
7:
8:
9:
10: end if

return y {Cache call}
compute y ← argmaxx∈P c(x) {LP call}
if c(x − y) > Φ/K then
return y and add y to cache
else

return false

end if

This could be the case, e.g., when linear optimization over
the feasible region is a hard problem or when solving large-
scale optimization problems or learning problems. As such
it is natural to ask the following questions:

(i) Does the linear optimization oracle have to be called

in every iteration?

(ii) Does one need approximately optimal solutions for

convergence?

(iii) Can one reuse information across iteration?

We will answer these questions in this work, showing that (i)
the LP oracle is not required to be called in every iteration,
that (ii) much weaker guarantees are sufﬁcient, and that (iii)
we can reuse information. To signiﬁcantly reduce the cost
of oracle calls while maintaining identical convergence rates
up to small constant factors, we replace the linear optimiza-
tion oracle by a (weak) separation oracle (see Oracle 1)
which approximately solves a certain separation problem
within a multiplicative factor and returns improving vertices
(or atoms). We stress that the weak separation oracle is
signiﬁcantly weaker than approximate minimization, which
has been already considered in (Jaggi, 2013). In fact, if the
oracle returns an improving vertex then this vertex does not
imply any guarantee in terms of solution quality with respect
to the linear minimization problem. It is this relaxation of
the dual guarantees that will provide a signiﬁcant speedup
as we will see later. At the same time, in case that the oracle
returns false, we directly obtain a dual bound via convexity.
A (weak) separation oracle can be realized by a single call
to a linear optimization oracle, however with two important
differences. It allows for caching and early termination:
Previous solutions are cached, and ﬁrst it is veriﬁed whether
any of the cached solutions satisfy the oracle’s separation
condition. The underlying linear optimization oracle has to
be called, only when none of the cached solutions satisfy
the condition, and the linear optimization can be stopped as
soon as a satisfactory solution with respect to the separation
condition is found. See Algorithm 2 for pseudo-code; early
termination is implicit in line 4.
We call this technique lazy optimization and we will demon-
strate signiﬁcant speedups in wall-clock performance (see
e.g., Figure 1), while maintaining identical theoretical con-
vergence rates.
To exemplify our approach we provide conditional gradi-
ent algorithms employing the weak separation oracle for
the standard Frank-Wolfe algorithm as well as the variants
in (Hazan & Kale, 2012; Garber & Meshi, 2016; Garber
& Hazan, 2013), which have been chosen due to requir-
ing modiﬁed convergence arguments that go beyond those
required for the vanilla Frank-Wolfe algorithm. Comple-
menting the theoretical analysis we report computational

results demonstrating effectiveness of our approach via a
signiﬁcant reduction in wall-clock running time compared
to their linear optimization counterparts.

Related Work

There has been extensive work on Frank-Wolfe algorithms
and conditional gradient descent algorithms and we will be
only able to review work most closely related to ours. The
Frank-Wolfe algorithm was originally introduced in (Frank
& Wolfe, 1956) (also known as conditional gradient descent
(Levitin & Polyak, 1966) and has been intensely studied in
particular in terms of achieving stronger convergence guar-
antees as well as afﬁne-invariant versions. We demonstrate
our approach for the vanilla Frank-Wolfe algorithm (Frank
& Wolfe, 1956) (see also (Jaggi, 2013)) as an introduc-
tory example. We then consider more complicated variants
that require non-trivial changes to the respective conver-
gence proofs to demonstrate the versatility of our approach.
This includes the linearly convergent variant via local lin-
ear optimization (Garber & Hazan, 2013) as well as the
pairwise conditional gradient variant of (Garber & Meshi,
2016), which is especially efﬁcient in terms of implementa-
tion. However, our technique also applies to the Away-Step
Frank-Wolfe algorithm, the Fully-Corrective Frank-Wolfe
algorithm, as well as the Block-Coordinate Frank-Wolfe al-
gorithm. Recently, in (Freund & Grigas, 2016) guarantees
for arbitrary step-size rules were provided and an analo-
gous analysis can be also performed for our approach. On
the other hand, the analysis of the inexact variants, e.g.,
with approximate linear minimization does not apply to our
case as our oracle is signiﬁcantly weaker than approximate
minimization as pointed out earlier. For more information,
we refer the interested reader to the excellent overview in
(Jaggi, 2013) for Frank-Wolfe methods in general as well as
(Lacoste-Julien & Jaggi, 2015) for an overview with respect
to global linear convergence.

Lazifying Conditional Gradient Algorithms

It was also recently shown in (Hazan & Kale, 2012) that
the Frank-Wolfe algorithm can be adjusted to the online
learning setting and here we provide a lazy version of this
algorithm. Combinatorial convex online optimization has
been investigated in a long line of work (see e.g., (Kalai
& Vempala, 2005; Audibert et al., 2013; Neu & Bart´ok,
2013)). It is important to note that our regret bounds hold
in the structured online learning setting, i.e., our bounds
depend on the (cid:96)1-diameter or sparsity of the polytope, rather
than its ambient dimension for arbitrary convex functions
(see e.g., (Cohen & Hazan, 2015; Gupta et al., 2016)). We
refer the interested reader to (Hazan, 2016) for an extensive
overview.
A key component of the new oracle is the ability to cache
and reuse old solutions, which accounts for the majority
of the observed speed up. The idea of caching of oracle
calls was already explored in various other contexts such
as cutting plane methods (see e.g., (Joachims et al., 2009))
as well as the Block-Coordinate Frank-Wolfe algorithm in
(Shah et al., 2015; Osokin et al., 2016). Our laziﬁcation
approach (which uses caching) is different however in the
sense that our weak separation oracle does not resemble an
approximate linear optimization oracle with a multiplicative
approximation guarantee; see (Osokin et al., 2016, Proof of
Theorem 3. Appendix F) and (Lacoste-Julien et al., 2013)
for comparison to our setup. In fact, our weaker oracle does
not imply any approximation guarantee and differs from
approximate minimization as done e.g., in (Jaggi, 2013)
substantially.

Contribution

The main technical contribution of this paper is a new ap-
proach, whereby instead of ﬁnding the optimal solution,
the oracle is used only to ﬁnd a good enough solution or a
certiﬁcate that such a solution does not exist, both ensur-
ing the desired convergence rate of the conditional gradient
algorithms.
Our contribution can be summarized as follows:

(i) Lazifying approach. We provide a general method to
lazify conditional gradient algorithms. For this we replace
the linear optimization oracle with a weak separation oracle,
which allows us to reuse feasible solutions from previous
oracle calls, so that in many cases the oracle call can be
skipped. In fact, once a simple representation of the under-
lying feasible region is learned no further oracle calls are
needed. We also demonstrate how parameter-free variants
can be obtained.

(ii) Laziﬁed conditional gradient algorithms. We exem-
plify our approach by providing lazy versions of the vanilla
Frank-Wolfe algorithm as well as of the conditional gradient
methods in (Hazan & Kale, 2012; Garber & Hazan, 2013;

Garber & Meshi, 2016).

(iii) Weak separation through augmentation. We show in
the case of 0/1 polytopes how to implement a weak separa-
tion oracle with at most k calls to an augmentation oracle
that on input c ∈ Rn and x ∈ P provides either an improv-
ing solution x ∈ P with cx < cx or ensures optimality,
where k denotes the (cid:96)1-diameter of P . This is useful when
the solution space is sparse.

(iv) Computational experiments. We demonstrate compu-
tational superiority by extensive comparisons of the weak
separation based versions with their original versions. In
all cases we report signiﬁcant speedups in wall-clock time
often of several orders of magnitude.

It is important to note that in all cases, we inherit the same
requirements, assumptions, and properties of the baseline
algorithm that we lazify. This includes applicable func-
tion classes, norm requirements, as well as smoothness and
(strong) convexity requirements. We also maintain identical
convergence rates up to (small!) constant factors.

Outline

We brieﬂy recall notation and notions in Section 2 and con-
sider conditional gradients algorithms in Section 3. In Sec-
tion 4 we explain how parameter-free variants of the pro-
posed algorithms can be obtained. Finally, in Section 5
we provide some experimental results. In the supplemental
material we consider two more variants of conditional gra-
dients algorithms (Sections B and C), we show that we can
realize a weak separation oracle with an even weaker oracle
in the case of combinatorial problem (Section D) and we
provide additional computational results (Section E).

2. Preliminaries
∗ denote
Let (cid:107)·(cid:107) be an arbitrary norm on Rn, and let (cid:107)·(cid:107)
the dual norm of (cid:107)·(cid:107). We will specify the applicable
norm in the later sections. A function f is L-Lipschitz
if |f (y) − f (x)| ≤ L(cid:107)y − x(cid:107) for all x, y ∈ dom f. A
convex function f is smooth with curvature at most C if
f (γy + (1 − γ)x) ≤ f (x) + γ∇f (x)(y − x) + Cγ2/2 for
all x, y ∈ dom f and 0 ≤ γ ≤ 1. A function f is S-strongly
2 (cid:107)y − x(cid:107)2 for
convex if f (y) − f (x) ≥ ∇f (x)(y − x) + S
all x, y ∈ dom f. Unless stated otherwise Lipschitz conti-
nuity and strong convexity will be measured in the norm
(cid:107)·(cid:107). Moreover, let Br (x) := {y | (cid:107)x − y(cid:107) ≤ r} be the ball
around x with radius r with respect to (cid:107).(cid:107). In the following,
P will denote the feasible region, a polytope and the vertices
of P will be denoted by v1, . . . , vN .

Lazifying Conditional Gradient Algorithms

3. Lazy Conditional Gradients
We start with the most basic Frank-Wolfe algorithm as a
simple example how a conditional gradient algorithm can
be laziﬁed by means of a weak separation oracle. We will
also use the basic variant to discuss various properties and
implications. We then show how the more complex Frank-
Wolfe algorithms in (Garber & Hazan, 2013) and (Garber &
Meshi, 2016) can be laziﬁed. Throughout this section (cid:107)·(cid:107)
denotes the (cid:96)2-norm.

3.1. Lazy Conditional Gradients: a basic example

We start with lazifying the original Frank-Wolfe algorithm
(arguably the simplest Conditional Gradients algorithm),
adapting the baseline argument from (Jaggi, 2013, Theo-
rem 1). While the vanilla version has suboptimal conver-
gence rate O(1/T ), its simplicity makes it an illustrative
example of the main idea of laziﬁcation. The lazy algo-
rithm (Algorithm 2) maintains an upper bound Φt on the
convergence rate, guiding its eagerness for progress when
searching for an improving vertex vt. If the oracle provides
an improving vertex vt we refer to this as a positive call and
we call it a negative call otherwise.

Algorithm 2 Lazy Conditional Gradients (LCG)
Input: smooth convex f function with curvature C, x1 ∈
P start vertex, LPsepP weak linear separation oracle,
accuracy K > 1, initial upper bound Φ0

2

1+ γt
K

Φt ← Φt−1+
vt ← LPsepP (∇f (xt), xt, Φt, K)
if vt = false then

Cγ2
t

Output: xt points in P
1: for t = 1 to T − 1 do
2:
3:
4:
5:
6:
7:
end if
8:
9: end for

else

xt+1 ← xt
xt+1 ← (1 − γt)xt + γtvt

The step size γt is chosen to (approximately) minimize Φt
in Line 2; roughly Φt−1/KC.
Theorem 3.1. Assume f is convex and smooth with cur-
vature C. Then Algorithm 2 with γt = 2(K2+1)
K(t+K2+2) has
convergence rate

f (xt) − f (x∗

) ≤

2 max{C, Φ0}(K 2 + 1)

,

t + K 2 + 2
where x∗ is a minimum point of f over P .
Proof. We prove by induction that f (xt) − f (x∗) ≤ Φt−1.
The claim is clear for t = 1 by the choice of Φ0. Assuming
the claim is true for t, we prove it for t + 1. We distin-

guish two cases depending on the return value of the weak
separation oracle in Line 3.
When the oracle returns an improving solution vt, which we
call the positive case, then ∇f (xt)(xt−vt) ≥ Φt/K, which
is used in the second inequality below. The ﬁrst inequality
follows by smoothness of f, and the third inequality by the
induction hypothesis:

f (xt+1) − f (x∗

)

≤ f (xt) − f (x∗

≤ f (xt) − f (x∗

Cγ2
t

2

) + γt∇f (xt)(vt − xt) +
Cγ2
t

) − γt

Φt
K
≤ Φt−1 − γt

+

Φt
K

2
Cγ2
t

2

+

= Φt,

When the oracle returns no improving solution, then in par-
ticular ∇f (xt)(xt − x∗) ≤ Φt, hence by Line 5 f (xt+1) −
f (x∗) = f (xt) − f (x∗) ≤ ∇f (xt)(xt − x∗) = Φt.
Finally, using the speciﬁc values of γt we prove the upper
bound

Φt−1 ≤

2 max{C, Φ0}(K 2 + 1)

t + K 2 + 2

by induction on t. The claim is obvious for t = 1. The in-
duction step is an easy computation relying on the deﬁnition
of Φt on Line 2:

Φt =

Φt−1 + Cγ2

t

2

1 + γt
K

≤

2 max{C,Φ0}(K2+1)

t+K2+2

+ max{C,Φ0}γ2

t

2

= 2 max{C, Φ0}(K 2 + 1)
2 max{C, Φ0}(K 2 + 1)

t + K 2 + 3

≤

(cid:0)1 + γt

1 + γt
K
1 + γt
2K

(cid:1) (t + K 2 + 2)

K

.

Here the second equation follows via plugging-in the choice
for γt for one of the γt in the quadratic term and last in-
equality follows from t ≥ 1 and the concrete choice of
γt.
Remark 3.2 (Discussion of the weak separation oracle). A
few remarks are in order:

(i) Interpretation of weak separation oracle. The weak
separation oracle provides new extreme points (or vertices)
vt that ensure necessary progress to converge at the proposed
rate Φt or it certiﬁes that we are already Φt-close to the
optimal solution. It is important to note that the two cases in
Oracle 1 are not mutually exclusive: the oracle might return
y ∈ P with c(x − y) > Φ/K (positive call: returning a
vertex y with improvement Φ/K), while still c(x − z) ≤ Φ
for all z ∈ P (negative call: certifying that there is no vertex

Lazifying Conditional Gradient Algorithms

z that can improve by Φ). This a desirable property as it
makes the separation problem much easier and the algorithm
works with either answer in the ambiguous case.

(ii) Choice of K. The K parameter can be used to bias
the oracle towards positive calls, i.e., returning improving
directions. We would also like to point out that the algorithm
above as well as those below will also work for K = 1,
however we show in supplemental material (Section D)
that we can use an even weaker oracle to realize a weak
separation oracle if K > 1 and for consistency, we require
K > 1 throughout. In the case K = 1 the two cases in the
oracle are mutually exclusive.

(iii) Effect of caching and early termination. When realiz-
ing the weak separation oracle, the actual linear optimiza-
tion oracle has to be only called if none of the previously
seen vertices (or atoms) satisﬁes the separation condition.
Moreover, the weak separation oracle has to only produce
a satisfactory solution and not an approximately optimal
one. These two properties are responsible for the observed
speedup (see Figure 1). Moreover, the convex combinations
of vertices of P that represent the solutions xt are extremely
sparse as we reuse (cached) vertices whenever possible.

(iv) Dual certiﬁcates. By not computing an approximately
optimal solution, we give up dual optimality certiﬁcates. For
a given point x ∈ P , let g(x) := maxv∈P ∇f (x)(x − v)
denote the Wolfe gap. We have f (x) − f (x∗) ≤ g(x)
where x∗ = argminx∈P f (x) by convexity.
In those
rounds t where we obtain an improving vertex we have
no information about g(xt). However, if the oracle re-
turns false in round t, then we obtain the dual certiﬁcate
f (xt) − f (x∗) ≤ g(xt) ≤ Φt.
(v) Rate of convergence. A close inspection of the algo-
rithm utilizing the weak separation oracle suggests that the
algorithm converges only at the worst-case convergence rate
that we propose with the Φt sequence. This however is only
an artefact of the simpliﬁed presentation for the proof of
the worst-case rate. We can easily adjust the algorithm to
implicitly perform a search over the rate Φt combined with
line search for γ. This leads to a parameter-free variant of
Algorithm 2as given in Section 4 and comes at the expense
of a (small!) constant factor deterioration of the worst-case
rate guarantee; see also Supplementary Material A.(iii) for
an in-depth discussion.

We discuss potential implementation improvements in Sup-
plementary Material A.

3.2. Lazy Pairwise Conditional Gradients

In this section we provide a lazy variant (Algorithm 3) of
the Pairwise Conditional Gradient algorithm from (Garber

& Meshi, 2016), using separation instead of linear optimiza-
tion. We make identical assumptions: the feasible region
is a 0/1 polytope given in the form P = {x ∈ Rn | 0 ≤
x ≤ 1, Ax = b}, where 1 denotes the all-one vector of
compatible dimension; in particular all vertices of P have
only 0/1 entries.

Algorithm 3 Lazy Pairwise Conditional Gradients (LPCG)
Input: polytope P , smooth and S-strongly convex function
f with curvature C, accuracy K > 1, ηt non-increasing
step-sizes

if (xt)i > 0
if (xt)i = 0

4:

5:

(cid:40)

Output: xt points
1: x1 ∈ P arbitrary and Φ0 ≥ f (x1) − f (x∗)
2: for t = 1, . . . , T do
3:

deﬁne ˜∇f (xt) ∈ Rm as follows:
∇f (xt)i
−∞

˜∇f (xt)i :=
(cid:17)
(cid:16)
Φt ← 2Φt−1+η2
(cid:16)
∇f (xt),− ˜∇f (xt)
ct ←
t , v−
(v+
6:
t ) ← LPsepP×P
t , v−
if (v+
t ) = false then
7:
8:
xt+1 ← xt
else
9:
˜ηt ← max{2−δ | δ ∈ Z≥0, 2−δ ≤ ηt}
10:
xt+1 ← xt + ˜ηt(v+
11:
end if
12:
13: end for

ct, (xt, xt), Φt
∆t

t − v−

2+ ηt
K∆t

t )

t C

(cid:17)

, K

Observe that Algorithm 3 calls LPsep on the cartesian prod-
uct of P with itself. Choosing the objective function as
in Line 5 allows us to simultaneously ﬁnd an improving
direction and an away-step direction.
(cid:113) S
Theorem 3.3. Let x∗ be a minimum point of f in P , and Φ0
an upper bound of f (x1) − f (x∗). Furthermore, let M1 :=
ηt := κ(cid:112)Φt−1 and ∆t :=
, 1/√Φ0},
8 card(x∗) , M2 := KC/2, κ := min{ M1
(cid:19)t

(cid:113) 2 card(x∗)Φt−1
(cid:18) 1 + B

rithm 3 has convergence rate

, then Algo-

2M2

S

f (xt+1) − f (x∗

) ≤ Φt ≤ Φ0

1 + 2B

,

where B := κ · M1
2K .
We recall a technical lemma for the proof.
(cid:80)k
i=1 λivi and y =(cid:80)k
Lemma 3.4 ((Garber & Meshi, 2016, Lemma 2)). Let
x, y ∈ P . There exists vertices vi of P such that x =
γi ∈ [0, λi], z ∈ P and(cid:80)k
z with

(cid:17)
card(y)(cid:107)x − y(cid:107).

i=1 (λi − γi) vi +

(cid:16)(cid:80)k

i=1 γi ≤

(cid:112)

i=1 γi

Lazifying Conditional Gradient Algorithms

Proof of Theorem 3.3. The feasibility of the iterates xt is
ensured by Line 10 and the monotonicity of the sequence
{ηt}t≥1 with the same argument as in (Garber & Meshi,
2016, Lemma 1 and Observation 2).
We ﬁrst show by induction that f (xt+1) − f (x∗) ≤ Φt. For
t = 0 we have Φ0 ≥ f (x1) − f (x∗). Now assume the state-
ment for some t ≥ 0. In the negative case (Line 8), we use
the guarantee of Oracle 1 to get ct((xt, xt)−(z1, z2)) ≤ Φt
for all z1, z2 ∈ P , which is equivalent to (as ct(xt, xt) = 0)
˜∇f (xt)z2 − ∇f (xt)z1 ≤ Φt

and therefore

∆t

∆t

Φt
∆t

,

∇f (xt)(˜z2 − z1) ≤

S

(cid:107) ≤

(cid:112)

card(x∗)(cid:107)xt − x∗

ther use Lemma 3.4 to write xt = (cid:80)k
i=1(λi − γi)vi +(cid:80)k
(cid:80)k
for all ˜z2, z1 ∈ P with supp(˜z2) ⊆ supp(xt). We fur-
i=1 λivi and x∗ =
(cid:113) 2 card(x∗)Φt−1
(cid:80)k
i=1 γiz with γi ∈ [0, λi], z ∈ P and
i=1 γi ≤
=
f (x∗) ≤ ∇f (xt)(xt − x∗) =(cid:80)k
∆t, using the induction hypothesis and the strong convexity
in the second inequality. Then f (xt+1) − f (x∗) = f (xt) −
i=1 γi(vi − z)·∇f (xt) ≤
Φt, where we used Equation 3.2 for the last inequality.
For the positive case (Lines 10 and 11) we get, using ﬁrst
smoothness of f, then ηt/2 < ˜ηt ≤ ηt and ∇f (xt)(v+
t −
v−
t ) ≤ −Φt/(∆tK), and ﬁnally the deﬁnition of Φt:
f (xt+1) − f (x∗
t )) − f (x∗
)
t − v−

t − v−
) = f (xt + ˜ηt(v+
≤ Φt−1 + ˜ηt∇f (xt)(v+
η2
t C
≤ Φt−1 −
+
2

Φt
∆tK

˜η2
t C
2

ηt
2 ·

= Φt.

t ) +

Plugging in the values of ηt and ∆t to the deﬁnition of Φt
gives the desired bound.

Φt =

2Φt−1 + η2

t C

2 + ηt
K∆t
1 + B
1 + 2B ≤ Φ0

≤ Φt−1

= Φt−1

1 + κ2M2/K
1 + κM1/K

(cid:18) 1 + B

(cid:19)t

.

1 + 2B

4. Parameter-free Conditional Gradients via

Weak Separation

We now provide a parameter-free variant of the Lazy Frank-
Wolfe Algorithm. We stress that the worst-case convergence
rate is identical up to a small constant factor. Here we ﬁnd a
tight initial bound Φ0 with a single extra LP call, which can
be also done approximately as long as Φ0 is a valid upper
bound. Alternatively, one can perform binary search via the
weak separation oracle as described earlier.
Note that the accuracy parameter K in Algorithm 4 is a
parameter of the oracle and not of the algorithm itself. We

{Initial bound}

Algorithm 4 Parameter-free Lazy Conditional Gradients
(LCG)
Input: smooth convex function f, x1 ∈ P start vertex,
LPsepP weak linear separation oracle, accuracy K > 1
Output: xt points in P
1: Φ0 ← maxx∈P ∇f (x1)(x1 − x)/2
2: for t = 1 to T − 1 do
vt ← LPsepP (∇f (xt), xt, Φt−1, K)
3:
if vt = false then
4:
5:
xt+1 ← xt
Φt ← Φt−1
6:
else
7:
8:
γt ← argmin0≤γ≤1 f ((1 − γ)xt + γvt)
9:
xt+1 ← (1 − γt)xt + γtvt
Φt ← Φt−1
10:
end if
11:
12: end for

{Update iterate}

{Update Φ}

2

will show now that Algorithm 4 converges in the worst-case
at a rate identical to Algorithm 2 (up to a small constant
factor).
Theorem 4.1. Let f be a smooth convex function with cur-
vature C. Algorithm 4 converges at a rate proportional to
1/t. In particular to achieve a bound f (xt) − f (x∗) ≤ ε,
given an initial upper bound f (x1) − f (x∗) ≤ 2Φ0, the

number of required steps is upper bounded by

t ≤ (cid:100)log Φ0/ε(cid:101) + 1 + 4K(cid:100)log Φ0/KC(cid:101) +

16K 2C

ε

.

Proof. The main idea of the proof is that while negative
answers to oracle calls halve the dual upper bound 2Φt,
positive oracle calls signiﬁcantly decrease the function value
of the current point.
We analyze iteration t of the algorithm. If Oracle 1 in Line 3
returns a negative answer (i.e., false, case (2)), then this
guarantees ∇f (xt)(xt − x) ≤ Φt−1 for all x ∈ P , in
particular, using convexity, f (xt+1) − f (x∗) = f (xt) −
f (x∗) ≤ ∇f (xt)(xt − x∗) ≤ Φt−1 = 2Φt.
If Oracle 1 returns a positive answer (case (1)), then we have
t by smoothness
f (xt) − f (xt+1) ≥ γtΦt−1/K − (C/2)γ2
of f. By minimality of γt, therefore f (xt) − f (xt+1) ≥
min0≤γ≤1(γΦt−1/K−(C/2)γ2), which is Φ2
t−1/(2CK 2)
if Φt−1 < KC, and Φt−1/K − C/2 ≥ C
2 if Φt−1 ≥ KC.
Now we bound the number t(cid:48) of consecutive positive oracle
calls immediately following an iteration t with a negative
oracle call. Note that the same argument bounds the number
of initial consecutive positive oracle calls with the choice
t = 0, as we only use f (xt+1) − f (x∗) ≤ 2Φt below.

Lazifying Conditional Gradient Algorithms

t+t(cid:48)(cid:88)

Note that Φt = Φt+1 = ··· = Φt+t(cid:48). Therefore
2Φt ≥ f (xt+1) − f (x∗
(cid:17)

t(cid:48) Φ2
t(cid:48)(cid:16) Φt−1

τ =t+1
if Φt−1 < KC
if Φt−1 ≥ KC

K − C

2

) ≥

t

2CK2

≥

,

(f (xτ ) − f (xτ +1))

Figure 1. Performance gain due to caching and early termination
for stochastic optimization over a maximum cut problem with
linear losses. The red line is the OCG baseline, the green one is
the lazy variant using only early termination, and the blue one
uses caching and early termination. Left: loss vs. wall-clock time.
Right: loss vs. total time spent in oracle calls. Time limit was
7200 seconds. Caching allows for a signiﬁcant improvement in
loss reduction in wall-clock time. The effect is even more obvious
in oracle time as caching cuts out a large number of oracle calls.

5. Experiments
As mentioned before, lazy algorithms have two improve-
ments: caching and early termination. Here we depict the
effect of caching in Figure 1, comparing OCG (no caching,
no early termination), LOCG (caching and early termina-
tion) and LOCG (only early termination) (see Algorithm 7).
We did not include a caching-only OCG variant, because
caching without early termination does not make much
sense: in each iteration a new linear optimization problem
has to be solved; previous solutions can hardly be reused as
they are unlikely to be optimal for the new linear optimiza-
tion problem.

5.1. Effect of K

If the parameter K of the oracle can be chosen, which
depends on the actual oracle implementation, then we can
increase K to bias the algorithm towards performing more
positive calls. At the same time the steps get shorter. As
such there is a natural trade-off between the cost of many
positive calls vs. a negative call. We depict the impact of
the parameter choice for K in Figure 6.

which gives in the case Φt < KC that t(cid:48)
in the case Φt ≥ KC that

≤ 4CK 2/Φt, and

t(cid:48)

≤

2Φt
K − C

Φt

2

=

4KΦt

2Φt − KC ≤

4KΦt
2Φt − Φt

= 4K.

Thus iteration t is followed by at most 4K consecutive
positive oracle calls as long as Φt ≥ KC, and 4CK 2/Φt <
2(cid:96)+1 · 4K ones for 2−(cid:96)−1KC < Φt ≤ 2−(cid:96)KC with (cid:96) ≥ 0.
Adding up the number of oracle calls gives the desired rate:
in addition to the positive oracle calls we also have at most
(cid:100)log(Φ0/ε)(cid:101) + 1 negative oracle calls, where log(·) is the
binary logarithm and ε is the (additive) accuracy. Thus after
a total of

(cid:100)log Φ0/ε(cid:101)+1+4K(cid:100)log Φ0/KC(cid:101)+

(cid:100)log KC/ε(cid:101)(cid:88)

(cid:96)=0

2(cid:96)+1·4K
16K 2C

≤ (cid:100)log Φ0/ε(cid:101) + 1 + 4K(cid:100)log Φ0/KC(cid:101) +

ε

iterations (or equivalently oracle calls) we have f (xt) −
f (x∗) ≤ ε.
Remark 4.2. Observe that Algorithm 4 might converge
much faster due to the aggressive halving of the rate. In
fact, Algorithm 4 convergences at a rate that is at most a
factor 4K 2 slower than the rate that the vanilla (non-lazy)
Frank-Wolfe algorithm would realize for the same problem.
In actual wall-clock time Algorithm 4 is much faster though
due to the use of the weaker oracle; see Figure 2 and 4
for a comparison and Section E.1.2 for more experimental
results.

Negative oracle calls tend to be signiﬁcantly more expen-
sive time-wise than positive oracle calls due to proving dual
bounds. The following corollary is an immediate conse-
quence of the argumentation from above:
Corollary 4.3. Algorithm 4 makes at most (cid:100)log Φ0/ε(cid:101) + 1
negative oracle calls.

If line search is too expensive we can choose γt =
min(1, Φt/KC) in Algorithm 4. In this case an estimate of
the curvature C is required, though no explicit knowledge
of the sequence Φt is needed as compared to the textbook
variant in Section 3.1.

02000400060008000Wall-clock time0.00.20.40.60.81.0LossLOCG with cacheLOCG without cacheOCG02000400060008000Oracle time0.00.20.40.60.81.0LossLOCG with cacheLOCG without cacheOCGLazifying Conditional Gradient Algorithms

Figure 2. Performance on an instance of the video colocalization
problem. We solve quadratic minimization over a ﬂow polytope
and report the achieved dual bound (or Wolfe-gap) over wall-clock
time in seconds in logscale on the left and over the number of actual
LP calls on the right. We used the parameter-free variant of the
Lazy CG algorithm, which performs in both measures signiﬁcantly
better than the non-lazy counterpart. The performance difference
is more prominent in the number of LP calls.

Figure 3. Performance on a large instance of the video colocal-
ization problem using PCG and its lazy variant. We observe that
lazy PCG is signiﬁcantly better both in terms of function value and
dual bound. Recall that the function value is normalized between
[0, 1].

Figure 5. Performance of the two laziﬁed variants LOCG (left
column) and LPCG (right column). The feasible regions are a
cut polytope on the left and the MIPLIB instance air04 on the
right. The objective functions are in both cases quadratic, on the
left randomly chosen in every step. We show the performance
over wall clock time in seconds (ﬁrst row) and over iterations
(second row). The last row shows the number of call to the linear
optimization oracle. The laziﬁed versions perform signiﬁcantly
better in wall clock time compared to the non-lazy counterparts.

Figure 4. Performance on a matrix completion instance. More
information about this problem can be found in the supplemental
material (Section E). The performance is reported as the objective
function value over wall-clock time in seconds on the left and over
LP calls on the right. In both measures after an initial phase the
function value using LCG is much lower than with the non-lazy
algorithm.

Figure 6. Impact of the oracle approximation parameter K de-
picted for the Lazy CG algorithm. We can see that increasing
K leads to a deterioration of progress in iterations but improves
performance in wall-clock time. The behavior is similar for other
algorithms.

0150300450600Wallclocktime(s)106107108109101010111012101310141015DualboundLCGCG0100200300400LPcalls106107108109101010111012101310141015DualboundLCGCG03006009001200Wall-clock time (s)0.00.20.40.60.81.0Function valueLPCG PCG 03006009001200Wall-clock time (s)103104105Dual boundLPCG PCG 0150300450Wallclocktime(s)105106107108FunctionvalueLCGCG0102030LPcalls105106107108FunctionvalueLCGCG02000400060008000Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 01000200030004000Wall-clock time (s)0.00.20.40.60.81.0Function valueLPCG PCG 080001600024000Iterations0.00.20.40.60.81.0LossLOCG OCG 0.00.51.01.52.0Iterations1e60.00.20.40.60.81.0Function valueLPCG PCG 080001600024000Iterations050100150200250300Number of LP callsLOCG 02468Iterations1e5051015Number of LP callsLPCG 0150300450Wallclocktime(s)101102FunctionvalueLCGK=1LCGK=1.5LCGK=5LCGK=10LCGK=50LCGK=1000200400600Iterations101102FunctionvalueLCGK=1LCGK=1.5LCGK=5LCGK=10LCGK=50LCGK=100Lazifying Conditional Gradient Algorithms

Acknowledgements
We are indebted to Alexandre D’Aspremont, Simon Lacoste-
Julien, and George Lan for the helpful discussions and for
providing us with relevant references. Research reported in
this paper was partially supported by NSF CAREER award
CMMI-1452463.

References
Achterberg, Tobias, Koch, Thorsten,

and Martin,
Alexander. MIPLIB 2003. Operations Research
Letters, 34(4):361–372, 2006.
10.1016/
j.orl.2005.07.009.
URL http://www.zib.de/
Publications/abstracts/ZR-05-28/.

doi:

Audibert, Jean-Yves, Bubeck, S´ebastien, and Lugosi, G´abor.
Regret in online combinatorial optimization. Mathemat-
ics of Operations Research, 39(1):31–45, 2013.

Bodic, Pierre Le, Pavelka, Jeffrey W, Pfetsch, Marc E, and
Pokutta, Sebastian. Solving MIPs via scaling-based aug-
mentation. arXiv preprint arXiv:1509.03206, 2015.

Cohen, Alon and Hazan, Tamir. Following the perturbed
leader for online structured learning. In Proceedings of
the 32nd International Conference on Machine Learning
(ICML-15), pp. 1034–1042, 2015.

Dash, Sanjeeb. A note on QUBO instances deﬁned on

Chimera graphs. preprint arXiv:1306.1202, 2013.

Frank, Andr´as and Tardos, ´Eva. An application of simulta-
neous Diophantine approximation in combinatorial opti-
mization. Combinatorica, 7(1):49–65, 1987.

Frank, Marguerite and Wolfe, Philip. An algorithm for
quadratic programming. Naval research logistics quar-
terly, 3(1-2):95–110, 1956.

Freund, Robert M. and Grigas, Paul. New analysis and
results for the frank–wolfe method. Mathematical Pro-
gramming, 155(1):199–230, 2016. ISSN 1436-4646. doi:
10.1007/s10107-014-0841-6. URL http://dx.doi.
org/10.1007/s10107-014-0841-6.

Garber, Dan and Hazan, Elad. A linearly convergent condi-
tional gradient algorithm with applications to online and
stochastic optimization. arXiv preprint arXiv:1301.4666,
2013.

Garber, Dan and Meshi, Ofer.

Linear-memory and
decomposition-invariant linearly convergent conditional
arXiv
gradient algorithm for structured polytopes.
preprint, arXiv:1605.06492v1, May 2016.

Gr¨otschel, Martin and Lov´asz, L´aszlo. Combinatorial opti-

mization: A survey, 1993.

Gupta, Swati, Goemans, Michel, and Jaillet, Patrick. Solv-
ing combinatorial games using products, projections
and lexicographically optimal bases. arXiv preprint
arXiv:1603.00522, 2016.

Gurobi Optimization. Gurobi optimizer reference man-
ual version 6.5, 2016. URL https://www.gurobi.
com/documentation/6.5/refman/.

Hazan, Elad.

Introduction to online convex optimiza-
tion. Foundations and Trends in Optimization, 2(3–
4):157–325, 2016. doi: 10.1561/2400000013. URL
http://ocobook.cs.princeton.edu/.

Hazan, Elad and Kale, Satyen. Projection-free online learn-

ing. arXiv preprint arXiv:1206.4657, 2012.

Jaggi, Martin. Revisiting Frank–Wolfe: Projection-free
sparse convex optimization. In Proceedings of the 30th
International Conference on Machine Learning (ICML-
13), pp. 427–435, 2013.

Joachims, Thorsten, Finley, Thomas, and Yu, Chun-
Nam John. Cutting-plane training of structural svms.
Machine Learning, 77(1):27–59, 2009.

Joulin, Armand, Tang, Kevin, and Fei-Fei, Li. Efﬁcient
image and video co-localization with frank-wolfe algo-
rithm. In European Conference on Computer Vision, pp.
253–268. Springer, 2014.

Kalai, Adam and Vempala, Santosh. Efﬁcient algorithms
for online decision problems. Journal of Computer and
System Sciences, 71(3):291–307, 2005.

Koch, Thorsten, Achterberg, Tobias, Andersen, Erling,
Bastert, Oliver, Berthold, Timo, Bixby, Robert E., Danna,
Emilie, Gamrath, Gerald, Gleixner, Ambros M., Heinz,
Stefan, Lodi, Andrea, Mittelmann, Hans, Ralphs, Ted,
Salvagnin, Domenico, Steffy, Daniel E., and Wolter, Kati.
MIPLIB 2010. Mathematical Programming Computa-
tion, 3(2):103–163, 2011. doi: 10.1007/s12532-011-
0025-9. URL http://mpc.zib.de/index.php/
MPC/article/view/56/28.

Lacoste-Julien, Simon and Jaggi, Martin. On the global
convergence of Frank–Wolfe optimization
linear
variants.
In Cortes, C., Lawrence, N. D., Lee, D. D.,
Sugiyama, M., and Garnett, R. (eds.), Advances in
Neural Information Processing Systems, volume 28, pp.
496–504. Curran Associates, Inc., 2015. URL http:
//papers.nips.cc/paper/5925-on-the-
global-linear-convergence-of-frank-
wolfe-optimization-variants.pdf.

Lacoste-Julien, Simon, Jaggi, Martin, Schmidt, Mark, and
Pletscher, Patrick. Block-coordinate frank-wolfe opti-
mization for structural svms. In ICML 2013 International
Conference on Machine Learning, pp. 53–61, 2013.

Lazifying Conditional Gradient Algorithms

Lan, Guanghui and Zhou, Yi. Conditional gradient sliding
for convex optimization. Optimization-Online preprint
(4605), 2014.

Levitin, Evgeny S and Polyak, Boris T. Constrained min-
imization methods. USSR Computational mathematics
and mathematical physics, 6(5):1–50, 1966.

Neu, Gergely and Bart´ok, G´abor. An efﬁcient algorithm
for learning with semi-bandit feedback. In Algorithmic
Learning Theory, pp. 234–248. Springer, 2013.

Oertel, Timm, Wagner, Christian, and Weismantel, Robert.
Integer convex minimization by mixed integer linear opti-
mization. Oper. Res. Lett., 42(6-7):424–428, 2014.

Osokin, Anton, Alayrac, Jean-Baptiste, Lukasewitz, Is-
abella, Dokania, Puneet K, and Lacoste-Julien, Simon.
Minding the gaps for block frank-wolfe optimization of
structured svms. ICML 2016 International Conference
on Machine Learning / arXiv preprint arXiv:1605.09346,
2016.

Schulz, Andreas S and Weismantel, Robert. The complexity
of generic primal algorithms for solving general integer
programs. Mathematics of Operations Research, 27(4):
681–692, 2002.

Schulz, Andreas S., Weismantel, Robert, and Ziegler,
G¨unter M. 0/1-integer programming: Optimization and
augmentation are equivalent. In Algorithms – ESA ’95,
Proceedings, pp. 473–483, 1995.

Shah, Neel, Kolmogorov, Vladimir,

and Lampert,
Christoph H. A multi-plane block-coordinate frank-wolfe
algorithm for training structural svms with a costly max-
oracle. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pp. 2737–2745,
2015.

Lazifying Conditional Gradient Algorithms

A. Implementation Improvements
Note that there are various obvious improvements to Algorithm 2 for actual implementations. These improvements do
not affect the theoretical (worst-case) performance and for the sake of clarity of the exposition we did not include them in
Algorithm 2.

(i) First of all, we can improve the update of Φt, using the actual gap closed, rather than the pessimistic update via the lower
bound on gap closed, i.e., we can update Φt ← Φt − (f (xt) − f (xt+1)), whenever we calculated a new point xt+1.
(ii) Moreover, we can better utilize information from negative oracle calls (i.e., when the oracle returns false): if the oracle
utilizes linear optimization at its core, then a negative oracle call will certify ∇f (xt)(xt − v) ≤ Φt via maximizing ∇f (xt)v
with v ∈ P , i.e., the linear optimization oracle computes g(xt) and we can reset Φt ← g(xt). If v∗ realizes the Wolfe
gap, which is obtained as a byproduct of the above linear maximization, we can further use v∗ to make a step: rather than
executing line 5, we can execute line 7 with vt = v∗. By doing so we maximize the use of information obtained from a
negative oracle call.
(iii) Finally, we can optimize the management of Φt. To obtain a better upper bound Φ0, we can solve v∗ :=
argmaxv∈P ∇f (x1)v at the expense of one extra LP call and set Φ0 := ∇f (x1)(x1 − v∗) = g(x1). Alternatively,
we can perform binary search over Φ0 until the weak separation oracle produces an actual step. If ¯Φ is the value of the
search for which we observe the ﬁrst step, we can reset Φ0 := 2 ¯Φ and we have f (x1) − f (x∗) ≤ g(x1) ≤ 2 ¯Φ.
Furthermore, we can change the strategy for managing Φt as follows: we keep the value of Φt ﬁxed in line 2 and perform
line search for γ. Whenever, we observe a negative oracle call, we set the current Φt to 1
2 g(xt) obtained from the negative
call. As such, we ensure Φt < g(xt) ≤ 2Φt, which biases the algorithm towards (much cheaper) positive calls. Convergence
is ensured by observing that an LPsepP (·,·, Φ/2, K) oracle is an LPsepP (·,·, Φ, 2K) oracle for which the theorem directly
applies. With this strategy we maintain the same theoretical (worst-case) convergence up to a constant factor, however in
case a faster convergence is possible, we adapt to that rate. The parameter-free version in Section 4 utilizes this technique.

B. Lazy Local Conditional Gradients
In this section we provide a lazy version (Algorithm 5) of the conditional gradient algorithm from (Garber & Hazan, 2013).
Let P ⊆ Rn be any polytope, D denote an upper bound on the (cid:96)2-diameter of P , and µ ≥ 1 be the afﬁne invariant of
P from (Garber & Hazan, 2013). As the algorithm is not afﬁne invariant by nature, we need a non-invariant version of
smoothness: Recall that a convex function f is β-smooth if

f (y) − f (x) ≤ ∇f (x)(y − x) + β(cid:107)y − x(cid:107)2/2.

Algorithm 5 Lazy Local Conditional Gradients (LLCG)
Input: feasible polytope P , β-smooth and S-strongly convex function f, parameters K, S, β, µ; diameter D
Output: xt points
1: x1 ∈ P arbitrary and Φ0 ≥ f (x1) − f (x∗)
2: α ← S
3: for t = 1, . . . , T do
4:

t ,D2}

2Kβnµ2

1+α/K

2 α2 min{nµ2r2

(cid:113) 2Φt−1
Φt ← Φt−1+ β
rt ←
pt ← LLPsepP (∇f (xt), xt, rt, Φt, K)
if pt = false then

xt+1 ← xt
xt+1 ← xt + α(pt − xt)

S

5:
6:
7:
8:
9:
10:
end if
11:
12: end for

else

As an intermediary step, we ﬁrst implement a local weak separation oracle in Algorithm 6, a local version of Oracle 1,
analogously to the local linear optimization oracle in (Garber & Hazan, 2013). To this end, we recall a technical lemma
from (Garber & Hazan, 2013).

Lazifying Conditional Gradient Algorithms

nµ
D r, 1

Algorithm 6 Weak Local Separation LLPsepP (c, x, r, Φ, K)
(cid:110)√
(cid:111)
Input: c ∈ Rn linear objective, x ∈ P point, r > 0 radius, Φ > 0 objective value
Output: Either (1) y ∈ P with (cid:107)x− y(cid:107) ≤ √nµr and c(x− y) > Φ/K, or (2) false: c(x− z) ≤ Φ for all z ∈ P ∩ Br (x).
j=1 λjvj, λj > 0,(cid:80)
2: Decompose x: x =(cid:80)M
1: ∆ ← min
4: k ← min{k :(cid:80)k
(cid:17)
(cid:16)
(cid:80)k−1
(cid:1)
(cid:0)c, p−

3: Sort vertices: i1, . . . , iM cvi1 ≥ ··· ≥ cviM .

j=1 λij ≥ ∆}
∆ −

(cid:80)k−1

j λj = 1.

j=1 λij

vik

∆ , Φ

j=1 λij vij +

5: p− ←
6: v∗
7: if v∗ = false then
8:
9: else
10:
11: end if

← LPsepP
return false
return y ← x − p− + ∆v∗

∆

and x =(cid:80)N
Lemma B.1. (Garber & Hazan, 2013, Lemma 7) Let P ⊆ Rn be a polytope and v1, . . . , vN be its vertices. Let x, y ∈ P
i=1 λivi a convex combination of the vertices of P . Then there are numbers 0 ≤ γi ≤ λi and z ∈ P satisfying

(cid:88)

i∈[N ]

 z

γi

(cid:88)

γivi +

i∈[N ]
√nµ
D (cid:107)x − y(cid:107).

y − x = −

(cid:88)

i∈[N ]

γi ≤

Now we prove the correctness of the weak local separation algorithm.
Lemma B.2. Algorithm 6 is correct. In particular LLPsepP (c, x, r, Φ, K)
(i) returns either an y ∈ P with (cid:107)x − y(cid:107) ≤ √nµr and c(x − y) > Φ/K,
(ii) or establishes c(x − z) ≤ Φ for all z ∈ P ∩ Br (x).
j=1(λij − γj)vij + ∆v∗ with ∆ =(cid:80)M
of vertices of P . Moreover by construction of y we can write y =(cid:80)M
Proof. We ﬁrst consider the case when the algorithm exits in Line 10. Observe that y ∈ P since y is a convex combination
√
D r.
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≤
Therefore
M(cid:88)
γj(cid:107)vij − v∗
≤ √nµr.
(cid:16) p−
∆ − v∗(cid:17)
(cid:32) N(cid:88)

If the algorithm exits in Line 8, we use Lemma B.1 to decompose any y ∈ P ∩ Br (x) in the following way:

Finally using the guarantee of LPsepP we get

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) M(cid:88)

γjvij − ∆v∗

c(x − y) = ∆c

(cid:107)x − y(cid:107) =

j=1 γj ≤

N(cid:88)

(cid:33)

Φ
K

≥

j=1

j=1

(cid:107)

nµ

.

y =

(λi − γi)vi +

i=1

γi

z,

i=1

with z ∈ P and(cid:80)N
(cid:80)N
i=1 η−

i = ∆. Let

i=1 γi ≤

√

D (cid:107)x − y(cid:107) ≤ ∆. Since(cid:80)N

nµ

Lazifying Conditional Gradient Algorithms

i=1 λi = 1 ≥ ∆, there are numbers γi ≤ η−

i ≤ λi with

N(cid:88)

˜p− :=

η−
i vi,

i=1

˜p+ := y − x + ˜p− =

N(cid:88)

i=1

(η−

i − γi)vi +

N(cid:88)

i=1

γiz,

for all u =(cid:80)N

i=1 ηivi with(cid:80)N

so that ˜p+/∆ ∈ P . To bound the function value we ﬁrst observe that the choice of p− in the algorithm assures that cu ≤ cp−
i=1 ηi = ∆ and all ηi ≥ 0. In particular, c˜p− ≤ cp−. The function value of the positive part
˜p+ can be bounded with the guarantee of LPsepP :

∆ −
i.e., c(p− − ˜p+) ≤ Φ. Finally combining these bounds gives

c

(cid:18) p−

(cid:19)

˜p+
∆

Φ
∆

,

≤

as desired.

c(x − y) = c (˜p− − ˜p+) ≤ c(p− − ˜p+) ≤ Φ

We are ready to examine the Conditional Gradient Algorithm based on LLPsepP :
Theorem B.3. Algorithm 5 converges with the following rate:

f (xt+1) − f (x∗

) ≤ Φt ≤ Φ0

(cid:18) 1 + α/(2K)

(cid:19)t

1 + α/K

.

Proof. The proof is similar to the proof of Theorem 3.3. We prove this rate by induction. For t = 0 the choice of Φ0
guarantees that f (x1) − f (x∗) ≤ Φ0. Now assume the theorem holds for t ≥ 0. With strong convexity and the induction
hypothesis we get

(cid:107)xt − x∗

(cid:107)2 ≤

2
S

(f (xt) − f (x∗

)) ≤

2
S

Φt−1 = r2
t ,

i.e., x∗

∈ P ∩ Brt (xt). In the negative case, i.e., when pt = false, then case (ii) of Lemma B.2 applies:

f (xt+1) − f (x∗

) = f (xt) − f (x∗

) ≤ ∇f (xt)(xt − x∗

) ≤ Φt.

In the positive case, i.e., when Line 10 is executed, we get the same inequality via:

f (xt+1) − f (x∗

) ≤ Φt−1 + α∇f (xt)(pt − xt) +

β
2

α2(cid:107)x − pt(cid:107)2 ≤ Φt−1 − α

Φt
K

+

β
2

α2 min{nµ2r2

t , D2} = Φt.

Therefore using the deﬁnition of α and rt we get the desired bound:

Φt−1 + β

2 α2r2
1 + α/K

t nµ2

Φt ≤

= Φt−1

(cid:18) 1 + α/(2K)

(cid:19)

1 + α/K

(cid:18) 1 + α/(2K)

(cid:19)t

≤ Φ0

1 + α/K

.

C. Lazy Online Conditional Gradients
In this section we lazify the online conditional gradient algorithm of (Hazan & Kale, 2012) over arbitrary polytopes
P = {x ∈ Rn | Ax ≤ b}, resulting in Algorithm 7. We slightly improve constant factors by replacing (Hazan & Kale, 2012,
Lemma 3.1) with a better estimation via solving a quadratic inequality arising from strong convexity. In this section the
norm (cid:107)·(cid:107) can be arbitrary.
Theorem C.1. Let 0 ≤ b, s < 1. Let K > 1 be an accuracy parameter. Assume ft is L-Lipschitz, and smooth with
curvature at most Ct−b. Let D := maxy1,y2∈P(cid:107)y1 − y2(cid:107) denote the diameter of P in norm (cid:107)·(cid:107). Then the following hold for
the points xt computed by Algorithm 7 where x∗

T is the minimizer of(cid:80)T

t=1 ft:

Lazifying Conditional Gradient Algorithms

Algorithm 7 Lazy Online Conditional Gradients (LOCG)
Input: ft functions, x1 ∈ P start vertex, LPsepP weak linear separation oracle, parameters K, C, b, S, s; diameter D
Output: xt points
1: for t = 1 to T − 1 do
2: ∇t ← ∇ft(xt)
if t = 1 then
3:
4:
5:

∗
h1 ← min{(cid:107)∇1(cid:107)

else

6:

7:
8:

9:

(cid:114)(cid:107)∇t(cid:107)∗ 2

D, 2(cid:107)∇1(cid:107)

∗2 /S}
(cid:17)
2St1−s + Φt−1

(cid:16)(cid:107)∇t(cid:107)∗ 2
(cid:110)

2St1−s

Kt ←
ht ← Φt−1 + min

∗

D,

(cid:107)∇t(cid:107)∗ 2
St1−s + 2Kt

(cid:107)∇t(cid:107)

(cid:111)

i=1 ∇fi(xt), xt, Φt, K)

ht+

1+ γt
K

Ct1−b γ2
t
2(1−b)

end if
Φt ←
if vt = false then

vt ← LPsepP ((cid:80)t
(cid:80)t
i=1 fi(xt) +(cid:80)t

xt+1 ← xt
xt+1 ← (1 − γt)xt + γtvt
Φt ← ht −

else

10:
11:
12:
13:
14:
15:
end if
16:
17: end for

i=1 fi(xt+1)

(i) With the choice

the xt satisfy

where

γt = t−(1−b)/2,

(ft(xT ) − ft(x∗

T )) ≤ AT −(1−b)/2,

T(cid:88)

t=1

1
T

A :=

+ L(K + 1)D.

CK
2(1 − b)

γt = t(b+s−2)/3,

(ii) Moreover, if all the ft are St−s-strongly convex, then with the choice

the xt satisfy

where

T(cid:88)

t=1

1
T

(ft(xT ) − ft(x∗
(cid:18)

T )) ≤ AT −(2(1+b)−s)/3,
(cid:19)

A := 2

(K + 1)(K + 2)

Proof. We prove only Claim (ii), as the proof of Claim (i) is similar and simpler. Let FT :=(cid:80)T

t=1 ft. Furthermore, let
hT := AT 1−(2(1+b)−s)/3 be T times the right-hand side of Equation (2). In particular, FT is ST -strongly convex, and
smooth with curvature at most CFT where

.

L2
S

+

CK
2(1 − b)

(2)

CFT :=

T(cid:88)
CT 1−b
1 − b ≥ C
T(cid:88)
ST := ST 1−s ≤ S

t=1

t=1

t−b,

t−s.

Lazifying Conditional Gradient Algorithms

We prove Ft(xt) − Ft(x∗
t ) ≤ ht ≤ ht by induction on t. The case t = 1 is clear. Let Φt denote the value of Φt in Line 9,
while we reserve Φt to denote its value as used in Line 7. We start by showing Ft(xt+1) − Ft(x∗
t ) ≤ Φt ≤ Φt. We
distinguish two cases depending on vt from Line 10. If vt is false, then Φt = Φt and the weak separation oracle asserts
maxy∈P ∇Ft(xt)(xt − y) ≤ Φt, which combined with the convexity of Ft provides

Ft(xt+1) − Ft(x∗

t ) = Ft(xt) − Ft(x∗

t ) ≤ ∇Ft(xt)(xt − xt∗ ) ≤ Φt = Φt.
Otherwise vt is a vertex of P , then Line 15 and the induction hypothesis provides Ft(xt+1) − Ft(x∗
t ) ≤ ht + Ft(xt+1) −
Ft(xt) = Φt. To prove Φt ≤ Φt, we apply the smoothness of Ft followed by the inequality provided by the choice of vt:

Ft(xt+1) − Ft(xt) −
Rearranging provides the inequality:

CFtγ2
t

2 ≤ ∇Ft(xt)(xt+1 − xt) = γt∇Ft(xt)(vt − xt) ≤ −

γtΦt
K

.

Φt = ht + Ft(xt+1) − Ft(xt) ≤ ht −

γtΦt
K

+

CFtγ2
t

2

= Φt.

For later use, we bound the difference between ht and Φt using the value of parameters, ht ≤ ht, and γt ≤ 1:
t[2s−(1+b)]/3.

CFt γ2

ht +

htγt

htγt

t

ht − Φt ≥ ht −
We now apply Ft(xt+1) − Ft(x∗
followed by strong convexity of Ft+1:

=

2
1 + γt
K
t ) ≤ Φt, together with convexity of ft+1, and the minimality Ft(x∗

≥

=

t

K − CFt γ2
1 + 1
K

2

t

K − CFt γ2
1 + γt
K

2

A − CK
2(1−b)
K + 1

t ) ≤ Ft(x∗

t+1) of x∗
t ,

Ft+1(xt+1) − Ft+1(x∗

t+1) ≤ (Ft(xt+1) − Ft(x∗
t )) + (ft+1(xt+1) − ft+1(x∗
(cid:115)
· (cid:107)xt+1 − x∗

t+1(cid:107)

t+1))

∗
≤ Φt + (cid:107)∇t+1(cid:107)
∗
≤ Φt + (cid:107)∇t+1(cid:107)

2

St+1

(Ft+1(xt+1) − Ft+1(x∗

t+1)).

(3)

Solving the quadratic inequality provides

Ft+1(xt+1) − Ft+1(x∗

∗2
t+1) ≤ Φt + (cid:107)∇t+1(cid:107)

St+1

+ 2

(cid:118)(cid:117)(cid:117)(cid:116)(cid:107)∇t+1(cid:107)

2St+1

∗2

(cid:32)

∗2
(cid:107)∇t+1(cid:107)
2St+1

(cid:33)

+ Φt

.

(4)

t+1(cid:107) ≤ D. Thus Ft+1(xt+1) − Ft+1(x∗

From Equation (3), ignoring the last line, we also obtain Ft+1(xt+1) − Ft+1(x∗
(cid:107)xt+1 − x∗
∗
Now we estimate the right-hand side of Equation (4) by using the actual value of the parameters, the estimate (cid:107)∇t+1(cid:107)
≤ L,
and the inequality s + b ≤ 2. In fact, we estimate a proxy for the right-hand side. Note that A was chosen to satisfy the
second inequality:

t+1) ≤ ht+1, by Line 7, as claimed.

t+1) ≤ Φt + (cid:107)∇t+1(cid:107)

D via the estimate

∗

(cid:115)

L2
St+1

+ 2

(cid:114)

L2

2St1−s ht

(cid:114)

t[2s−(1+b)]/3 + 2

L2

2St+1

ht ≤

≤

=

L2
St1−s + 2
L2
S

(cid:114)

(cid:32)

L2
S

+

(cid:33)

L2

2St1−s ht
t[2s−(1+b)]/3

2

L2
S

A

t[2s−(1+b)]/3

A − CK
2(1−b)
K + 1

≤
≤ ht − Φt ≤ ht − Φt.

In particular, L2
2St+1

+ Φt ≤ ht hence combining with Equation (4) we obtain

Lazifying Conditional Gradient Algorithms

(cid:115)
(cid:115)

(cid:18) L2

(cid:19)

+ Φt

L2

2St+1

2St+1

L2

2St+1

ht

ht+1 ≤ Φt +

L2
St+1

+ 2

+ 2

L2
≤ Φt +
St+1
≤ ht ≤ ht+1.

C.1. Stochastic and Adversarial Versions

Complementing the ofﬂine algorithms from Section 3, we will now derive various versions for the online case. The presented
cases here are similar to those in (Hazan & Kale, 2012) and thus we state them without proof.
For stochastic cost functions ft, we obtain bounds from Theorem C.1 (i) similar to (Hazan & Kale, 2012, Theorems 4.1
and 4.3) (with δ replaced by δ/T in the bound to correct an inaccuracy in the original argument). The proof is analogous
and hence omitted, but note that (cid:107)y1 − y2(cid:107)2 ≤
Corollary C.2. Let ft be convex functions sampled i.i.d. with expectation E [ft] = f∗, and δ > 0. Assume that the ft are
L-Lipschitz in the 2-norm.

(cid:112)
(cid:107)y1 − y2(cid:107)1(cid:107)y1 − y2(cid:107)∞ ≤ √k for all y1, y2 ∈ P .

(i) If all the ft are smooth with curvature at most C, then Algorithm 7 applied to the ft (with b = 0) yields with probability

1 − δ

f∗

(xt) − min
x∈P

f∗

(x) ≤ O

(ii) Without any smoothness assumption, Algorithm 7 (applied to smoothenings of the ft) provides with probability 1 − δ

T(cid:88)

t=1

T(cid:88)

t=1

T(cid:88)

t=1

T(cid:88)

t=1

f∗

(xt) − min
x∈P

f∗

(x) ≤ O

(cid:16)

(cid:17)

C√T + Lk(cid:112)nT log(nT 2/δ) log T
(cid:16)√nLkT 2/3 + Lk(cid:112)nT log(nT 2/δ) log T

.

(cid:17)

.

Similar to (Hazan & Kale, 2012, Theorem 4.4), from Theorem C.1 (ii) we obtain the following regret bound for adversarial
cost functions with an analogous proof.
Corollary C.3. For any L-Lipschitz convex cost functions ft, Algorithm 7 applied to the functions ˜ft(x) := ∇ft(xt)x +
2L√
k

2 (with b = s = 1/4, C = L√k, S = L/√k, and Lipschitz constant 3L) achieving regret

t−1/4(cid:107)x − x1(cid:107)2

T(cid:88)

t=1

T(cid:88)

t=1

ft(xt) − min
x∈P

ft(x) ≤ O(L√kT 3/4)

with at most T calls to the weak separation oracle.

Note that the gradient of the ˜ft are easily computed via the formula ∇ ˜ft(x) = ∇ft(xt) + 4Lt−1/4(x− x1)/√k, particularly
because the gradient of the ft need not be recomputed, so that we obtain a weak separation-based stochastic gradient
descent algorithm, where we only have access to the ft through a stochastic gradient oracle, while retaining all the favorable
properties of the Frank-Wolfe algorithm with a convergence rate O(T −1/4) (c.f., (Garber & Hazan, 2013)).

D. Weak Separation through Augmentation
So far we realized the weak separation oracle via lazy optimization. We will now create a (weak) separation oracle for
integral polytopes, employing an even weaker, so-called augmentation oracle, which only provides an improving solution
but provides no guarantee with respect to optimality. We call this approach lazy augmentation. This is especially useful
when a fast augmentation oracle is available or the vertices of the underlying polytope P are particularly sparse. As before
theoretical convergence rates are maintained.

Lazifying Conditional Gradient Algorithms

For simplicity of exposition we restrict to 0/1 polytopes P here. For general integral polytopes, one considers a so-called
directed augmentation oracle, which can be similarly linearized after splitting variables in positive and negative parts; we
refer the interested reader to see (Schulz & Weismantel, 2002; Bodic et al., 2015) for an in-depth discussion.
Let k denote the (cid:96)1-diameter of P . Upon presentation with a 0/1 solution x and a linear objective c ∈ Rn, an augmentation
oracle either provides an improving 0/1 solution ¯x with c¯x < cx or asserts optimality for c:

Oracle 3 Linear Augmentation Oracle AUGP (c, x)
Input: c ∈ Rn linear objective, x ∈ P vertex,
Output: ¯x ∈ P vertex with c¯x < cx when exists, otherwise ¯x = x

Such an oracle is signiﬁcantly weaker than a linear optimization oracle but also signiﬁcantly easier to implement and much
faster; we refer the interested reader to (Gr¨otschel & Lov´asz, 1993; Schulz et al., 1995; Schulz & Weismantel, 2002) for
an extensive list of examples. While augmentation and optimization are polynomially equivalent (even for convex integer
programming (Oertel et al., 2014)) the current best linear optimization algorithms based on an augmentation oracle are
slow for general objectives. While optimizing an integral objective c ∈ Rn needs O(k log(cid:107)c(cid:107)∞) calls to an augmentation
oracle (see (Schulz et al., 1995; Schulz & Weismantel, 2002; Bodic et al., 2015)), a general objective function, such as the
gradient in Frank–Wolfe algorithms has only an O(kn3) guarantee in terms of required oracle calls (e.g., via simultaneous
diophantine approximations (Frank & Tardos, 1987)), which is not desirable for large n. In contrast, here we use an
augmentation oracle to perform separation, without ﬁnding the optimal solution. Allowing a multiplicative error K > 1, we
realize an augmentation-based weak separation oracle (see Algorithm 8), which decides given a linear objective function
c ∈ Rn, an objective value Φ > 0, and a starting point x ∈ P , whether there is a y ∈ P with c(x − y) > Φ/K or
c(x − y) ≤ Φ for all y ∈ P . In the former case, it actually provides a certifying y ∈ P , i.e., with c(x − y) > Φ/K. Note
that a constant accuracy K requires a linear number of oracle calls in the diameter k, e.g., K = (1 − 1/e)−1 ≈ 1.582 needs
at most N ≤ k oracle calls.
At the beginning, in Line 2, the algorithm has to replace the input point x with an integral point x0. If the point x is given as
a convex combination of integral points, then a possible solution is to evaluate the objective c on these integral points, and
choose x0 the ﬁrst one with cx0 ≤ cx. This can be easily arranged for Frank–Wolfe algorithms as they maintain convex
combinations.

Algorithm 8 Augmenting Weak Separation LPsepP (c, x, Φ, K)
Input: c ∈ Rn linear objective, x ∈ P point, Φ > 0 objective value; K > 1 accuracy
Output: Either (1) y ∈ P vertex with c(x − y) > Φ/K, or (2) false: c(x − z) ≤ Φ for all z ∈ P .
1: N ← (cid:100)log(1 − 1/K)/ log(1 − 1/k)(cid:101)
2: Choose x0 ∈ P vertex with cx0 ≤ cx.
3: for i = 1 to N do
4:
5:
6:
7:
8:
9:
end if
10:
11: end for
12: return xN

if c(x − xi−1) ≥ Φ then
return xi−1
end if
xi ← AUGP (c + Φ−c(x−xi−1)
if xi = xi−1 then
return false

k

(1 − 2xi−1), xi−1)

Proposition D.1. Assume (cid:107)y1 − y2(cid:107)1 ≤ k for all y1, y2 ∈ P . Then Algorithm 8 is correct, i.e., it outputs either (1) y ∈ P
with c(x − y) > Φ/K, or (2) false. In the latter case c(x − y) ≤ Φ for all y ∈ P holds. The algorithm calls AUGP at
most N ≤ (cid:100)log(1 − 1/K)/ log(1 − 1/k)(cid:101) many times.
Proof. First note that (1 − 2x)v + (cid:107)x(cid:107)1 = (cid:107)v − x(cid:107)1 for x, v ∈ {0, 1}n, hence Line 7 is equivalent to xi ← AUGP (c +
Φ−c(x−xi−1)

k

(cid:107)· − xi−1(cid:107)1, xi−1).

Lazifying Conditional Gradient Algorithms

The algorithm obviously calls the oracle at most N times by design, and always returns a value, so we need to verify only
the correctness of the returned value. We distinguish cases according to the output.
Clearly, Line 5 always returns an xi−1 with c(x − xi−1) ≥ Φ > [1 − (1 − 1/k)N ]Φ. When Line 9 is executed, the
augmentation oracle just returned xi = xi−1, i.e., for all y ∈ P

Φ − c(x − xi−1)
cxi−1 ≤ cy +
(cid:107)y − xi−1(cid:107)1
Φ − c(x − xi−1)
k
≤ cy +
= c(y − x) + cxi−1 + Φ,

k

k

so that c(x − y) ≤ Φ, as claimed.
Finally, when Line 12 is executed, the augmentation oracle has found an improving vertex xi at every iteration, i.e.,

using (cid:107)xi − xi−1(cid:107)1 ≥ 1 by integrality. Rearranging provides the convenient form

which by an easy induction provides

cxi−1 > cxi +

≥ cxi +

(cid:107)xi − xi−1(cid:107)1
,

k

k

Φ − c(x − xi−1)
Φ − c(x − xi−1)
(cid:18)
(cid:19)N

(cid:19)

1 −

1
k

Φ − c(x − xi) <

(cid:18)

[Φ − c(x − xi−1)],
(cid:18)

(cid:19)

1
K

Φ,

Φ − c(x − xN ) <

1 −

[Φ − c(x − x0)] ≤

1 −

1
k

K , ﬁnishing the proof.

i.e., c(x − xN ) ≥ Φ
E. Experiments
We implemented and compared the parameter-free variant of LCG (Algorithm 4) to the standard Frank-Wolf algorithm
(CG). Moreover, we implemented and compared Algorithm 3 (LPCG) to the Pairwise Conditional Gradient algorithm (PCG)
variant of (Garber & Meshi, 2016) as well as implemented and compared Algorithm 7 (LOCG) to the Online Frank-Wolfe
algorithm (OCG) of (Hazan & Kale, 2012). While we did implement the Local Conditional Gradient variant in (Garber &
Hazan, 2013) as well, the very large constants in the original algorithms made it impractical to run.
We have used K = 1.1 and K = 1 as multiplicative factors for the weak separation oracle; for the impact of the choice of
K see Section E.2.1. For the baseline algorithms we use inexact variants, i.e., we solve linear optimization problems only
approximately. This is a signiﬁcant speedup in favor of non-lazy algorithms at the (potential) cost of accuracy, while neutral
to lazy optimization as it solves an even more relaxed problem anyways. To put things in perspective, the non-lazy baselines
could not complete even a single iteration for a signiﬁcant fraction of the considered problems in the given time frame if we
were to exactly solve the linear optimization problems.
The linear optimization oracle over P × P for LPCG was implemented by calling the respective oracle over P twice: once
for either component. Contrary to the non-lazy version, the lazy algorithms depend on the initial upper bound Φ0. For
the instances that need a very long time to solve the (approximate) linear optimization even once, we used for the lazy
algorithms a binary search for Φ0: starting from a conservative initial value, using the update rule Φ0 ← Φ0/2 until the
separation oracle returns an improvement for the ﬁrst time and then we start the algorithm with 2Φ0, which is an upper
bound on the Wolfe gap and hence also on the primal gap. This initial phase is also included in the reported wall-clock time.
Alternatively, if the linear optimization is less time consuming we used a single (approximate) linear optimization at the
start to obtain an initial bound on Φ0 (see e.g., Section 4).
In some cases, especially when the underlying feasible region has a high dimension and the (approximate) linear optimization
can be solved relatively fast compared to the cost of computing an inner product, we observed that the costs of maintaining

Lazifying Conditional Gradient Algorithms

the cache was very high. In these cases we reduce the cache size every 100 steps by keeping only the 100 points that were
used the most so far. Both, the number of steps and the approximate size of the cache are chosen arbitrarily, however 100 for
both worked very well for all our examples. Of course there are many different strategies for maintaining the cache, which
could be used here and which could lead to further improvements in performance.
The stopping criteria for each of the experiments is a given wall clock time limit in seconds. The time limit was enforced
separately for the main code, and the oracle code so in some cases the actual time used can be larger, when the last oracle
call started before the time limit was reached and took longer than the time left.
We implemented all algorithms in Python 2.7 with critical functions cythonized for performance employing Numpy.
We used these packages from the Anaconda 4.2.0 distribution as well as Gurobi 7.0 (Gurobi Optimization, 2016)
as a black box solver for the linear optimization oracle and the weak separation oracle. The latter was implemented via a
callback function to stop the optimization as soon as a good enough feasible solution has been found. The parameters for
Gurobi were kept at their default settings except for enforcing the time limit of the tests and setting the acceptable duality
gap to 10%, allowing Gurobi to terminate the linear optimization early avoiding the expensive proof of optimality. This is
used to realize the inexact versions of the baseline algorithms. All experiments were performed on a 16-core machine with
Intel Xeon E5-2630 v3 @ 2.40GHz CPUs and 128GB of main memory. While our code does not explicitly use multiple
threads, both Gurobi and the numerical libraries use multiple threads internally.

E.1. Computational results

We performed computational tests on a large variety of different problems that are instances of the three machine learning
tasks video colocalization, matrix completion and structured regression.

Video colocalization. Video colocalization is the problem of identifying objects in a sequence of multiple frames in a
video. In (Joulin et al., 2014) it is shown that video colocalization can be reduced to optimizing a quadratic objective
function over a ﬂow or a path polytope , which is the problem we are going to solve. The quadratic functions are of the
form (cid:107)Ax − b(cid:107)2 where we choose the non-zero entries in A according to a density parameter at random and then each of
these entries to be [0, 1]-uniformly distributed, while b is chosen as a linear combination of the columns of A with random
multipliers from [0, 1]. For some of the instances we also use (cid:107)x − b(cid:107)2 as the objective function with bi ∈ [0, 1] uniformly
at random.

Matrix completion. The formulation of the matrix completion problem we are going to use is the following:

(cid:88)

min

(i,j)∈Ω

(cid:107)Xi,j − ai,j(cid:107)2

s.t.

(cid:107)X(cid:107)∗ ≤ R,

(5)

where (cid:107)·(cid:107)∗ denotes the nuclear norm, i.e., (cid:107)A(cid:107)∗ = tr(√AtA). The set Ω, the matrix A with entries ai,j, and R are given
parameters. Similarly to (Lan & Zhou, 2014) we generate the m × n matrix A as the product of AL of size m × r and AR
of size r × n. The entries in AL and AR are chosen from a standard Gaussian. The set of entries Ω is chosen uniformly of
size s = min(5r(m + n − r),(cid:100)0.99mn(cid:101)). The linear optimization oracle is implemented in this case by a singular value
decomposition of the linear objective function.

Structured regression. The structured regression problem consists of solving a quadratic function of the form (cid:107)Ax − b(cid:107)2
over some structured feasible set or a polytope. We construct the objective functions in the same way as for the video
colocalization problem.
We will present in the following two sections the complete set of results for various problems grouped by the different
versions of the considered algorithms. Every ﬁgure contains two columns, each containing one experiment. We use different
measures to report performance: the ﬁrst row reports loss or function value in wall-clock time (including time spent by the
oracle), the second row contains loss or function value in the number of iterations. In some cases we include a row reporting
the loss or function value over the number of linear optimization calls. In some other cases we report in another row the dual
bound or Wolfe gap in wall-clock time. The last row always reports the cumulative number of calls to the linear optimization
oracle for the lazy algorithm. The red line denotes the non-lazy algorithm and the greed line denotes the lazy variants. For
each experiment we also report the cache hit rate, which is the number of oracle calls answered with a point from the cache
over all oracle calls given in percent.

Lazifying Conditional Gradient Algorithms

While we found convergence rates in the number of iterations quite similar (as expected!), we consistently observe a
signiﬁcant speedup in wall-clock time. In particular for many large-scale or hard combinatorial problems, lazy algorithms
performed several thousand iterations whereas the non-lazy versions completed only a handful of iterations due to the large
time spent approximately solving the linear optimization problem. The observed cache hit rate was at least 90% in most
cases, and often even above 99%.

E.1.1. ONLINE RESULTS

Additionally to the quadratic objective functions we tested the online version on random linear functions cx + b with
c ∈ [−1, +1]n and b ∈ [0, 1]. For online algorithms, each experiment used a random sequence of 100 different random loss
functions. For online conditional gradient algorithms, in every ﬁgure the left column uses linear loss functions, the right one
uses quadratic loss functions of the form as described above over the same polytope.
As an instance of the structured regression problem we used the ﬂow-based formulation for Hamiltonian cycles in graphs,
i.e., the traveling salesman problem (TSP) for graphs with 11 and 16 nodes (Figures 7 and 8). While relatively small,
the oracle problem can be solved in reasonable time for these instances. Another instance of the structured regression
problem uses the standard formulation of the cut polytope for graphs with 23 and 28 nodes as the feasible region (Figures 9
and 10). Another set of feasible regions corresponding to NP-hard problems for the structured regression problem we
tested our algorithm on are the quadratic unconstrained boolean optimization (QUBO) instances deﬁned on Chimera
graphs (Dash, 2013), which are available at http://researcher.watson.ibm.com/researcher/files/us-
sanjeebd/chimera-data.zip. The instances are relatively hard albeit their rather small size (Figure 11 and 12).
One instance of the video colocalization problem uses a path polytope from http://lime.cs.elte.hu/˜kpeter/
data/mcf/netgen/ that was generated with the netgen graph generator (Figure 13). Most of these instances are very
large-scale minimum cost ﬂow instances with several tens of thousands nodes in the underlying graphs, therefore solving still
takes considerable time despite the problem being in P. We tested on the structured regression problems with the MIPLIB
(Achterberg et al., 2006; Koch et al., 2011)) instances eil33-2 (Figure 14) and air04 (Figure 15) as feasible regions.
Finally for the spanning tree problem, we used the well-known extended formulation with O(n3) inequalities for an n-node
graph. We considered graphs with 10 and 25 nodes (Figures 16 and 17).
We observed that while OCG and LOCG converge comparably in the number of iterations, the lazy LOCG performed
signiﬁcantly more iterations; for hard problems, where linear optimization is costly and convergence requires a large number
of iterations, this led LOCG converging much faster in wall-clock time. In extreme cases OCG could not complete even a
single iteration. This is due to LOCG only requiring some good enough solution, whereas OCG requires a stronger guarantee.
This is reﬂected in faster oracle calls for LOCG.

E.1.2. OFFLINE RESULTS

We describe the considered instances in the ofﬂine case separately for the vanilla Frank-Wolfe method and the Pairwise
Conditional Gradients method.

Vanilla Frank-Wolfe Method We tested the vanilla Frank-Wolfe algorithm on the six video colocalization instances
with underlying path polytopes from http://lime.cs.elte.hu/˜kpeter/data/mcf/netgen/ (Figures 18,
19 and 20). In these instances we additionally report the dual bound or Wolfe gap in wall clock time. We further tested the
vanilla Frank-Wolfe algorithm on eight instances of the matrix completion problem generated as described above. For these
examples we did not use line search. We give the used parameters for each example in the ﬁgures below (Figures 21, 22, 23
and 24). The last tests for this version were performed on three instances of the structured regression problem, two with the
feasible region containing ﬂow-based formulations of Hamiltonian cycles in graphs (Figures 25), two on two different cut
polytope instances (Figure 26) and ﬁnally two on two spanning tree instances of different size (Figure 27).
Similarly to the online case, we observe a signiﬁcant speedup of LCG compared to CG, due to the faster iteration of the lazy
algorithm.

Pairwise Conditional Gradient Algorithm As we inherit structural restrictions of PCG on the feasible region, the
problem repertoire is limited in this case. We tested the Pairwise Conditional Gradient algorithm on the structured
regression problem with feasible regions from the MIPLIB instances eil33-2, air04, eilB101, nw04, disctom,
m100n500k4r1 (Figures 28, 29 and 30).

Lazifying Conditional Gradient Algorithms

Again similarly to the online case and the vanilla Frank-Wolfe algorihtm, we observe a signiﬁcant improvement in wall-clock
time of LPCG compared to CG, due to the faster iteration of the lazy algorithm.

E.2. Performance improvements, parameter sensitivity, and tuning

E.2.1. PARAMTER-FREE VS. TEXTBOOK VARIANT

For illustrative purposes, we compare the textbook variant of the lazy conditional gradients (Algorithm 2) with its parameter-
free counterpart (Algorithm 4) in Figure 31. The parameter-free variant outperforms the textbook variant due to the active
management of Φ combined with line search.
Similar parameter-free variants can be derived for the other algorithms; see discussion in Section 4.

F. Final Remarks
We would like to close with a few ﬁnal remarks. If a given baseline algorithm works over general compact convex sets P ,
then so does the laziﬁed version. In fact, as the laziﬁed algorithm runs, it produces a polyhedral approximation of the set P
with very few vertices (subject to optimality vs. sparsity tradeoffs; see (Jaggi, 2013, Appendix C)).
Moreover, the weak separation oracle does not need to return extreme points. All algorithms also work with maximal
solutions that are not necessarily extremal (e.g., lying in a higher-dimensional face). However, in that case we lose the
desirable property that the ﬁnal solution is a sparse convex combination of extreme points (typically vertices in the polyhedral
setup) or atoms.

Lazifying Conditional Gradient Algorithms

cache hit rate: 99.7%

cache hit rate: 99.0%

Figure 7. LOCG vs. OCG with the TSP polytope for a graph with 11 nodes as the feasible region and with a 500 seconds time limit.
OCG completed only a few iterations, resulting in a several times larger ﬁnal loss for quadratic loss functions. Notice that with time
LOCG needed fewer and fewer LP calls.

0150300450600Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 0150300450600Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 01500300045006000Iterations0.00.20.40.60.81.0LossLOCG OCG 030006000900012000Iterations0.00.20.40.60.81.0LossLOCG OCG 01500300045006000Iterations051015Number of LP callsLOCG 030006000900012000Iterations020406080100Number of LP callsLOCG Lazifying Conditional Gradient Algorithms

cache hit rate: 89.1%

cache hit rate: 20.6%

Figure 8. LOCG vs. OCG on the TSP polytope for a graph with 16 nodes with a time limit of 7200 seconds. OCG was not able to
complete a single iteration and in the quadratic case even LOCG could not complete any more iteration after 50s. The quadratic losses on
the right nicely demonstrate speed improvements (mostly) through early termination of the linear optimization as the cache rate is only
20.6%.

0400080001200016000Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 0153045Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 050100150Iterations0.00.20.40.60.81.0LossLOCG OCG 0204060Iterations0.00.20.40.60.81.0LossLOCG OCG 050100150Iterations05101520Number of LP callsLOCG 0204060Iterations01020304050Number of LP callsLOCG Lazifying Conditional Gradient Algorithms

cache hit rate: 99.6%

cache hit rate: 97.5%

Figure 9. LOCG vs. OCG on the cut polytope for a graph with 23 nodes. Both LOCG and OCG converge to the optimum in a few
iterations for linear losses, while LOCG is remarkably faster for quadratic losses. It demonstrates that the advantage of lazy algorithms
strongly correlates with the difﬁculty of linear optimization. For linear losses, remarkably LOCG needed no oracle calls after one third of
the time.

0150300450600Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 0150300450600Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 080016002400Iterations0.00.20.40.60.81.0LossLOCG OCG 02000400060008000Iterations0.00.20.40.60.81.0LossLOCG OCG 080016002400Iterations024681012Number of LP callsLOCG 02000400060008000Iterations050100150200Number of LP callsLOCG Lazifying Conditional Gradient Algorithms

cache hit rate: 99.7%

cache hit rate: 98.6%

Figure 10. LOCG vs. OCG on the cut polytope for a 28-node graph. As for the smaller problem, this also illustrates the advantage of lazy
algorithms when linear optimization is expensive. Again, LOCG needed no oracle calls after a small initial amount of time.

02000400060008000Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 02000400060008000Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 02000400060008000Iterations0.00.20.40.60.81.0LossLOCG OCG 080001600024000Iterations0.00.20.40.60.81.0LossLOCG OCG 02000400060008000Iterations05101520Number of LP callsLOCG 080001600024000Iterations050100150200250300Number of LP callsLOCG Lazifying Conditional Gradient Algorithms

cache hit rate: 99.6%

cache hit rate: 98.5%

Figure 11. LOCG vs. OCG on a small QUBO instance. For quadratic losses, both algorithms converged very fast while LOCG still has a
signiﬁcant edge. For linear losses, LOCG is noticeably faster than OCG.

0150300450600Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 0150300450600Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 0200040006000Iterations0.00.20.40.60.81.0LossLOCG OCG 030006000900012000Iterations0.00.20.40.60.81.0LossLOCG OCG 0200040006000Iterations0510152025Number of LP callsLOCG 030006000900012000Iterations050100150Number of LP callsLOCG Lazifying Conditional Gradient Algorithms

cache hit rate: 99.8%

cache hit rate: 99.99%

Figure 12. LOCG vs. OCG on a large QUBO instance. Both algorithms converge fast to the optimum. Interestingly, LOCG only performs
4 LP calls.

02000400060008000Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 02000400060008000Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 030006000900012000Iterations0.00.20.40.60.81.0LossLOCG OCG 080001600024000Iterations0.00.20.40.60.81.0LossLOCG OCG 030006000900012000Iterations051015Number of LP callsLOCG 080001600024000Iterations0.00.51.01.52.02.53.03.54.0Number of LP callsLOCG Lazifying Conditional Gradient Algorithms

cache hit rate: 97.0%

cache hit rate: 89.6%

Figure 13. LOCG vs. OCG on a path polytope. Similar convergence rate in the number of iterations, but signiﬁcant difference in terms of
wall-clock time.

0150300450600Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 0150300450600Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 0150300450Iterations0.00.20.40.60.81.0LossLOCG OCG 0150300450600Iterations0.00.20.40.60.81.0LossLOCG OCG 0150300450Iterations02468101214Number of LP callsLOCG 0150300450600Iterations01020304050Number of LP callsLOCG Lazifying Conditional Gradient Algorithms

cache hit rate: 53.1%

cache hit rate: 72.2%

Figure 14. LOCG vs. OCG on the MIPLIB instance eil33-2. All algorithms performed comparably, due to fast convergence in this
case.

0150300450600Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 0150300450600Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 0200400600Iterations0.00.20.40.60.81.0LossLOCG OCG 050010001500Iterations0.00.20.40.60.81.0LossLOCG OCG 0200400600Iterations050100150200250300Number of LP callsLOCG 050010001500Iterations0100200300400500Number of LP callsLOCG Lazifying Conditional Gradient Algorithms

cache hit rate: 96.8%

cache hit rate: 99.9%

Figure 15. LOCG vs. OCG on the MIPLIB instance air04. LOCG clearly outperforms OCG as the provided time was not enough for
OCG to complete the necessary number of iterations for entering reasonable convergence.

0250050007500Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 02000400060008000Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 080016002400Iterations0.00.20.40.60.81.0LossLOCG OCG 01500300045006000Iterations0.00.20.40.60.81.0LossLOCG OCG 080016002400Iterations010203040506070Number of LP callsLOCG 01500300045006000Iterations012345678Number of LP callsLOCG Lazifying Conditional Gradient Algorithms

cache hit rate: 98.8%

cache hit rate: 99.8%

Figure 16. LOCG vs. OCG on a spanning tree instance for a 10-node graph. LOCG makes signiﬁcantly more iterations, few oracle calls,
and converges faster in wall-clock time.

0150300450600Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 0200400600Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 01000200030004000Iterations0.00.20.40.60.81.0LossLOCG OCG 025005000750010000Iterations0.00.20.40.60.81.0LossLOCG OCG 01000200030004000Iterations01020304050Number of LP callsLOCG 025005000750010000Iterations051015Number of LP callsLOCG Lazifying Conditional Gradient Algorithms

cache hit rate: 95.9%

cache hit rate: 99.7%

Figure 17. LOCG vs. OCG on a spanning tree instance for a 25-node graph. On the left, early ﬂuctuation can be observed, bearing no
consequence for later convergence rate. OCG did not get past this early stage. In both cases LOCG converges signiﬁcantly faster.

02000400060008000Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 02000400060008000Wall-clock time (s)0.00.20.40.60.81.0LossLOCG OCG 080016002400Iterations0.00.20.40.60.81.0LossLOCG OCG 01500300045006000Iterations0.00.20.40.60.81.0LossLOCG OCG 080016002400Iterations020406080100120Number of LP callsLOCG 01500300045006000Iterations02468101214Number of LP callsLOCG Lazifying Conditional Gradient Algorithms

cache hit rate: 95.72%

cache hit rate: 94.83%

Figure 18. LCG vs. CG on small netgen instances netgen 08a (left) and netgen 10a (right) with quadratic objective functions. In
both cases both algorithms are able to reduce the function value very fast, however the dual bound or Wolfe gap is reduced much faster by
LCG. Observe that the vertical axis is given with a logscale.

080160240Wallclocktime(s)101210131014FunctionvalueLCGCG0150300450Wallclocktime(s)10131014FunctionvalueLCGCG01000200030004000Iterations101210131014FunctionvalueLCGCG050010001500Iterations10131014FunctionvalueLCGCG080160240Wallclocktime(s)10610710810910101011101210131014DualboundLCGCG0150300450600Wallclocktime(s)10610710810910101011101210131014DualboundLCGCG01000200030004000Iterations020406080100120140160180NumberofLPcallsLCG050010001500Iterations020406080100NumberofLPcallsLCGLazifying Conditional Gradient Algorithms

cache hit rate: 86.43%

cache hit rate: 50.00%

Figure 19. LCG vs. CG on medium sized netgen instances netgen 12b (left) and netgen 14a (right) with quadratic objective
functions. The behavior of both versions on these instances is very similar to the small netgen instances (Figure 18), however both in the
function value and the dual bound the difference between the lazy and the non-lazy version is more prominent. Again, we used a logscale
for the vertical axis.

0150300450Wallclocktime(s)10141015FunctionvalueLCGCG0150300450Wallclocktime(s)101410151016FunctionvalueLCGCG0100200300400Iterations10141015FunctionvalueLCGCG0306090120Iterations101410151016FunctionvalueLCGCG0150300450600Wallclocktime(s)106107108109101010111012101310141015DualboundLCGCG0150300450600Wallclocktime(s)1071081091010101110121013101410151016DualboundLCGCG0100200300400Iterations0102030405060NumberofLPcallsLCG0306090120Iterations0102030405060NumberofLPcallsLCGLazifying Conditional Gradient Algorithms

cache hit rate: 48.72%

cache hit rate: 50.00%

Figure 20. LCG vs. CG on large netgen instances netgen 16a (left) and netgen 16b (right) with quadratic objective functions. In
both cases the difference in function value between the two versions of the algorithm is large. In the dual bound the performance of the
lazy version is multiple orders of magnitude better than the performance of the non-lazy counterpart. The cache hit rates for these two
instances are lower due to the high dimension of the polytope.

0150300450Wallclocktime(s)1014101510161017FunctionvalueLCGCG0150300450Wallclocktime(s)1014101510161017FunctionvalueLCGCG010203040Iterations1014101510161017FunctionvalueLCGCG010203040Iterations1014101510161017FunctionvalueLCGCG150300450Wallclocktime(s)10610710810910101011101210131014101510161017DualboundLCGCG150300450Wallclocktime(s)10710810910101011101210131014101510161017DualboundLCGCG010203040Iterations05101520NumberofLPcallsLCG010203040Iterations05101520NumberofLPcallsLCGLazifying Conditional Gradient Algorithms

cache hit rate: 94.24%

cache hit rate: 84.80%

Figure 21. LCG vs. CG on two matrix completion instances. We solve the problem as given in Equation (5) with the paramters n = 3000,
m = 1000, r = 10 and R = 30000 for the left instance and n = 10000, m = 100, r = 10 and R = 10000 for the right instance. In
both cases the lazy version is slower in interations, however signiﬁcantly faster in wall clock time.

0150300450Wallclocktime(s)104105106107108FunctionvalueLCGCG0150300450Wallclocktime(s)105106107108FunctionvalueLCGCG050010001500Iterations104105106107108FunctionvalueLCGCG080160240Iterations105106107108FunctionvalueLCGCG050010001500Iterations020406080100NumberofLPcallsLCG080160240Iterations05101520253035NumberofLPcallsLCGLazifying Conditional Gradient Algorithms

cache hit rate: 97.50%

cache hit rate: 59.46%

Figure 22. LCG vs. CG on two more matrix completion instances. The parameters for Equation (5) are given by n = 5000, m = 4000,
r = 10 and R = 50000 for the left instance and n = 100, m = 20000, r = 10 and R = 15000 for the right instance. In both of these
cases the performance of the lazy and the non-lazy version are comparable in interations, however in wall clock time the lazy version
reaches lower function values faster.

02505007501000Wallclocktime(s)106107108109FunctionvalueLCGCG02505007501000Wallclocktime(s)106107108109FunctionvalueLCGCG0150300450600Iterations106107108109FunctionvalueLCGCG010203040Iterations106107108109FunctionvalueLCGCG0200400600Iterations0246810121416NumberofLPcallsLCG010203040Iterations0246810121416NumberofLPcallsLCGLazifying Conditional Gradient Algorithms

cache hit rate: 80.80%

cache hit rate: 82.98%

Figure 23. LCG vs. CG on our ﬁfth and sixth instances of the matrix completion problem. The parameters are n = 5000, m = 100,
r = 10 and R = 15000 for the left instance and n = 3000, m = 2000, r = 10 and R = 10000 for the right instance. The behavior is
very similar to Figure 22. similar performance over iterations however advantages for the lazy version over wall clock time.

0150300450Wallclocktime(s)103104105106107108109FunctionvalueLCGCG0150300450Wallclocktime(s)105106107108FunctionvalueLCGCG0200400600800Iterations103104105106107108109FunctionvalueLCGCG0100200300400Iterations105106107108FunctionvalueLCGCG0200400600800Iterations020406080100120140NumberofLPcallsLCG0100200300400Iterations010203040506070NumberofLPcallsLCGLazifying Conditional Gradient Algorithms

cache hit rate: 87.10%

cache hit rate: 91.55%

Figure 24. LCG vs. CG on the ﬁnal two matrix completion instances. The parameters are n = 10000, m = 1000, r = 10 and
R = 1 − 000 for the left instance and n = 5000, m = 1000, r = 10 and R = 30000 for the right instance. On the left in both
measures, instances and wall clock time, the lazy version performs better than the non-lazy counterpart, due to a suboptimal direction at
the beginning with a fairly large step size in the non-lazy version.

0150300450Wallclocktime(s)106107108FunctionvalueLCGCG0150300450Wallclocktime(s)105106107108109FunctionvalueLCGCG080160240Iterations106107108FunctionvalueLCGCG02505007501000Iterations105106107108109FunctionvalueLCGCG080160240Iterations0510152025303540NumberofLPcallsLCG02505007501000Iterations01020304050607080NumberofLPcallsLCGLazifying Conditional Gradient Algorithms

cache hit rate: 69.46%

cache hit rate: 43.06%

Figure 25. LCG vs. CG on structured regression problems with feasible regions being a TSP polytope over 11 nodes (left) and 12 nodes
(right). In both cases LCG is signiﬁcantly faster in wall-clock time.

01000200030004000Wallclocktime(s)10−410−310−210−1100101102FunctionvalueLCGCG0100020003000Wallclocktime(s)10−210−1100101102FunctionvalueLCGCG0150030004500Iterations10−410−310−210−1100101102FunctionvalueLCGCG080160240Iterations10−210−1100101102FunctionvalueLCGCG0150030004500Iterations0200400600800100012001400NumberofLPcallsLCG080160240Iterations020406080100120140NumberofLPcallsLCGLazifying Conditional Gradient Algorithms

cache hit rate: 85.61%

cache hit rate: 87.48%

Figure 26. LCG vs. CG on structured regression instances using cut polytopes over a graph on 23 nodes (left) and over 28 nodes (right)
as feasible region. In both instances LCG performs signiﬁcantly better than CG.

01000200030004000Wallclocktime(s)10−310−210−1100101102103104105FunctionvalueLCGCG01000200030004000Wallclocktime(s)10−1100101102103104105FunctionvalueLCGCG0150030004500Iterations10−310−210−1100101102103104105FunctionvalueLCGCG03006009001200Iterations10−1100101102103104105FunctionvalueLCGCG0150030004500Iterations0100200300400500600NumberofLPcallsLCG03006009001200Iterations020406080100120140NumberofLPcallsLCGLazifying Conditional Gradient Algorithms

cache hit rate: 90.83%

cache hit rate: 95.59%

Figure 27. LCG vs. CG on structured regression instances with extended formulation of the spanning tree problem on a 10 node graph on
the left and a 15 node graph on the right.

0150300450Wallclocktime(s)10−210−1100101102103FunctionvalueLCGCG01000200030004000Wallclocktime(s)10−210−1100101102103104FunctionvalueLCGCG02505007501000Iterations10−210−1100101102103FunctionvalueLCGCG080016002400Iterations10−210−1100101102103104FunctionvalueLCGCG02505007501000Iterations0102030405060708090NumberofLPcallsLCG080016002400Iterations020406080100NumberofLPcallsLCGLazifying Conditional Gradient Algorithms

eil33-2, 4516 dimensions

air04, 8904 dimensions

cache hit rate: 99.8%

cache hit rate: 99.999%

Figure 28. LPCG vs. PCG on two MIPLIB instances eil33-2 and air04. LPCG converges very fast, making millions of iterations
with a relatively few oracle calls, while PCG completed only comparably few iterations due to the time-consuming oracle calls. This
clearly illustrates the advantage of lazy methods when the cost of linear optimization is non-negligible. On the left, when reaching
ε-optimality, LPCG performs many (negative) oracle calls to (re-)prove optimality; at that point one might opt for stopping the algorithm.
On the right LPCG needed a rather long time for the initial bound tigthening of Φ0, before converging signiﬁcantly faster than PCG.

01000200030004000Wall-clock time (s)0.00.20.40.60.81.0Function valueLPCG PCG 0150030004500Wall-clock time (s)0.00.20.40.60.81.0Function valueLPCG PCG 0.00.51.01.52.0Iterations1e60.00.20.40.60.81.0Function valueLPCG PCG 02468Iterations1e50.00.20.40.60.81.0Function valueLPCG PCG 0.00.51.01.52.0Iterations1e60100020003000400050006000Number of LP callsLPCG 02468Iterations1e5051015Number of LP callsLPCG Lazifying Conditional Gradient Algorithms

eilB101, 2818 dimensions

nw04, 87482 dimensions

cache hit rate: 99.995%

cache hit rate: 99.995%

Figure 29. LPCG vs. PCG on MIPLIB instances eilB101 and nw04 with quadratic loss functions. For the eilB101 instance, LPCG
spent most of the time tightening Φ0, after which it converged very fast, while PCG was unable to complete a single iteration even solving
the problem only approximately. For the nw04 instance LPCG needed no more oracle calls after an initial phase, while signiﬁcantly
outperforming PCG.

1600240032004000Wall-clock time (s)0.00.20.40.60.81.0Function valueLPCG PCG 01000200030004000Wall-clock time (s)0.00.20.40.60.81.0Function valueLPCG PCG 0.01.53.04.5Iterations1e50.00.20.40.60.81.0Function valueLPCG PCG 0.00.81.62.4Iterations1e50.00.20.40.60.81.0Function valueLPCG PCG 0.01.53.04.5Iterations1e50510152025303540Number of LP callsLPCG 0.00.81.62.4Iterations1e50510152025Number of LP callsLPCG Lazifying Conditional Gradient Algorithms

disctom, 10000 dimensions

m100n500k4r1, 600 dimensions

cache hit rate: 99.9%

cache hit rate: 48.4%

Figure 30. LPCG vs. PCG on MIPLIB instances disctom and m100n500k4r1. After very fast convergence, there is a huge increase
in the number of oracle calls for the lazy algorithm LPCG due to reaching ε-optimality as explained before. On the right the initial bound
tightening for Φ0 took a considerable amount of time but then convergence is almost instantaneous.

01000200030004000Wall-clock time (s)0.00.20.40.60.81.0Function valueLPCG PCG 7500100001250015000Wall-clock time (s)0.00.20.40.60.81.0Function valueLPCG PCG 0123Iterations1e50.00.20.40.60.81.0Function valueLPCG PCG 0.00.30.60.91.2Iterations1e50.00.20.40.60.81.0Function valueLPCG PCG 0123Iterations1e50200400600800Number of LP callsLPCG 0.00.30.60.91.2Iterations1e5020000400006000080000100000120000Number of LP callsLPCG Lazifying Conditional Gradient Algorithms

Figure 31. Comparison of the ‘textbook’ variant of the Lazy CG algorithm (Algorithm 2) vs. the Parameter-free Lazy CG (Algorithm 4)
depicted for two sample instances to demonstrate behavior. The parameter-free variant usually has a slighlty improved behavior in terms of
iterations and a signiﬁcantly improved behavior in terms of wall-clock performance. In particular, the parameter-free variant can execute
signiﬁcantly more oracle calls, due to the Φ-halving strategy and the associated bounded number of negative calls (see Theorem 4.3).

01000200030004000Wallclocktime(s)0.00.20.40.60.81.0FunctionvalueLCGparamfreeLCG02000400060008000Wallclocktime(s)0.00.20.40.60.81.0FunctionvalueLCGparamfreeLCG04080120Iterations0.00.20.40.60.81.0FunctionvalueLCGparamfreeLCG0200400600800Iterations0.00.20.40.60.81.0FunctionvalueLCGparamfreeLCG04080120Iterations020406080100NumberofLPcallsLCGparamfreeLCG0200400600800Iterations050100150200NumberofLPcallsLCGparamfreeLCG