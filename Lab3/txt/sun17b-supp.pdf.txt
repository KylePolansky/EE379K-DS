Supplementary Material for “Relative Fisher Information
and Natural Gradient for Learning Large Modular Models”

Ke Sun and Frank Nielsen

1

2
3
4
5
5
5
6
7
7

8

8

(1)

Contents

1 Non-linear Activation Functions

2 Examples of RFIMs

2.1 A Single tanh Neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.2 A Single sigm Neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.3 A Single relu Neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.4 A Single elu Neuron . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.5 RFIM of a Linear Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.6 RFIM of a Non-Linear Layer
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.7 RFIM of a Softmax Layer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2.8 RFIM of Two layers
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

3 Proof of Theorem 3

4 Experimental Settings & Zoomed Learning Curves

1 Non-linear Activation Functions

By deﬁnition,

and

tanh(t) def=

exp(t) − exp(−t)
exp(t) + exp(−t)

,

sech(t) def=

2

exp(t) + exp(−t)

.

It is easy to verify that

sech2(t) = [1 + tanh(t)] [1 − tanh(t)] = 1 − tanh2(t).

1

By eq. (1),

(cid:48)(t) =

tanh

=

exp(t) + exp(−t)
exp(t) + exp(−t)
[exp(t) + exp(−t)]2 − [exp(t) − exp(−t)]2

− exp(t) − exp(−t)
[exp(t) + exp(−t)]2 [exp(t) − exp(−t)]

[exp(t) + exp(−t)]2

=

4

[exp(t) + exp(−t)]2 = sech2(t).

By deﬁnition,

Therefore

(cid:48)(t) = −

sigm

A smoothed version of the relu function is given by

where ω > 0 and 0 ≤ ι < 1. Then,

relu

sigm(t) def=

1

1 + exp(−t)

.

exp(−t)

ω

,

1

=

1

ω

ω

ω

1
ω

ω

exp

+ exp

+

exp

exp

ω

(cid:19)(cid:19)

(cid:18) t

(cid:19)(cid:19)

(cid:18) ιt

ω

(cid:48)
ω(t) = ω

reluω(t) def= ω ln

(cid:18) t
(cid:19)

ω

exp(cid:0) ιt
ι exp(cid:0) ιt
exp(cid:0) ιt

[1 + exp(−t)]2 (− exp(−t)) =
[1 + exp(−t)]2 = sigm(t) [1 − sigm(t)] .
(cid:18) ιt
(cid:18)
(cid:19)
(cid:1)(cid:18) ι
(cid:1) + exp(cid:0) t
(cid:1) + exp(cid:0) t
(cid:1)
(cid:1)
(cid:1) + exp(cid:0) t
exp(cid:0) t
(cid:1)
(cid:1)
exp(cid:0) ιt
(cid:1) + exp(cid:0) t
(cid:1) + 1
exp(cid:0)(ι − 1) t
(cid:19)
(cid:18) 1 − ι
(cid:26) t
(cid:26) 1

= ι + (1 − ι)sigm

α (exp(t) − 1)

if t ≥ 0
if t < 0.

= ι + (1 − ι)

= ι + (1 − ι)

elu(t) =

ω

t

.

ω

ω

ω

1

ω

ω

ω

ω

(cid:48)(t) =

elu

if t ≥ 0
if t < 0.

α exp(t)

(2)

(3)

By deﬁnition,

Therefore

2 Examples of RFIMs

Table 1 shows a list of commonly used RFIMs, with detailed derivations given in the following
subsections.

2

Table 1: Commonly used RFIMs

A tanh neuron
A sigm neuron
A relu neuron

(cid:124)
˜x)] ˜x ˜x
(cid:124)

˜x(cid:1)(cid:3)2 ˜x ˜x

(cid:2)ι + (1 − ι)sigm(cid:0) 1−ι
(cid:26) ˜x ˜x

Subsystem the RFIM gy(w)
(cid:124)
(cid:124)
sech2(w
˜x) ˜x ˜x
˜x) [1 − sigm(w
(cid:124)
(cid:124)
sigm(w
(cid:124)
ω w
(cid:124)
(cid:124)
˜x))2 ˜x ˜x
,··· , ˜x ˜x
(cid:124)
diag [ ˜x ˜x
]
,··· , νf (wm, ˜x) ˜x ˜x
(cid:124)
diag [νf (w1, ˜x) ˜x ˜x
a dense matrix as shown in eq. (10)
a dense matrix as shown in eq. (12)

˜x ≥ 0
˜x < 0

if w
if w

(α exp(w

(cid:124)
(cid:124)

A linear layer
A non-linear layer
A soft-max layer
Two layers

A elu neuron

(cid:124)

(cid:124)

(cid:124)

]

2.1 A Single tanh Neuron
Consider a neuron with parameters w and a Bernoulli output y ∈ {+,−}, p(y = +) = p+,
p(y = −) = p−, and p+ + p− = 1. By the deﬁnition of RFIM, we have
∂ ln p−
∂w(cid:124)

∂ ln p+

gy(w) = p+ ∂ ln p+
∂w
∂p+
∂w

1
p+

=

∂w(cid:124) + p− ∂ ln p−
∂w
∂p−
∂p+
∂w(cid:124) +
∂w(cid:124) .

∂p−
∂w

1
p−

Since p+ + p− = 1,

∂p+
∂w

+

∂p−
∂w

= 0.

(cid:18) 1

(cid:19) ∂p+

Therefore, the RFIM of a Bernoulli neuron has the general form

∂p+
∂w(cid:124) .
A single tanh neuron with stochastic output y ∈ {−1, 1} is given by

∂p+
∂w(cid:124) =

gy(w) =

∂p+
∂w

p+ +

p+p−

1
p−

∂w

1

By eq. (4),

p(y = −1) =

p(y = 1) =

1 − µ(x)

2

1 + µ(x)

,

,

2

µ(x) = tanh(w

(cid:124)

˜x).

gy(w) =

(cid:19)(cid:18) 1
(cid:19)
(cid:2)1 − µ2(x)(cid:3)2

∂µ
∂w(cid:124)

2

∂µ
∂w

(cid:124)
˜x ˜x

1
1−µ(x)
1+µ(x)

(cid:18) 1
=(cid:2)1 − µ2(x)(cid:3) ˜x ˜x
=(cid:2)1 − tanh2(w

2
1

=

(cid:124)

(cid:124)

2

2

= sech2(w

˜x) ˜x ˜x

.

(cid:124)

(cid:124)

(1 − µ(x)) (1 + µ(x))

˜x)(cid:3) ˜x ˜x

(cid:124)

3

(4)

(5)

(6)

(7)

Then,

(cid:19)

˜x)]

(ﬁrst linear term vanishes)

An alternative analysis is given as follows. By eqs. (5) to (7),

p(y = −1) =

p(y = 1) =

exp(−w

(cid:124)

˜x)

exp(w(cid:124)x) + exp(−w(cid:124)x)

exp(w

˜x)

exp(w(cid:124) ˜x) + exp(−w(cid:124) ˜x)

(cid:124)

,

.

(cid:18)
(cid:20) exp(w

gy(w) = Ey∼p(y | x)

− ∂2 ln p(y)
∂w∂w(cid:124)

(cid:124)

(cid:21)

˜x

=

=

=

(cid:124)

∂2

˜x) + exp(−w
˜x)
exp(w(cid:124) ˜x) + exp(−w(cid:124) ˜x)

˜x) − exp(−w

(cid:124)

∂w∂w(cid:124) ln [exp(w
∂
∂w(cid:124)
∂
∂w(cid:124) tanh(w

˜x) ˜x

(cid:124)

(cid:124)

= sech2(w

(cid:124)

(cid:124)
˜x) ˜x ˜x

.

The intuitive meaning of gy(w) is a weighted covariance to emphasize such “informative” x’s

that

• are in the linear region of tanh
• contain “ambiguous” samples

We will need at least dim(w) samples to make gy(w) full rank.

2.2 A Single sigm Neuron

A single sigm neuron is given by

p(y = 0) = 1 − µ(x),
p(y = 1) = µ(x),

µ(x) = sigm(w

˜x).

(cid:124)

By eq. (4),

gy(w) =

1

∂p(y = 1)

∂p(y = 1)

∂w(cid:124)

∂w
∂µ
∂w(cid:124)

∂µ
∂w
µ2(x)(1 − µ(x))2 ˜x ˜x

(cid:124)

p(y = 0)p(y = 1)

=

1

µ(x)(1 − µ(x))

1

µ(x)(1 − µ(x))

=
= µ(x)(1 − µ(x)) ˜x ˜x
= sigm(w

(cid:124)

˜x) [1 − sigm(w

(cid:124)

(cid:124)

(cid:124)
˜x)] ˜x ˜x

.

4

2.3 A Single relu Neuron
Consider a single neuron with Gaussian output p(y | w, x) = G(y | µ(w, x), σ2). Then

gy(w | x) = Ep(y | w,x)

(cid:20) ∂ ln G(y | µ, σ2)
(cid:20) ∂
∂w
(cid:34)(cid:18)
2σ2 (y − µ)2
− 1
(cid:19)2 ∂µ

(cid:18)

= Ep(y | w,x)

∂w
− 1
σ2 (µ − y)
= Ep(y | w,x)
σ4 Ep(y | w,x) (µ − y)2 ∂µ

∂w

=

1

∂w

∂µ
∂w(cid:124)

(cid:21)

∂ ln G(y | µ, σ2)

(cid:18)

∂w(cid:124)

(cid:19) ∂

(cid:35)

∂w(cid:124)
∂µ
∂w(cid:124)

(cid:19)(cid:21)

2σ2 (y − µ)2
− 1

=

1
σ2

∂µ
∂w

∂µ
∂w(cid:124) .

We set σ = 1 to get rid of a scale parameter of the RFIM. We get

gy(w | x) =

∂µ
∂w

∂µ
∂w(cid:124) .

(8)

A single relu neuron is given by

By eqs. (2) and (8),

µ(w, x) = reluω(w

(cid:124)

˜x).

(cid:20)

ι + (1 − ι)sigm

(cid:18) 1 − ι

ω

(cid:19)(cid:21)2

(cid:124)

˜x

w

(cid:124)

.

˜x ˜x

gy(w) =

2.4 A Single elu Neuron

Similar to the analysis in Subsec. 2.3, a single elu neuron is given by

By eq. (3),

By eq. (8),

∂µ
∂w

gy(w) =

(cid:26) ˜x
(cid:26) ˜x ˜x

=

(cid:124)

µ(w, x) = elu(w

(cid:124)

˜x).

(cid:124)

if w
˜x) ˜x if w

(cid:124)
(cid:124)

˜x ≥ 0
˜x < 0.

α exp(w

(cid:124)

(cid:124)
˜x))2 ˜x ˜x

(cid:124)
(cid:124)

˜x ≥ 0
˜x < 0.

if w
if w

(α exp(w

2.5 RFIM of a Linear Layer

Consider a linear layer

p(y) = G(cid:0)y | W

(cid:124)

where W = (w1,··· , wDy ). By the deﬁnition of the multivariate Gaussian distribution,

ln p(y) = − 1
2

ln 2π − Dy
2

ln σ2 − 1
2σ2

(yi − w

(cid:124)
i ˜x)2 .

˜x, σ2I(cid:1) ,
Dy(cid:88)

i=1

5

Therefore,

Therefore,

∀i,

∂

∂wi

ln p(y) = − 1

σ2 (w

(cid:124)
i ˜x − yi) ˜x.

i ˜x)(cid:0)yj − w

(cid:124)

j ˜x(cid:1) ˜x ˜x

(cid:124)

(cid:124)

.

∀i,∀j

∂

∂wi

ln p(y)

∂
(cid:124)
∂w
j

ln p(y) =

1

σ4 (yi − w

W is vectorized by stacking its columns {wi}Dy
i=1. In the following W will be used interchangeably
to denote either the matrix or its vector form. Correspondingly, the RFIM gy(W ) has Dy × Dy
blocks, where the oﬀ-diagonal blocks are

(cid:33)

(cid:2)(yi − w

i ˜x)(cid:0)yj − w

(cid:124)

j ˜x(cid:1)(cid:3) ˜x ˜x

(cid:124)

(cid:124)

= 0,

∀i (cid:54)= j, Ep(y)

∂

∂wi

ln p(y)

∂
∂w

(cid:124)
j

ln p(y)

=

1
σ4 Ep(y)

and the diagonal blocks are

∀i, Ep(y)

In summary,

ln p(y)

∂
(cid:124)
∂w
i

∂wi

(cid:19)

ln p(y)

=

1

σ4 Ep(y) (yi − w

(cid:124)
i ˜x)2 ˜x ˜x

(cid:124)

=

(cid:124)
˜x ˜x

.

1
σ2

By setting σ = 1 we get

gy(W ) =

,··· , ˜x ˜x

(cid:124)

] .

(cid:124)
1
σ2 diag [ ˜x ˜x
(cid:124)

gy(W ) = diag [ ˜x ˜x

,··· , ˜x ˜x

(cid:124)

] .

(cid:32)

(cid:18) ∂

2.6 RFIM of a Non-Linear Layer

The statistical model of a non-linear layer with independent output units is

 .

ln p(yDy | wDy , x)
(cid:21)

ln p(yi | wi, x)

,

Dy(cid:89)
Dy(cid:88)

i=1

p(y | W , x) =

ln p(y | W , x) =

p(yi | wi, x).

ln p(yi | wi, x).

i=1

ln p(y1 | w1, x)

 ∂2

(cid:124)
∂w1∂w
1

. . .

∂2

∂wDy ∂w

(cid:124)
Dy

Then,

Therefore,

∂2

∂W ∂W (cid:124) ln p(y | W , x) =

Therefore the RFIM gy(W ) is a block-diagonal matrix, with the i’th block given by

(cid:20)

(cid:21)

(cid:20)

−Ep(y | W ,x)

∂2

(cid:124)
∂wi∂w
i

ln p(yi | wi, x)

= −Ep(yi | wi,x)

∂2

∂wi∂w

(cid:124)
i

which is simply the single neuron RFIM of the i’th neuron.

6

2.7 RFIM of a Softmax Layer

Recall that

Then

Hence

∀i ∈ {1,··· , m} ,

p(y = i) =

∀i,

ln p(y = i) = wi ˜x − ln

∀i, ∀j,

∂ ln p(y = i)

∂wj

= δij ˜x −

(cid:80)m
m(cid:88)
(cid:80)m

i=1

exp(wi ˜x)
i=1 exp(wi ˜x)

exp(wi ˜x).

exp(wj ˜x)
i=1 exp(wi ˜x)

.

˜x,

where δij = 1 if and only if i = j and δij = 0 otherwise. Then

∀i, ∀j, ∀k,

∂2 ln p(y = i)

∂wj∂w

(cid:80)m

= −δjk
exp(wj ˜x)
i=1 exp(wi ˜x)
(cid:124)
= (−δjkηj + ηjηk) ˜x ˜x
.

(cid:124)
˜x ˜x

+

((cid:80)m
exp(wj ˜x)
i=1 exp(wi ˜x))2 exp(wk ˜x) ˜x ˜x

(cid:124)

(9)

The right-hand-side of eq. (9) does not depend on i. Therefore

gy(W ) =

(η1 − η2
1) ˜x ˜x
−η2η1 ˜x ˜x
(cid:124)

...

−ηmη1 ˜x ˜x
(cid:124)

(cid:124)

(cid:124)

−η1η2 ˜x ˜x
(η2 − η2
2) ˜x ˜x
...

−ηmη2 ˜x ˜x

(cid:124)

(cid:124)

···
···
. . .
···

−η1ηm ˜x ˜x
−η2ηm ˜x ˜x

(cid:124)
(cid:124)

...
(ηm − η2

m) ˜x ˜x

 .

(cid:124)

(10)

(cid:124)
k



2.8 RFIM of Two layers

Consider a two layer structure, where the output y satisﬁes a multivariate Bernoulli distribution
with independent dimensions. By a similar analysis to Subsec. 2.1, we have

gy(W ) =

νf (cl, h)

(cid:124)
∂c
l h
∂W

(cid:124)
∂c
l h
∂W (cid:124) .

(11)

It can be written block by block as gy(W ) = [Gij]Dh×Dh
, where each block Gij means the
correlation between the i’th hidden neuron with weights wi and the j’th hidden neuron with
weights wj. By eq. (11),

Dy(cid:88)

l=1

Dy(cid:88)
Dy(cid:88)

l=1

=

l=1

l=1

Dy(cid:88)
Dy(cid:88)
Dy(cid:88)

l=1

Gij =

=

=

νf (cl, h)

(cid:124)
∂c
l h
∂wi

(cid:124)
l h
(cid:124)
j

∂c
∂w

=

νf (cl, h)cilcjl

∂hi
∂wi

∂hj
(cid:124)
∂w
j

νf (cl, h)

∂cilhi
∂wi

∂cjlhj
(cid:124)
∂w
j

(cid:124)
νf (cl, h)cilcjl (νf (wi, x) ˜x) (νf (wj, x) ˜x

)

cilcjlνf (cl, h)νf (wi, x)νf (wj, x) ˜x ˜x

(cid:124)

.

(12)

l=1

The proof of the other case, where two relu layers have stochastic output y satisfying a
multivariate Gaussian distribution with independent dimensions, is very similar and is omitted.

7

3 Proof of Theorem 3

Proof. By assumption, the joint distribution p(x, h) is in a factorable form. Therefore

log p(x, h) =

log p(hl | θl, rl),

(13)

where l = 1,··· , L is the index of subsystems, hl is the subsystem output, and rl is the reference

of the subsystem. We have(cid:85)L

l=1

l=1{hl} = {x, h} and(cid:85)L

L(cid:88)

(cid:18)

− ∂2
∂θl∂θ

(cid:124)
l

(cid:19)

log p(x, h)

= Ep

(cid:19)

l=1{θl} = {Θ}. Therefore
(cid:18)
log p(hl | θl, rl)

− ∂2
∂θl∂θ

(cid:124)
l

log p(hl | θl, rl)

(cid:19)(cid:19)

(cid:124)
l

− ∂2
∂θl∂θ

(cid:18)
(cid:18)
(cid:0)ghl (θl)(cid:1) ,
(cid:33)

= Ep(rl)

Ep(hl | rl)

= Ep

(cid:32)

Ep

and

Ep

− ∂2
∂θl1∂θ

(cid:124)
l2

log p(x, h)

= 0 (∀l1 (cid:54)= l2).

Based on the Hessian expression of RFIM, J (Θ) is in a block-diagonal form, with each block
given by Ep

(cid:0)ghl (θl)(cid:1).

4 Experimental Settings & Zoomed Learning Curves

The training/validation/testing sets have 50,000/10,000/10,000 images, respectively. Each sam-
ple is a gray scale image of size 28 × 28 (784 dimensional feature space) and is labeled as one of
ten diﬀerent classes. For all methods, the mini-batch size is ﬁxed to 50 and the L2 regularization
strength is ﬁxed to 10−3. For each optimizer, we try to ﬁnd the best learning rate in the range
{··· , 10−1, 5× 10−2, 10−2, 5× 10−3, 10−3,···}. On the tested architectures, a good learning rate
conﬁguration for RNGD is usually around 10−2 or 5 × 10−3. The optimizers are in their default
settings in TensorFlow 1.0. For the Adam optimizer, β1 = 0.9, β2 = 0.999,  = 10−8. For RNGD,
we set empirically T = 100, λ = 0.005 and ω = 1. We use the Glorot uniform initializer to set
the initial weights.

For each method and each learning rate conﬁguration, we try 40 independent runs with
diﬀerent random seeds. Then, we select the best conﬁguration based on the validation accuracy.
Then, we plot the 40 learning curves as well as the average validation curve. The learning curves
are obtained by evaluating the training error and validation accuracy after each epoch (one pass
over all available training data).

See the following ﬁgs. (1–4) for the learning curves on four diﬀerent architectures with relu
activation units and L2 regularization. Only the training curves and validation curves are shown
for a clear presentation. The testing accuracy is close to the validation accuracy (run our codes
to see the detailed results).

8

Figure 1: A MLP with shape 784–80–80–80–10.

9

0.9660.9680.9700.9720.9740.976accuracy020406080100#epochs0.100.150.200.250.300.350.40errorPLAIN+SGD (train)PLAIN+SGD (valid)PLAIN+ADAM (train)PLAIN+ADAM (valid)PLAIN+RNGD (train)PLAIN+RNGD (valid)Figure 2: A MLP with shape 784–80–80–80–10 and batch normalization after each hidden layer.

10

0.9700.9710.9720.9730.9740.9750.9760.9770.978accuracy020406080100#epochs0.10.20.30.40.5errorBNA+SGD (train)BNA+SGD (valid)BNA+ADAM (train)BNA+ADAM (valid)BNA+RNGD (train)BNA+RNGD (valid)Figure 3: A MLP with shape 784–100–100–100–10.

11

0.9660.9680.9700.9720.9740.9760.978accuracy020406080100#epochs0.100.150.200.250.300.350.40errorPLAIN+SGD (train)PLAIN+SGD (valid)PLAIN+ADAM (train)PLAIN+ADAM (valid)PLAIN+RNGD (train)PLAIN+RNGD (valid)Figure 4: A MLP with shape 784–100–100–100–10 and batch normalization after each hidden
layer.

12

0.9700.9720.9740.9760.978accuracy020406080100#epochs0.10.20.30.40.5errorBNA+SGD (train)BNA+SGD (valid)BNA+ADAM (train)BNA+ADAM (valid)BNA+RNGD (train)BNA+RNGD (valid)