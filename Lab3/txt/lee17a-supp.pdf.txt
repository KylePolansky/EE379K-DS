Supplementary Material: Bayesian inference on random simple graphs

with power law degree distributions

Juho Lee 1 Creighton Heaukulani 2 Zoubin Ghahramani 2 3 Lancelot F. James 4 Seungjin Choi 1

1. Proofs
We prove Theorem 3.1 and Theorem 5.1 in the paper. First consider the following redeﬁnition of our model with slightly
different notation; let Wn be a random variable constrained on (0, Cn], with density
w−α−1(1 − e−w)1{0<w≤Cn}dw,

fn(dw) =

(1)

1
Zn

where C1, C2, . . . , is a sequence of positive numbers satisfying

(2)
Note that Zn → Γ(1 − α)/α as n → ∞, and so the sequence of densities fn(dw) converges pointwise to the density of
the BFRY distribution

n→∞ C α
lim

n /n = 0.

n→∞ Cn = ∞,

lim

f (w) =

α

Γ(1 − α)

w−α−1(1 − e−w)1{w>0},

(3)

and Wn converges in distribution to a BFRY random variable. Let Wn,1, . . . , Wn,n be n i.i.d. copies of Wn. A random
simple graph X is then deﬁned to be a collection of Bernoulli random variables as follows:
Wn,i√
,
Ln
i=1 Wn,i. We will write X | r ∼ GRG(n, r), where r := (ri,j : i < j ≤ n).

where Ln :=(cid:80)n

P{Xij = 1 | ri,j} =

ri,j = UiUj, Ui =

1 + ri,j

ri,j

(4)

,

We begin with a sequence of Lemmas. Deﬁne a sequence of random variables Vs,n, for every s, n ≥ 1, by

Let Vs,n,1, . . . , Vs,n,n be n i.i.d. copies of Vs,n, and denote the empirical mean of these copies by

The expectation of Vs,n is ﬁnite for all s, n < ∞, and is computed as

Vs,n :=

Wn
C s−α

n

.

¯Vs,n :=

E[Vs,n] =

=

1

(cid:26) 1

ZnC s−α
1
Zn

n

s − α

i=1

1
n

Vs,n,i.

n(cid:88)
(cid:90) Cn
− γ(s − α, Cn)

0

C s−α

n

(cid:27)

,

ws−α−1(1 − e−w)dw

(5)

(6)

(7)

1Pohang University of Science and Technology (POSTECH), Pohang, South Korea 2University of Cambridge, Cambridge, United
Kingdom 3Uber AI Labs, San Francisco, CA, USA 4Hong Kong University of Science and Technology (HKUST), Clearwater Bay,
Hong Kong. Correspondence to: Juho Lee <stonecold@postech.ac.kr>.

Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, 2017. JMLR: W&CP. Copyright 2017 by
the author(s).

where γ(·,·) is the lower incomplete gamma function.

Supp: Power-law simple graphs

P→ denote convergence in probability. The following lemma is a standard mean convergence result:

Let
Lemma 1.1. ¯Vs,n

P→ E[Vs,n], as n → ∞.

Proof. For all ε > 0, by Chebyshev’s inequality and the condition in Eq. (2),

P{| ¯Vs,n − E[Vs,n]| ≥ ε} ≤ Var(Vs,n)

nε2

≤ E[V 2
s,n]
nε2 =

1

Znε2

as n → ∞, as desired.

(cid:26)

C α
n

n(2s − α)

− γ(2s − α, Cn)
nC 2s−2α

n

The following lemma will be used to study various higher order moments in later results:
Lemma 1.2. For s ≥ 2,

(cid:80)n
((cid:80)n

i=1 W s
n,i
i=1 Wn,i)s

Ms,n :=

(cid:27)

→ 0,

(8)

(9)

(10)

Proof. We have

Ms,n =

nC s−α
nsC s−sα

n

n

¯Vs,n
¯V s
1,n

P→ 0,

as n → ∞.

(cid:18) C α

n
n

(cid:19)s−1 ¯Vs,n

¯V s
1,n

.

=

Recall that Dn,i :=(cid:80)

As n → ∞, the ﬁrst factor on the right hand side clearly converges to zero (c.f. Eq. (2)), and, by Lemma 1.1, the second
term converges to a constant in probability.

j(cid:54)=i Xi,j is the degree of the i-th node in the graph X | r ∼ GRG(n, r), given by Eq. (4). The
following result will show up in later calculations involving the probability generating function (PGF) of the degree random
variables Dn,i:
Lemma 1.3. For every collection t1, . . . , tn with |ti| ≤ 1, for i ≤ n,

E(cid:104) n(cid:89)

(cid:105)

(cid:89)

i<j≤n

tDn,i
i

| Wn,1 = w1, . . . , Wn,n = wn

=

Ln + titjwiwj

Ln + wiwj

,

(11)

i=1

for positive w1, . . . , wn.

Proof. The proof is given by Britton et al. (2006).

(cid:89)

i(cid:54)=k

where Ln,−k :=(cid:80)

hold:

1. F (s)
2. F (s)

n,k(t; wk) is uniformly bounded, for all n ≥ 1;
k exp{(t − 1)wk}, as n → ∞.
n,k(t; wk)

P→ ws

The following result studies a representation of the PGF of the degree random variables and their higher order moments:
Lemma 1.4. Fix a node k ≤ n. Deﬁne

Fn,k(t; wk) :=

Ln,−k + wk + twkWn,i
Ln,−k + wk + wkWn,i

,

for |t| ≤ 1, and wk > 0,

(12)

i(cid:54)=k Wn,i. Note that the s-th derivative F (s)

n,k(t; wk) exists for all s ≥ 0. For all s ≥ 0, the following

Proof. In the case s = 0, Fn,k(t; wk) is trivially bounded by 1 since |t| ≤ 1. By the Taylor series expansion log(1 + x) =
x + O(x2), we have

Supp: Power-law simple graphs

(cid:26)

(t − 1)wk

Ln,−k

Ln,−k + wk

+ O

w2
k

i(cid:54)=k W 2
n,i

(Ln,−k + wk)2

(cid:18)

(cid:80)

(cid:19)(cid:27)

.

Fn,k(t; wk) = exp

By Lemma 1.1,

Ln,−k

Ln,−k + wk

=

¯V1,n,−k

¯V1,n,−k + wk/(n − 1)/C 1−α

n

P→ 1,

where ¯V1,n,−k is the empirical mean in Eq. (6) excluding the element V1,n,k. Furthermore, by Lemma 1.2,

(cid:18)

(cid:80)

(cid:19)

O

w2
k

i(cid:54)=k W 2
n,i

(Ln,−k + wk)2

≤ O(w2

kM2,n,−k)

P→ 0,

where Ms,n,−k is Ms,n computed without Vs,n,k. Combining, we have

Fn,k(t; wk)

P→ exp{(t − 1)wk}.

Before proceeding for s ≥ 1, we deﬁne

Qr,n,k(t; wk) :=

(cid:88)

i(cid:54)=k

W r
n,i

(Ln,−k + wk + twkWn,i)r ,

for all r, n ≥ 1. One can easily see that Qr,n,k(t; wk) ≤ 1 for all r, n ≥ 1. For r = 1, we have

(cid:88)

Wn,i

≤ Q1,n,k(t; wk) ≤ 1,

Ln,−k + wk + twkCn

i(cid:54)=k

and

(cid:88)

i(cid:54)=k

Wn,i

Ln,−k + wk + twkCn

=

1

1 + wk/Ln,−k + twkCn/Ln,−k

(cid:26)

=

1 +

wk

(n − 1)C 1−α

n

¯V −1
s,n,−k + twk

C α
n
n

n
n − 1

¯V −1
s,n,−k

(cid:27)−1 P→ 1.

Hence, by the squeeze theorem, Q1,n,k(t; wk)

P→ 1. For r ≥ 2, we have
0 ≤ Qr,n,k(t; wk) ≤ Mr,n,−k

P→ 0,

by Lemma 1.2. Hence, we have Qr,n,k(t; wk)
Now we show that

P→ 0 for r ≥ 2.

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

for some constants {as,r} for all s ≥ 1 and r ≥ 2. We proceed by the mathematical induction. For s = 1,

(t; wk)Q1,n,k(t; wk) +

(t; wk)Qr,n,k(t; wk),

(21)

n,k

n,k(t; wk) = wkF (s−1)
F (s)
(cid:88)

F (1)

n,k(t; wk) =

wkWn,i

Ln,−k + wk + wkWn,i

i(cid:54)=k

j(cid:54)=i,k

Ln,−k + wk + twkWn,j
Ln,−k + wk + wkWn,j

= wkFn,k(t; wk)Q1,n,k(t; wk).

(22)

s(cid:88)

r=2

n,k

as,rF (s−r)
(cid:89)

Now by the inductive hypothesis,

Supp: Power-law simple graphs

F (s+1)

n,k

(t; wk) = wkF (s)

(t; wk)Q2,n,k(t; wk)

s(cid:88)

r=2

+

n,k

kF (s−1)
n,k(t; wk)Q1,n,k(t; wk) − w2
as,r(F (s+1−r)
s+1(cid:88)

n,k(t; wk)Q1,n,k(t; wk) +

n,k

= wkF (s)

(t; wk)Qr,n,k(t; wk) − rwkF (s−r)

n,k Qr+1,n,k(t; wk))

as+1,rF (s+1−r)

n,k

(t; wk)Qr,n,k(t; wk),

where

r=2

as+1,2 = as,2 − w2
k,

as+1,r = as,r − as,r−1(r − 1)wk

for r ≥ 2,

so the inductive argument holds.

Having (21), by mathematical induction, we can easily show that F (s)
Moreover,

n,k(t; wk) is uniformly bounded for all s, n ≥ 1.

by (16) and (19). Combining this with (20), by mathematical induction, we can show that for all s ≥ 1,

F (1)

n,k(t; wk) = wkFn,k(t; wk)Q1,n,k(t; wk)

P→ wk exp{(t − 1)wk},

F (s)

n,k(t; wk)

P→ ws

k exp{(t − 1)wk}.

We will now use our collected results to analyze the asymptotic distribution of the degree random variables; the following
result characterizes this distribution:
Lemma 1.5. Fix a node k. Given {Wn,k = wk}, for some wk > 0, the degree Dn,k of node k converges in distribution to
a Poisson random variable with rate wk, as n → ∞.

Proof. The PGF of Dn,k is given by

E[tDn,k | Wn,k = wk] = E[Fn,k(t; wk)],

(27)
Note that these expectations are under the σ-ﬁeld generated by {Wk = wk}. For all s ≥ 0, we will derive the limit of
P{Dn,k = s | wk}, as n → ∞, which we note is given by the s-th order derivatives of the PGF in Eq. (27), evaluated at
k exp{(t − 1)wk}, as n → ∞, for all s ≥ 0.
the argument t = 0. It therefore sufﬁces to show that E[F (s)
k exp{(t − 1)wk}, as n → ∞.
By Lemma 1.4, we know that F (s)
n,k(t; wk)
Therefore, by uniform integrability,

n,k(t; wk)] → ws
n,k(t; wk) is uniformly bounded and that F (s)

P→ ws

for |t| ≤ 1.

n,k(t; wk)] = E(cid:104)

E[F (s)

lim
n→∞

(cid:105)

n→∞ F (s)
lim

n,k(t; wk)

= ws

k exp{(t − 1)wk}.

(28)

We are now ready to prove the main theorems in the paper.
Proof of Theorem 3.1. We will ﬁrst verify that, for y (cid:29) 1, P{Dn,k = y} → cy−1−α for every node k and for some
constant c > 0 as n → ∞. By Lemma 1.5, conditioned on {Wk = wk}, the degree Dn,k converges in distribution to a
Poisson random variable with rate wk. Then by dominated convergence,

(23)

(24)

(25)

(26)

(29)

lim
n→∞

P{Dn,k = y} = lim
n→∞

P{Dk = y|wk}pn(dwk)

(cid:90) ∞

(cid:90) ∞
0
ke−wk
wy
y!
αΓ(y − α)
y!Γ(1 − α)

0

=

=

p(dwk)

(1 − 2α−y).

By the asymptotics of the Gamma function, for y (cid:29) 1, we have

Supp: Power-law simple graphs

P{Dn,k = y} = cy−1−α,

lim
n→∞

(30)

for some constant c.
Next we show that, for any ﬁnite m, the collection of random variables Dn,1, . . . , Dn,m are asymptotically independent,
as n → ∞. We compute the (joint) probability generating function of (Dn,1, . . . , Dn,m), with |ti| ≤ 1 for i = 1, . . . , m.
By Lemma 1.3,

E(cid:104) m(cid:89)

i=1

tDn,i
i

(cid:105)

i=1

= E(cid:104) m(cid:89)
m(cid:89)
= E(cid:104)E(cid:104) m(cid:89)
m(cid:89)
× n(cid:89)

j=i+1

i=1

j=i+1

n(cid:89)

(cid:105)

Ln + titjWn,iWn,j

Ln + Wn,iWn,j

j=m+1

Ln + tiWn,iWn,j
Ln + Wn,iWn,j

Ln,m+1:n + (cid:96)n,1:m + titjwiWn,j

Ln,m+1:n + (cid:96)n,1:m + wiWn,j

Ln,m+1:n + (cid:96)n,1:m + tiwiWn,j
Ln,m+1:n + (cid:96)n,1:m + wiWn,j

| Wn,1:m = w1:m

j=m+1

Given w1:m, by a similar argument as in the proof of Lemma 1.4, one can easily show that

m(cid:89)

j=i+1

Ln,m+1:n + (cid:96)n,1:m + titjwiWn,j

Ln,m+1:n + (cid:96)n,1:m + wiWn,j

P→ 1,

as n → ∞,

and

n(cid:89)

j=m+1

Ln,m+1:n + (cid:96)n,1:m + tiwiWn,j
Ln,m+1:n + (cid:96)n,1:m + wiWn,j

P→ exp{(ti − 1)wi},

as n → ∞.

Hence, again by a similar argument as in the proof of Lemma 1.4, we have

E(cid:104) m(cid:89)

(cid:105)

m(cid:89)

lim
n→∞

tDn,i
i

=

i=1

i=1

E[exp{(ti − 1)Wi}],

(cid:105)(cid:105)

.

(31)

(32)

(33)

(34)

We evaluate the derivative of the PGF to obtain the ﬁrst moment

that is, the joint PGF asymptotically factorizes into the product of the PGFs for i.i.d. random variables, and the result
follows.

Proof of Theorem 3.2. Using the fact that the expected number of nodes En :=(cid:80)n

i=1 Dn,i/2, we may take t1 = ··· =

√

tn =

t and obtain

E[tEn] = E(cid:104) (cid:89)
= E(cid:104) (cid:88)

i<j≤n

Wn,iWn,j

i<j≤n

Ln + Wn,iWn,j

(cid:12)(cid:12)(cid:12)t=1

(cid:105) ≤ 1

2

Ln + tWn,iWn,j
Ln + Wn,iWn,j

(cid:110) C 1−α

n

1 − α

E[Wn] =

1
Zn

− γ(1 − α, Cn)

,

.

(cid:105)
E(cid:104) (cid:88)
(cid:111)

i≤j≤n

E[En] = O(nC 1−α

n

).

(cid:105)

Wn,iWn,j

Ln

=

n
2

E[Wn].

(35)

(36)

(37)

(38)

E[En] =

∂E[tEn ]

∂t

Since

we have

Proof of Theorem 5.1. Recall that

P{X = x | r} =

where A := (Ai,j)i<j≤n and

Since(cid:80)

x

P{X = x | r} = 1, we have

Supp: Power-law simple graphs

(cid:89)

ri,j

i<j≤n

1 + ri,j

(cid:89)

i<j≤n

G(r) :=

(cid:89)

i<j≤n

Axi,j
i,j

n(cid:89)

i=1

U Dn,i

i

,

= G−1(r)

(1 + Ai,jUiUj).

(cid:88)

(cid:89)

i<j≤n

x

Axi,j
i,j

n(cid:89)

i=1

uDn,i
i

.

G(r) =

The joint PGF of (Dn,1, . . . , Dn,n) is then

E(cid:104) n(cid:89)

i=1

tDn,i
i

| A, Wn,1:n

(cid:105)

=

(cid:88)

x

P{X = x | r} n(cid:89)
(cid:89)
(cid:88)

i=1

i<j≤n

x

= G−1(r)

(cid:89)

=

i<j≤n

1 + Ai,jtitjUiUj

1 + Ai,jUiUj

Axi,j
i,j

(tiUi)Dn,i

tDn,i(x)
i

n(cid:89)

i=1

.

(39)

(40)

(41)

(42)

The remainder of the proof follows analogously to the proof of Theorem 3.1 above.

References
Britton, T., Deijfen, M., and Martin-L¨of, A. Generating simple random graphs with prescribed degree distribution. Journal

of Statistical Physics, 124(6):1377–1397, 2006.

Kingma, D. P. and Welling, M. Auto-encoding variational Bayes. In ICLR, 2014.

Knowles, D. A.

Stochastic gradient variational Bayes for gamma approximating distributions.

arXiv:1509.01631, 2015.

arXiv preprint

Salimans, T. and Knowles, D. A. Fixed-form variational posterior approximation through stochastic linear regression.

Bayesian Analysis, 8(4):837–882, 2013.

