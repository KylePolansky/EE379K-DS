Recursive Partitioning for Personalization using Observational Data

Supplementary Material for:

Nathan Kallus 1

Omitted Proofs
Proof of Theorem 1. By Asn. 1, we have

E [Y | X = x, T = t] = E [Y (T ) | X = x, T = t]
= E [Y (t) | X = x, T = t]
= E [Y (t) | X = x]

(deﬁnition of Y = Y (T ))
(conditioned on T = t)
(Asn. 1).

Consider a realization of the data and X = x where convergence occurs for all t ∈ [m]. Let

(x) = inf{ζ : s ∈ [m], ζ =(cid:0)E [Y | X = x, T = s] − mint∈[m] E [Y | X = x, T = t](cid:1) > 0},

where inf(∅) = ∞. By assumption of convergence at this realization of the data and X = x, we have that even-
tually for all t ∈ [m], |ˆµt,nt(x) − E [Y | X = x, T = t]| < (x)/2, at which point we must necessarily also have
ˆτn(x) ∈ arg mint∈[m] E [Y | X = x, T = t] = arg mint∈[m] E [Y (t) | X = x]. By assumption of pointwise consistency
and because the intersection of ﬁnitely many a.s. events is a.s., the set of such realization of the data and X = x have
probability 1.

Proof of Theorem 2. First note that, given any x with P (T = t | X = x) > 0, we have

E [Y | X = x, T = t] =

P(T =t|X=x) = E(cid:104) Y I[T =t]

E[Y I[T =t]|X=x]

φ(t,x)

| X = x

(cid:105)

= E(cid:104) Y I[T =t]

φ(T,X) | X = x

(cid:105)

= E(cid:104) Y I[T =t]

Q

(cid:105)

.

| X = x

Therefore, since P (T = t | X) > 0 almost surely,

R(τ ) = E [Y (τ (X))] = E [E [Y (τ (X)) | X]]

= E [E [Y (τ (X)) | X, T = τ (X)]]
= E [E [Y | X, T = τ (X)]]
= E [E [Y I [T = τ (X)]/Q | X]]
= E [Y I [T = τ (X)]/Q]

(iterated expectations)
(Asn. 1)
(deﬁnition of Y )
(above observation)
(iterated expectations) .

Proof of Theorem 4. We start with 1vA. Restrict to x such that φ(s, x) > 0 ∀s (almost everywhere). Let µ(t, x) =
E [Y (t) | X = x]. Under Asn. 1,

δtvA(x) = E [Y | X = x, T = t] − E [Y | X = x, T (cid:54)= t]

= E [Y | X = x, T = t] −(cid:80)
= µ(t, x) −(cid:80)

s(cid:54)=t φ(s, x)µ(s, x)/(cid:80)

s(cid:54)=t

s(cid:54)=t φ(s, x).

E [Y | X = x, T = s] P (T = s | X = x, T (cid:54)= t)

1School of Operations Research and Information Engineering and Cornell Tech, Cornell University. Correspondence to: Nathan

Kallus <kallus@cornell.edu>.

Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the
author(s).

Supplementary Material for: Recursive Partitioning for Personalization using Observational Data

Since φ(s, x) > 0, it’s clear that δtvA(x) ≤ δsvA(x) ∀s if and only if µ(t, x) ≤ µ(s, x) ∀s. The rest of the proof for 1vA
follows the same way as Thm. 1, showing that, under the assumption of pointwise consistent estimation, the estimation
gap supt∈[m]

(cid:12)(cid:12)(cid:12) is eventually smaller than half the decision gap, 1vA(x) = inf{ζ : s ∈ [m], ζ =

(cid:0)δsvA(x) − mint∈[m] δtvA(x)(cid:1) > 0}, a.s. and for almost everywhere x.

n (x) − δtvA(x)

(cid:12)(cid:12)(cid:12)ˆδtvA

Next, we deal with 1v1-A. Fix x. Fix any tm ∈ arg maxt∈[m] µ(t, x). Let δtvmin(x) = mins(cid:54)=t δtvs(x). If t, s (cid:54)= tm, then
δtvmin(x)−δsvmin(x) = µ(t, x)−µ(s, x). On the other hand, for any t ∈ [m], we always have both µ(t, x)−µ(tm, x) ≤ 0
and δtvmin(x) − δtmvmin(x) ≤ 0. Therefore, we have

t ∈ arg mint∈[m] µ(t, x) ⇐⇒ µ(t, x) − µ(s, x) ≤ 0 ∀s (cid:54)= t ⇐⇒ µ(t, x) − µ(s, x) ≤ 0 ∀s (cid:54)= t, tm

⇐⇒ δtvmin(x) − δsvmin(x) ≤ 0 ∀s (cid:54)= t, tm ⇐⇒ δtvmin(x) − δsvmin(x) ≤ 0 ∀s (cid:54)= t
⇐⇒ t ∈ arg mint∈[m] δtvmin(x).

(x)

(x)

nt+ns

and

note

that

mins(cid:54)=t

ˆδtvs
nt+ns

Let

ˆδtvmin
n

(cid:12)(cid:12)(cid:12)ˆδtvs

=
(x) − δtvs(x)

supt∈[m],s∈[m]
for 1v1-A follows as above, showing that this estimation gap is eventually smaller than half the decision gap,

1v1-A(x) = inf{ζ : s ∈ [m], ζ =(cid:0)δsvmin(x) − mint∈[m] δtvmin(x)(cid:1) > 0}, a.s. and for almost everywhere x.
(cid:12)(cid:12)(cid:12) ≤ |δtvs(x)| /2 for all t (cid:54)= s such that δtvs(x) (cid:54)= 0. That is, eventually I(cid:104)ˆδtvs
(cid:12)(cid:12)(cid:12)ˆδtvs
I [δtvs(x) < 0] for all t (cid:54)= s such that δtvs(x) (cid:54)= 0. Restrict to such large enough n. Let kt(x) = (cid:80)
I(cid:104)ˆδtvs
ˆkt(x) =(cid:80)

(cid:12)(cid:12)(cid:12), which converges to zero under pointwise consistency. The rest of the proof
(cid:105)
, and kmin(x) =(cid:12)(cid:12)arg mint∈[m] µ(t, x)(cid:12)(cid:12). Then, t ∈ arg mint∈[m] µ(t, x) ⇐⇒ kt(x) =

Next, we deal with 1v1-B. Fix x and a realization of the data where convergence holds for all t (cid:54)= s. Then, eventually
=
I [δtvs(x) < 0],

(x) − δtvmin(x)

(x) − δtvs(x)

nt+ns
t(cid:54)=s

supt∈[m]

(x) < 0

(x) < 0

(cid:105)

t(cid:54)=s

nt+ns

nt+ns

≤

(cid:12)(cid:12)(cid:12)ˆδtvmin

n

(cid:12)(cid:12)(cid:12)

m − kmin(x) ⇐⇒ ˆkt(x) ≥ m − kmin(x) ⇐= t ∈ arg maxt∈[m]

(cid:80)

I(cid:104)ˆδtvs

s(cid:54)=t

nt+ns

(x) < 0

(cid:105)

.

Proof of Theorem 5. By random sampling, (Xij , Tij , Yij (1), . . . , Yij (m)) are distributed iid as (X, T, Y (1), . . . , Y (m))
is in population. For j ∈ [ntest], let ijt be ij’s match for treatment t, or ij if Tij = t. Under exact matching,
(cid:80)m
Yijt(1), . . . , Yijt(m) | Xji is distributed the same as Yij (1), . . . , Yij (m) | Xji, Tji = t. By writing ˆYij t = Yijt =

s=1

I [t = s] Yijs(s), we see that

E[ ˆYij τ (Xij )] = E(cid:2)E(cid:2)(cid:80)m
=(cid:80)m
=(cid:80)m
=(cid:80)m
= E(cid:2)E(cid:2)(cid:80)m
= E(cid:2)Yij (τ (Xij ))(cid:3)

(cid:3)(cid:3)
I(cid:2)s = τ (Xij )(cid:3) Yijs(s) | Xij
E(cid:2)I(cid:2)s = τ (Xij )(cid:3) E(cid:2)Yijs(s) | Xji
(cid:3)(cid:3)
E(cid:2)I [s = τ (Xi)] E(cid:2)Yij (s) | Xi, Ti = s(cid:3)(cid:3)
(cid:3)(cid:3)
E(cid:2)I [s = τ (Xi)] E(cid:2)Yij (s) | Xi
(cid:3)(cid:3)

I [s = τ (Xi)] Yij (s) | Xi

s=1

s=1

s=1

s=1

s=1

(iterated expectation)
(linearity)
(exact matching)
(Asn. 1)
(linearity)
(iterated expectation)

