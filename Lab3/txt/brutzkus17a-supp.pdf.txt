Globally Optimal Gradient Descent for a ConvNet with

Gaussian Inputs

Supplementary Material

Contents

A Proof of Lemma 3.2

B Proof of Proposition 4.1

C Missing Proofs for Section 5

C.1 Proof of Proposition 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
C.2 Proof of Theorem 5.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

D Missing Proofs for Section 7.1

D.1 Proof of Proposition 7.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
D.2 Proof of Proposition 7.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

E Uniqueness of Global Minimum in the Population Risk

F Experimental Setup for Section 7.2

1

2

3
3
6

12
12
13

16

16

A Proof of Lemma 3.2
First assume that θu,v (cid:54)= 0, π . Then we have

∂g
∂ui

=

1
2π

1
2π

−

(cid:33)

+

+

+

vi

u·v

(cid:107)u(cid:107)(cid:107)v(cid:107)

(cid:32)

(cid:113)

(cid:107)u(cid:107)(cid:107)v(cid:107)

u·v

(cid:107)u(cid:107)(cid:107)v(cid:107)

(cid:17)(cid:17) u · v
(cid:33)

(cid:107)u(cid:107)(cid:107)v(cid:107)

(cid:16) u · v

(cid:107)u(cid:107)(cid:107)v(cid:107)

π − arccos

(cid:16)
(cid:33)(cid:32)

(cid:17)2
(cid:17)2

(cid:32)(cid:115)
(cid:107)v(cid:107) ui(cid:107)u(cid:107)
(cid:32)(cid:32)
(cid:107)u(cid:107)(cid:107)v(cid:107)

1 −(cid:16) u · v
(cid:114)
1 −(cid:16) u·v
(cid:32)
(cid:107)u(cid:107)(cid:107)v(cid:107)
1 −(cid:0) u·v
(cid:1)2
(cid:107)u(cid:107)(cid:107)v(cid:107) − ui
(cid:17)(cid:17)(cid:32)
(cid:107)u(cid:107)2
(cid:16) u · v
(cid:32)(cid:115)
(cid:17)2
1 −(cid:16) u · v
(cid:107)v(cid:107) ui(cid:107)u(cid:107)
(cid:16) u · v
(cid:107)u(cid:107)(cid:107)v(cid:107)(cid:16)

(cid:107)u(cid:107)(cid:107)v(cid:107) − ui
(cid:107)u(cid:107)2
(cid:16)
(cid:17)(cid:17)(cid:32)

π − arccos

π − arccos

π − arccos

u · v
(cid:107)u(cid:107)(cid:107)v(cid:107)

u · v
(cid:107)u(cid:107)(cid:107)v(cid:107)

(cid:107)u(cid:107)(cid:107)v(cid:107) − ui
(cid:107)u(cid:107)2
(cid:33)(cid:33)
(cid:33)(cid:33)
(cid:33)
(cid:17)(cid:17) u · v
(cid:33)(cid:33)

(cid:16) u · v

u · v
(cid:107)u(cid:107)(cid:107)v(cid:107)

(cid:107)u(cid:107)(cid:107)v(cid:107)

(cid:107)u(cid:107)(cid:107)v(cid:107)
u · v
(cid:107)u(cid:107)(cid:107)v(cid:107)

(cid:107)u(cid:107)(cid:107)v(cid:107) − ui
(cid:107)u(cid:107)2

(cid:107)u(cid:107)(cid:107)v(cid:107)

(cid:107)u(cid:107)(cid:107)v(cid:107)

(cid:107)u(cid:107)(cid:107)v(cid:107)

+

(cid:107)u(cid:107)(cid:107)v(cid:107)

+

=

vi

vi

+

=

(cid:16)

1
2π

1
2π

vi

1

(cid:17)2

(cid:115)
1 −(cid:16) u · v
(cid:107)v(cid:107) ui(cid:107)u(cid:107) sin θu,v +

(cid:107)u(cid:107)(cid:107)v(cid:107)

1
2π

+

1
2π

1
2π

(cid:107)v(cid:107) ui(cid:107)u(cid:107)

Hence,

(cid:16)

1
2π

(cid:16)

(cid:17)(cid:17)

vi =

(cid:16) u · v
(cid:17)

(cid:107)u(cid:107)(cid:107)v(cid:107)

π − arccos

π − θu,v

vi

∂g
∂u

=

1
2π

(cid:107)v(cid:107) u

(cid:107)u(cid:107) sin θu,v +

1
2π

(cid:16)

(cid:17)

v

π − θu,v

(1)

Now we assume that u is parallel to v. We ﬁrst show that g is diﬀerentiable in this case. Without
loss of generality we can assume that u and v lie on the u1 axis. This follows since g is a function of
(cid:107)u(cid:107), (cid:107)v(cid:107) and θu,v and therefore g(·, v) has a directional derivative in direction d at u if and only if
g(·, Rv) has a directional derivative in direction Rd at Ru where R is a rotation matrix. Hence g(·, v)
is diﬀerentiable at u if and only if g(·, Rv) is diﬀerentiable at Ru. Furthermore, if v and u are on the
u1 axis, then by symmetry the partial derivatives with respect to other axes at u are all equal, hence
we only need to consider the partial derivative with respect to the u1 and u2 axes.
Let v = (1, 0, ..., 0) and u = (u, 0, ..., 0) where u (cid:54)= 0. In order to show diﬀerentiability, we will
prove that g(u, v) has continuous partial derivatives at u (by equality (1) the partial derivatives are
clearly continuous at points that are not on the u1 axis. Deﬁne u = (u, , 0, ..., 0). Then

(cid:32)

1

2π (cid:107)u(cid:107)(cid:107)v(cid:107)

∂g
∂u2

(u, v) = lim
→0

sin θu,v +

cos θu,v

− g(u, v)

(cid:33)

(cid:17)

(cid:16)

π − θu,v



By L’hopital’s rule and the calculation of equality (1) we get

∂g
∂u2

(u, v) = lim
→0

1
2π

(cid:107)v(cid:107)


(cid:107)u(cid:107) sin θ = 0
(u(cid:48), v) = 0 since limu(cid:48)→u sin θu(cid:48),v = 0.

Furthermore, by equality (1) we see that limu(cid:48)→u

For a ﬁxed θu,v equal to 0 or π, ∂g
∂u1

(u, v) is the same as

∂g
∂(cid:107)u(cid:107) (u, v). Hence,

(cid:32)

∂g
∂u2

(cid:16)

(cid:17)

(cid:33)

(cid:26) 1

∂g
∂u1

(u, v) =

(cid:107)v(cid:107)

1
2π

sin θu,v +

π − θu,v

cos θu,v

=

2 if u > 0
0 if u < 0

and the partial derivative is continuous since

lim
u(cid:48)→u

∂g
∂u1

(u(cid:48), v) =

(cid:26) 1

2 if u > 0
0 if u < 0

Finally, we see that for the case where u and v are parallel, the values we got for the partial

derivatives coincide with equation Eq. 1. This concludes the proof.

B Proof of Proposition 4.1

We will prove the claim by induction on k. For the base case we will show that Set-Splitting-by-2-Sets
is NP-complete. We will prove this via a reduction from a variant of the 3-SAT problem with the
restriction of equal number of variables and clauses, which we denote Equal-3SAT. We will ﬁrst prove
that Equal-3SAT is NP-complete.

Lemma B.1. Equal-3SAT is NP-complete.

2

Proof. This can be shown via a reduction from 3SAT. Given a formula φ with n variables and m
clauses we can increase n − m by 1 by adding a new clause of the form (x ∨ y) for new variables x
and y. Furthermore, we can decrease n − m by 1 by adding two new identical clauses of the form (z)
for a new variable z. In each case the formula with the new clause(s) is satisﬁable if and only if φ is.
Therefore given a formula φ we can construct a new formula ψ with equal number of variables and
clauses such that φ is satisﬁable if and only if ψ is.

We will now give a reduction from Equal-3SAT to Set-Splitting-by-2-Sets.

Lemma B.2. Set-Splitting-by-2-Sets is NP-complete.

Proof. The following reduction is exactly the reduction from 3SAT to Splitting-Sets and we include
it here for completeness. Let φ be a formula with set of variables V and equal number of variables
and clauses. We construct the sets S and C as follows. Deﬁne
S = {¯x | x ∈ V } ∪ V ∪ {n}

where ¯x is the negation of variable x and n is a new variable not in V . For each clause c with set
of variables or negations of variables Vc that appear in the clause (for example, if c = (¯x ∨ y) then
Vc = {¯x, y}) construct a set Sc = Vc ∪ {n}. Furthermore, for each variable x ∈ V construct a set
Sx = {x, ¯x}. Let C be the family of subsets Sc and Sx for all clauses c and x ∈ V . Note that |C|≤ |S|
which is required by the deﬁnition of Set-Splitting-by-2-Sets.
Assume that φ is satisﬁable and let A be the satisfying assignment. Deﬁne S1 = {x|A(x) =
true}∪{¯x|A(x) = f alse} and S2 = {x|A(x) = f alse}∪{¯x|A(x) = true}∪{n}. Note that S1∪ S2 = S.
Assume by contradiction that there exists a set T ∈ C such that T ⊆ S1 or T ⊆ S2. If T ⊆ S1 then T
is not a set Sc for some clause c because n /∈ S1. However, by the construction of S1 a variable and
its negation cannot be in S1. Hence T ⊆ S1 is impossible. If T ⊆ S2 then as in the previous claim
T cannot be a set Sx for a variable x. Hence T = Sc for some clause c. However, this implies that
A(c) = f alse, a contradiction.
Conversely, assume there exists splitting sets S1 and S2 and w.l.o.g. n ∈ S1. We note that it
follows that no variable x and its negation ¯x are both contained in one of the sets S1 or S2. Deﬁne
the following assignment A for φ. For all x ∈ V if x ∈ S1 let A(x) = f alse, otherwise let A(x) = true.
Note that A is a well deﬁned assignment. Assume by contradiction that there is a clause c in φ which
is not satisﬁable. Since S2 splits Sc it follows that there exists a variable x such that it or its negation
¯x are in S2 (recall that n ∈ S1). If x ∈ S2 then A(x) = true and if ¯x ∈ S2 then A(¯x) = true since
x ∈ S1. In both cases c is satisﬁable, a contradiction.

This proves the base case. We will now prove the induction step by giving a reduction from Set-
Splitting-by-k-Sets to Set-Splitting-by-(k+1)-Sets. Given S = {1, 2, ..., d} and C = {Cj}j such that |C|
≤ (k − 1)d, deﬁne S(cid:48) = {1, 2, ..., d + 1} and C(cid:48) = C ∪{Dj}j where Dj = {j, d + 1} for all 1 ≤ j ≤ d.
Note that |C(cid:48)| ≤ kd < k(d + 1). Assume that there are S1, ..., Sk that split the sets in C. Then if we
i=1 Si = S and S1, ..., Sk, Sk+1 are disjoint and split the sets
in C(cid:48).
Conversely, assume that S1, ..., Sk, Sk+1 split the sets in C(cid:48). Let w.l.o.g. Sk+1 be the set that
contains d + 1. Then for all 1 ≤ j ≤ d we have Dj (cid:54)⊆ Sk+1. It follows that for all 1 ≤ j ≤ d, j /∈ Sk+1,
i=1 Si = S and S1, ..., Sk are disjoint and split the sets in C,

deﬁne Sk+1 = {d + 1}, it follows that(cid:83)k+1
or equivalently, Sk+1 = {d + 1}. Hence,(cid:83)k

as desired.

C Missing Proofs for Section 5

C.1 Proof of Proposition 5.1

1. For w (cid:54)= 0, the claim follows from Lemma 3.2. As in the proof of Lemma 3.2 we can assume
that w = (0, 0, ..., 0) and w∗ = (1, 0, ..., 0). Let f (w, w∗) = 2kg(w, w∗) + (k2 −

w.l.o.g.

3

(cid:107)w(cid:107)(cid:107)w∗(cid:107)

k)
then by L’hopital’s rule

. It suﬃces to show that ∂f
∂w2

π

(w, w∗) does not exist. Indeed, let w = (0, , 0, ..., 0)

f (w, w∗) − f (w, w∗)



lim
→0+

= lim
→0+

k
π

(cid:107)w∗(cid:107) 

|| sin θw,w∗ + (k2 − k)

(cid:107)w∗(cid:107)
π

=

k
π

+

k2 − k

π

and

lim
→0−

f (w, w∗) − f (w, w∗)



= lim
→0−

k
π

(cid:107)w∗(cid:107) 

|| sin θw,w∗ − (k2 − k)

(cid:107)w∗(cid:107)
π

= − k
π

− k2 − k

π

Hence the left and right partial derivatives with respect to variable w2 are not equal, and thus
∂f
∂w2

(w, w∗) does not exist.

2. We ﬁrst show that w = 0 is a local maximum if and only if k > 1. Indeed, by considering the
loss function as a function of the variable x = (cid:107)w(cid:107), for any ﬁxed angle θw,w∗ we get a quadratic
function of the form (cid:96)(x) = ax2 − bx, where a > 0 and b ≥ 0. Since f (θ) = sin θ + (π − θ) cos θ
is a non-negative function for 0 ≤ θ ≤ π and f (θ) = 0 if and only if θ = π, it follows that
b = 0 if and only if k = 1 and θw,w∗ = π. Therefore if k > 1, then for all ﬁxed angles θw,w∗ ,
the minimum of (cid:96)(x) is attained at x > 0, which implies that w = 0 is a local maximum. If
k = 1 and θw,w∗ = π the minimum of (cid:96)(x) is attained at x = 0, and thus w = 0 is not a local
maximum in this case.

We will now ﬁnd the other critical points of (cid:96). By Lemma 3.2 we get

(cid:34)(cid:0)k +
(cid:34)(cid:32)

k +

(cid:16)

(cid:107)w∗(cid:107) w

(cid:1)w − k
(cid:107)w(cid:107) sin θw,w∗ − k
− k (cid:107)w∗(cid:107)
π (cid:107)w(cid:107) sin θw,w∗ − k2 − k

π

π

π
(cid:107)w∗(cid:107)
(cid:107)w(cid:107)

k2 − k

π
k2 − k

π

π − θw,w∗

(cid:33)

w − k
π

∇(cid:96)(w) =

=

1
k2

1
k2

(cid:35)

(cid:17)
w∗ − k2 − k
(cid:17)
(cid:16)

π − θw,w∗

π

w∗

(cid:107)w∗(cid:107) w
(cid:107)w(cid:107)

(cid:35)

and assume it vanishes.
Denote θ (cid:44) θw,w∗ . If θ = 0 then let w = αw∗ for some α > 0. It follows that

(2)

k2 − k

− k2 − k

k +

1
α

− k
α

= 0

π

k2−k

k2−k

k2+(π−1)k (cid:107)w∗(cid:107) and thus w = −(

k2+(π−1)k )w∗. By setting θ = π in the loss
k2+(π−1)k )w∗ is a one-dimensional local minimum, whereas
(cid:16)
k2+(π−1)k )w∗
= 0

π
or equivalently α = 1, and thus w = w∗.
If θ = π then (cid:107)w(cid:107) = k2−k
function, one can see that w = −(
by ﬁxing (cid:107)w(cid:107) and decreasing θ, the loss function decreases. It follows that w = −(
is a saddle point. If θ (cid:54)= 0, π then w and w∗ are linearly independent and thus k
which is a contradiction.
It remains to show that u = −γ(k)w∗ where γ(k) = k2−k
k2+(π−1)k is a degenerate saddle point. We
will show that the Hessian at u denoted by ∇2(cid:96)(u), has only nonnegative eigenvalues and at least
one zero eigenvalue. Let ˜(cid:96)(w) (cid:44) (cid:96)(w, Rw∗), where the second entry denotes the ground truth
weight vector and R is a rotation matrix. Denote by fd1,d2 the second directional derivative of
a function f in directions d1 and d2. Similarly to the proof of Lemma 3.2, since (cid:96) depends only
on (cid:107)w(cid:107), (cid:107)w∗(cid:107) and θw,w∗ , we notice that

k2−k
π − θ

(cid:17)

π

(cid:96)d1,d2(w) = ˜(cid:96)Rd1,Rd2 (Rw)

4

or equivalently

1 ∇2(cid:96)(w)d2 = (Rd1)T∇2 ˜(cid:96)(Rw)Rd2 = dT
dT

1 RT∇2 ˜(cid:96)(Rw)Rd2

for any w and directions d1 and d2. It follows that

∇2(cid:96)(w) = RT∇2 ˜(cid:96)(Rw)R

for all w. Since R is an orthogonal matrix, we have that ∇2(cid:96)(w) and ∇2 ˜(cid:96)(Rw) are similar
matrices and thus have the same eigenvalues. Therefore, we can w.l.o.g. rotate w∗ such that it
will be on the w1 axis.
By symmetry we have

∂(cid:96)

∂w1∂wi

(u) =

∂(cid:96)

∂w1∂wj

(u),

∂(cid:96)

∂wi∂w1

(u) =

∂(cid:96)

∂wj∂w1

(u)

and

∂(cid:96)
∂w2
i

∂(cid:96)
∂w2
j

∂(cid:96)

(u) =

(u),

(u) =

∂wi∂wj

∂ws∂wt

∂(cid:96)

(u)

for i (cid:54)= j, s (cid:54)= t such that i, j, s, t (cid:54)= 1. It follows that we only need to consider second partial
derivatives with respect to 3 axes w1,w2 and w3. Denote u = (−γ(k), , 0, ..., 0) and w∗ =
(1, 0, ..., 0) and β(k) = k2−k

and note that γ(k) = β(k)

β(k)+k . Then by equation Eq. 2 we have

π

∂(cid:96)
∂w2
2

(u) = lim
→0

= lim
→0
1
k2

=



∇(cid:96)(u)x − ∇(cid:96)(u)x

(cid:34)(cid:0)k + β(k)(cid:1) − k
π (cid:107)w∗(cid:107) (cid:107)u(cid:107) sin θu,w∗ − β(k)(cid:107)w∗(cid:107) (cid:107)u(cid:107)
(cid:1) = 0
(cid:0)k + β(k) − β(k)

1
k2



γ(k)

(cid:35)

(3)

Furthermore,

∂(cid:96)

∂w1∂w2

(u) = lim
→0

(cid:34)

∇(cid:96)(u)y − ∇(cid:96)(u)y



−(cid:0)k + β(k)(cid:1)γ(k) + k

1
k2

= lim
→0

π (cid:107)w∗(cid:107) γ(k)

(cid:107)u(cid:107) sin θu,w∗ + β(k)(cid:107)w∗(cid:107) γ(k)

(cid:107)u(cid:107) − k



(cid:35)
π (π − θu,w∗ )

where θu,w∗ = arccos(

√

−γ(k)
2+γ2(k)

).

By L’Hopital’s rule we have

∂(cid:96)

∂w1∂w2

(u) = lim
→0

γ(k) cos θu,w∗ ∂θu ,w∗

∂w2

− γ(k) sin θu,w∗

πk(cid:107)u(cid:107)3
∂θu,w∗

+

(cid:16) γ(k) cos θu,w∗

πk (cid:107)u(cid:107)

(cid:17)

+ 1

=

1
πk

lim
→0

∂w2

(cid:107)u(cid:107)

5

(4)

(5)

− β(k)γ(k)

(cid:107)u(cid:107)3 +

∂θu ,w∗

∂w2
πk

Since

it follows that

∂θu,w∗

∂w2

(u) = −

1
||√

2+γ2(k)

γ(k)

(2 + γ2(k)) 3

2

= −

γ(k)

(2 + γ2(k))||

(cid:12)(cid:12)(cid:12)(cid:12)

∂(cid:96)

∂w1∂w2

(cid:12)(cid:12)(cid:12)(cid:12) =

(u)

≤

1
πk

lim
→0
1

γ(k)πk

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) γ(k) cos θu,w∗
(cid:12)(cid:12)(cid:12)(cid:12) ∂θu,w∗
(cid:12)(cid:12)(cid:12)(cid:12) γ(k) cos θu,w∗
(cid:12)(cid:12)(cid:12)(cid:12) = 0

(cid:107)u(cid:107)

(cid:107)u(cid:107)

lim
→0

∂w2

+ 1

+ 1

(cid:12)(cid:12)(cid:12)(cid:12)

(6)

and thus

∂(cid:96)

∂w1∂w2

(u) = 0.

Taking derivatives of the gradient with respect to w1 is easier because the expressions in Eq. 2

that depend on θw,w∗ and w(cid:107)w(cid:107) are constant. Therefore,

and

∂(cid:96)

Finally let ˜u = (0,−γ(k), , 0, ..., 0) then it is easy to see that

∂(cid:96)
∂w2
1

(u) =

k + β(k)

k2

(u) = 0

∂w2∂w1

∂(cid:96)

∂w2∂w3

(u) = lim
→0

∇(cid:96)(˜u)w2 − ∇(cid:96)(u)w2



= 0

.

Therefore, overall we see that ∇2(cid:96)(u) is a diagonal matrix with zeros and k+β(k)

k2 > 0 on the

diagonal, which proves our claim.

C.2 Proof of Theorem 5.2
For the following lemmas let wt+1 = wt − λ∇(cid:96)(wt), θt be the angle between wt and w∗ (t ≥ 0) and
πk2 . Note that α(k) ≤ 1 for all k ≥ 1 The following lemma
deﬁne ˜λ = α(k)λ where α(k) = 1
shows that for λ < 1, the angle between wt and w∗ decreases in each iteration.

k + k2−k

Lemma C.1. If 0 < θt < π and λ < 1 then θt+1 < θt.

Proof. This follows from the fact that adding

(cid:32)

− λ
k2

k2 − k

π

− k (cid:107)w∗(cid:107)
π (cid:107)wt(cid:107) sin θt − k2 − k

π

(cid:107)w∗(cid:107)
(cid:107)wt(cid:107)

k +

(cid:33)

wt

to wt does not change θt for λ < 1, since k+ k2−k
decreases θt.

π
k2

≤ 1 for k ≥ 1. In addition, adding λ

πk

We will need the following two lemmas to establish a lower bound on (cid:107)wt(cid:107).

Lemma C.2. If π

2 < θt < π then (cid:107)wt+1(cid:107) ≥ sin θt

sin θt+1

min{(cid:107)wt(cid:107) ,

(cid:107)w∗(cid:107) sin θt

α(k)π

}.

6

(cid:16)

(cid:17)

w∗

π − θ

Proof. Let

(cid:32)

k +

ut = wt − λ
k2

k2 − k

π

− k (cid:107)w∗(cid:107)
π (cid:107)wt(cid:107) sin θt − k2 − k

π

(cid:107)w∗(cid:107)
(cid:107)wt(cid:107)

(cid:33)

wt

Notice that if (cid:107)wt(cid:107) ≤ (cid:107)w∗(cid:107) sin θt

α(k)π

then

(cid:107)ut(cid:107) = (1 − ˜λ)(cid:107)wt(cid:107) +

(1 − ˜λ)(cid:107)wt(cid:107) +

λ(cid:107)w∗(cid:107)

πk
λk (cid:107)w∗(cid:107) sin θt

sin θt +

λ(k2 − k)(cid:107)w∗(cid:107)

πk2

λ(k2 − k)(cid:107)w∗(cid:107) sin θt

+

≥

=

πk2
(1 − ˜λ)(cid:107)wt(cid:107) +
then (cid:107)ut(cid:107) ≥ (cid:107)w∗(cid:107) sin θt

˜λ(cid:107)w∗(cid:107) sin θt

α(k)π

πk2
≥ (cid:107)wt(cid:107)

Similarly, if (cid:107)wt(cid:107) ≥ (cid:107)w∗(cid:107) sin θt
tion we see that (cid:107)wt+1(cid:107) cos(θt+1 − π
2 ) if θt+1 ≤ π
(cid:107)ut(cid:107) cos(θt − π
(cid:107)w∗(cid:107) sin θt

α(k)π

sin θt

min{(cid:107)wt(cid:107) ,

α(k)π

sin θt+1

2 ) = (cid:107)ut(cid:107) cos(θt − π

α(k)π

2 ) if θt+1 > π

2 . This is equivalent to (cid:107)wt+1(cid:107) = sin θt
} as desired.

sin θt+1

. Furthermore, by a simple geometric observa-

2 and (cid:107)wt+1(cid:107) cos( π
2 − θt+1) =
(cid:107)ut(cid:107). It follows that (cid:107)wt+1(cid:107) ≥

Lemma C.3. If 0 < θt ≤ π
Proof. First assume that k ≥ 2. Let ut be as in Lemma C.2, then

2 then (cid:107)wt+1(cid:107) ≥ min{(cid:107)wt(cid:107) ,

2 and 0 < λ < 1

(cid:107)w∗(cid:107)
8 }

(cid:107)ut(cid:107) ≥ (1 − ˜λ)(cid:107)wt(cid:107) +

˜λ(k2 − k)(cid:107)w∗(cid:107)

It follows that if (cid:107)wt(cid:107) ≥ (k2−k)(cid:107)w∗(cid:107)
then (cid:107)ut(cid:107) ≥ (cid:107)wt(cid:107). Since wt+1 = ut + λ
min{(cid:107)w∗(cid:107)

(cid:16)
α(k)πk2 ≥ (cid:107)w∗(cid:107)

πk

2π

α(k)πk2
then (cid:107)ut(cid:107) ≥ (cid:107)w∗(cid:107)
π − θ

(cid:17)

w∗ and 0 < θt ≤ π

2π . Otherwise if (cid:107)wt(cid:107) ≤ (k2−k)(cid:107)w∗(cid:107)
2 we have (cid:107)wt+1(cid:107) ≥ (cid:107)ut(cid:107) ≥

α(k)πk2

2π ,(cid:107)wt(cid:107)}.
(cid:16)

(cid:17)

Now let k = 1. Note that in this case ˜λ = λ. First assume that θt < π
using the same notation as in Lemma C.2, (cid:107)ut(cid:107) ≥ (1 − λ)(cid:107)wt(cid:107) + λ(cid:107)w∗(cid:107) sin θt
2 we have (cid:107)wt+1(cid:107) ≥ (cid:107)ut(cid:107) ≥ (cid:107)w∗(cid:107)
wt+1 = ut + λ
π
the facts 0 < θt ≤ π
(cid:16)

w∗ and 0 < θt ≤ π
2 we get

2 and cos θt > 1

π − θt

(cid:17)

(cid:16)

π

8

(cid:107)wt+1(cid:107)2 = (cid:107)ut(cid:107)2 + 2(cid:107)ut(cid:107)

w∗(cid:13)(cid:13)(cid:13)(cid:13) cos θt +

(cid:13)(cid:13)(cid:13)(cid:13) λ

π

(cid:13)(cid:13)(cid:13)(cid:13) λ

π − θt

(cid:17)

w∗(cid:13)(cid:13)(cid:13)(cid:13)2 ≥

3 . If (cid:107)wt(cid:107) ≥ (cid:107)w∗(cid:107)
2 ≥ (cid:107)w∗(cid:107)
8
(cid:107)w∗(cid:107)

≥ (cid:107)wt(cid:107)
. If (cid:107)wt(cid:107) <

4

4

then,
. Since

then by

π − θt

π
(1 − λ)λ

(1 − λ)2 (cid:107)wt(cid:107)2 +
(cid:107)w∗(cid:107)2 ≥
(1 − λ)2 (cid:107)wt(cid:107)2 + 2(1 − λ)λ(cid:107)wt(cid:107)2 + 4λ2 (cid:107)wt(cid:107)2 =

(cid:107)wt(cid:107)(cid:107)w∗(cid:107) +

λ2
4

2

(1 + 3λ2)(cid:107)wt(cid:107)2 ≥ (cid:107)wt(cid:107)2

3 . As in the proof of Lemma C.2, if (cid:107)wt(cid:107) ≥ (cid:107)w∗(cid:107) sin θt
Finally, assume θt ≥ π
(cid:107)w∗(cid:107)
π . Otherwise, if (cid:107)wt(cid:107) <

(cid:107)wt+1(cid:107) ≥ (cid:107)ut(cid:107) ≥ √

then
then (cid:107)wt+1(cid:107) ≥ (cid:107)ut(cid:107) ≥ (cid:107)wt(cid:107). This

(cid:107)w∗(cid:107) sin θt

(cid:107)w∗(cid:107)

3
2

3
2

π

π

π

≥ √

concludes our proof.

We can now show that in each iteration (cid:107)wt(cid:107) is bounded away from 0 by a constant.

7

Proposition C.4. Assume GD is initialized at w0 such that θ0 (cid:54)= π and runs for T iterations with
learning rate 0 < λ < 1

2 . Then for all 0 ≤ t ≤ T ,

(cid:107)wt(cid:107) ≥ min{(cid:107)w0(cid:107) sin θ0,

(cid:107)w∗(cid:107) sin2 θ0

α(k)π

(cid:107)w∗(cid:107)

8

}

,

Proof. Let θ0 > θ1 > ... > θT (by Lemma C.1). Let i be the last index such that θi > π
2 (if such i
does not exist let i = −1). Since sin θj > sin θ0 for all 0 ≤ j ≤ i, by applying Lemma C.2 at most
j + 1 times we have

(cid:107)wj+1(cid:107) ≥ min{(cid:107)w0(cid:107) sin θ0,

(cid:107)w∗(cid:107) sin2 θ0

}

α(k)π

for all 0 ≤ j ≤ i.

Finally, by Lemma C.3 and the fact that θj ≤ π

2 for all i < j ≤ T , we get

for all i + 1 < j ≤ T , from which the claim follows.

(cid:107)wj(cid:107) ≥ min{(cid:107)wi+1(cid:107) ,

(cid:107)w∗(cid:107)

8

}

The following lemma shows that ∇(cid:96) is Lipschitz continuous at points that are bounded away from

0.
Lemma C.5. Assume (cid:107)w1(cid:107) ,(cid:107)w2(cid:107) ≥ M , w1, w2 and w∗ are on the same two dimensional half-plane
deﬁned by w∗, then

(cid:107)∇(cid:96)(w1) − ∇(cid:96)(w2)(cid:107) ≤ L(cid:107)w1 − w2(cid:107)

for L = 1 + 3(cid:107)w∗(cid:107)
M .

Proof. Recall that by equality Eq. 1,

(w, w∗) =

∂g
∂w

1
2π

(cid:107)w∗(cid:107) w

(cid:107)w(cid:107) sin θw,w∗ +

1
2π

(cid:16)

π − θw,w∗

(cid:17)

w∗

Let θ1 and θ2 be the angles between w1,w∗ and w2,w∗, respectively. By the inequality x0 sin x

≥ x

sin x0

for 0 ≤ x ≤ x0 < π and since

|θ1−θ2|

2 ≤ π

2 we have
|θ1 − θ2|

2

≤ π sin

|θ1−θ2|

2

2

Furthermore (cid:107)w1 − w2(cid:107) is minimized (for ﬁxed angles θ1 and θ2) when (cid:107)w1(cid:107) = (cid:107)w2(cid:107) = M and is
equal to 2M sin

. Thus, under our assumptions we have,

|θ1−θ2|

2

|θ1 − θ2|

≤ π sin

|θ1−θ2|

2

2

Thus we get

(cid:16)

(cid:13)(cid:13)(cid:13)(cid:13) 1

2π

2

(cid:17)

π − θ1

(cid:16)

w∗ − 1
2π

π − θ2

≤ π (cid:107)w1 − w2(cid:107)

(cid:17)

4M

w∗(cid:13)(cid:13)(cid:13)(cid:13) ≤ (cid:107)w∗(cid:107)

4M

(cid:107)w1 − w2(cid:107)

For the ﬁrst summand, we will ﬁrst ﬁnd the parameterization of a two dimensional vector of length
sin θ where θ is the angle between the vector and the positive x axis. Denote this vector by (a, b),
then the following holds

and

a2 + b2 = sin2 θ

b
a

= tan θ

8

The solution to these equations is (a, b) = ( sin 2θ

, sin2 θ). Hence (here we use the fact that w1,w2 are

2

on the same half-plane) (cid:13)(cid:13)(cid:13)(cid:13) 1

2π

(cid:13)(cid:13)(cid:13)(cid:13) =
(cid:17)2 ≤

(cid:107)w∗(cid:107)

1
2π

2π

(cid:107)w∗(cid:107) w1(cid:107)w1(cid:107) sin θ1 − 1
(cid:107)w∗(cid:107) w2(cid:107)w2(cid:107) sin θ2
(cid:114)(cid:16) sin 2θ1
(cid:17)2
(cid:16)
− sin 2θ2
sin2 θ1 − sin2 θ2
(cid:107)w∗(cid:107)(cid:112)(θ1 − θ2)2 + 4(θ1 − θ2)2 ≤
(cid:107)w∗(cid:107) π (cid:107)w1 − w2(cid:107)

(cid:107)w1 − w2(cid:107)

1
2π

√

=

+

2

2

5(cid:107)w∗(cid:107)
4M

4M

√

5
π

where the ﬁrst inequality follows from the fact that | sin x−sin y| ≤ |x−y| and the second inequality

from previous results. In conclusion, we have

(w1, w∗) − ∂g
∂w

(w2, w∗)

(cid:13)(cid:13)(cid:13)(cid:13) ≤ (

√

5 + 1)(cid:107)w∗(cid:107)

4M

(cid:107)w1 − w2(cid:107)

Similarly, in order to show that the function f (w) = w(cid:107)w(cid:107) is Lipschitz continuous, we parameterize
the unit vector by (cos θ, sin θ) where θ is the angle between the vector and the positive x axis. We
now obtain

∂w

(cid:13)(cid:13)(cid:13)(cid:13) ∂g
(cid:13)(cid:13)(cid:13)(cid:13) w1(cid:107)w1(cid:107) − w2(cid:107)w2(cid:107)

(cid:13)(cid:13)(cid:13)(cid:13) =(cid:112)(cos θ1 − cos θ2)2 + (sin θ1 − sin θ2)2 ≤
(cid:112)2(θ1 − θ2)2 ≤ π (cid:107)w1 − w2(cid:107)
(cid:13)(cid:13)(cid:13)(cid:13) ∂g

(cid:1)(cid:107)w1 − w2(cid:107) +

(w1, w∗) − ∂g
∂w

k2 − k
πk2

2M

√

2
k

∂w
(k2 − k)(cid:107)w∗(cid:107)

+

k2 − k
πk2 +
√
5 + 1)(cid:107)w∗(cid:107)
(

√

2M

2M k2

≤ 1 +

3(cid:107)w∗(cid:107)

M

√
(

+

5 + 1)(cid:107)w∗(cid:107)

2M k

(cid:107)w∗(cid:107)√

2M

+

1 +

(w2, w∗)

(cid:13)(cid:13)(cid:13)(cid:13) +
(cid:17)(cid:107)w1 − w2(cid:107) ≤

Now we can conclude that

(cid:107)∇(cid:96)(w1) − ∇(cid:96)(w2)(cid:107) ≤(cid:0) 1
(cid:17)(cid:13)(cid:13)(cid:13)(cid:13) w1(cid:107)w1(cid:107) − w2(cid:107)w2(cid:107)

k

πk2

(cid:16) (k2 − k)(cid:107)w∗(cid:107)

+

(cid:13)(cid:13)(cid:13)(cid:13) ≤(cid:16) 1

k

Given that (cid:96) is Lipschitz continuous we can now follow standard optimization analysis (Nesterov

(2004)) to show that limt→∞ (cid:107)∇(cid:96)(wt)(cid:107) = 0.
Proposition C.6. Assume GD is initialized at w0 such that θ0 (cid:54)= π and runs with a constant learning
rate 0 < λ < min{ 2

2} where L = ˜O(1). Then for all T

L , 1

T(cid:88)

t=0

(cid:107)∇(cid:96)(wt)(cid:107)2 ≤

1
λ(1 − λ

2 L)

(cid:96)(w0)

Proof. We will need the following lemma
Lemma C.7. Let f : Rn → R be a continuously diﬀerentiable function on a set D ⊆ Rn and x, y ∈ D
such that for all 0 ≤ τ ≤ 1, x + τ (y − x) ∈ D and (cid:107)∇f (x + τ (y − x)) − ∇f (x)(cid:107) ≤ L(cid:107)x − y(cid:107). Then
we have

|f (y) − f (x) − (cid:104)∇f (x), y − x(cid:105)| ≤ L
2

(cid:107)x − y(cid:107)2

9

Proof. The proof exactly follows the proof of Lemma 1.2.3 in Nesterov (2004) and note that the proof
only requires Lipschitz continuity of the gradient on the set S = {x + τ (y − x) | 0 ≤ τ ≤ 1} and that
S ⊆ D.

By Proposition C.4, for all t, (cid:107)wt(cid:107) ≥ M(cid:48) where

M(cid:48) = min{(cid:107)w0(cid:107) sin θ0,

(cid:107)w∗(cid:107) sin2 θ0

α(k)π

(cid:107)w∗(cid:107)

8

}

,

. Furthermore, by a simple geometric observation we have

0≤τ≤1,(cid:107)w1(cid:107),(cid:107)w2(cid:107)≥M(cid:48),arccos

min

(cid:107)τ w1 + (1 − τ )w2(cid:107) = M(cid:48) cos

θ
2

=θ

(cid:16) w1·w2

(cid:107)w1(cid:107)(cid:107)w2(cid:107)

(cid:17)

.

It follows by Lemma C.5 that for any t and x1, x2 ∈ St (cid:44) {wt + τ (wt+1 − wt) | 0 ≤ τ ≤ 1},

(cid:107)∇(cid:96)(x1) − ∇(cid:96)(x2)(cid:107) ≤ L(cid:107)x1 − x2(cid:107)

where L = 1 + 3(cid:107)w∗(cid:107)

M and M = M(cid:48) cos θ0

2 (Note that cos θt−θt+1

2

Hence by Lemma C.7, for any t we have

≥ cos θ0

2 for all t by Lemma C.1).

(cid:96)(wt+1) ≤ (cid:96)(wt) + (cid:104)∇(cid:96)(wt), wt+1 − wt(cid:105) +

(cid:107)wt+1 − wt(cid:107)2 =

(cid:96)(wt) − λ(1 − λ
2

which implies that

T(cid:88)

t=0

(cid:107)∇(cid:96)(wt)(cid:107)2 ≤

1
λ(1 − λ

2 L)

We are now ready to prove the theorem.

(cid:16)

L
2
L)(cid:107)∇(cid:96)(wt)(cid:107)2

(cid:17) ≤

(cid:96)(w0) − (cid:96)(wT )

1
λ(1 − λ

2 L)

(cid:96)(w0)

Proof of Theorem 5.2. First, we observe that for a randomly initialized point w0, 0 ≤ θ0 ≤ π(1 − δ)
with probability 1−δ. Hence by Proposition C.6 we have for L = 1+ 3(cid:107)w∗(cid:107)
M where M = min{sin(π(1−
δ)), sin2(π(1−δ))
(cid:0) k
L (we assume w.l.o.g. that L > 2),

8} cos( π(1−δ)
, 1
T(cid:88)

) and α(k) = k + k2−k

π , and for λ = 1

(cid:1)

α(k)π

2

(cid:107)∇(cid:96)(wt)(cid:107)2 ≤

1
λ(1 − λ

2 L)

(cid:96)(w0) = 2L(cid:96)(w0) ≤ 4L
k2

+

2

k2 − k
2π

t=0

Therefore,

It follows that gradient descent reaches a point wt such that (cid:107)∇(cid:96)(wt)(cid:107) <  after T iterations where

(cid:1)

2π

(cid:0) k
2 + k2−k
T
(cid:1)(cid:17)2

{(cid:107)∇(cid:96)(wt)(cid:107)2} ≤ 4L

k2

min
0≤t≤T

(cid:16) 4L

k2

(cid:0) k
2 + k2−k
√

2

2π

T >

10

We will now show that if (cid:107)∇(cid:96)(wt)(cid:107) <  then wt is O(

)-close to the global minimum w∗. First
2 ≤ θt ≤ π(1−δ) then a vector of the form v = αw∗ +βw where α ≥ 0 is of minimal norm

note that if π

equal to α sin(π − θt)(cid:107)w∗(cid:107) when it is perpendicular to w. Since the gradient is a vector of this form,
we have (cid:107)∇(cid:96)(wt)(cid:107) > πδ(cid:107)w∗(cid:107) sin πδ

k ≥ . Hence, from now on we assume that 0 ≤ θt < π
2 .

≥ δ sin πδ

πk

Similarly to the previous argument, we have

Hence, θt < arcsin(2k) = O(). It follows by the triangle inequality that

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) ≥

w∗

(cid:33)
wt − k(π − θt)

(cid:107)w∗(cid:107)
(cid:107)wt(cid:107)
sin θt − kθt (cid:107)w∗(cid:107)

≥

π

π

 > (cid:107)∇(cid:96)(wt)(cid:107) >

(cid:107)w∗(cid:107) (π − π
πk

2 ) sin θt

≥ sin θt
2k

π

π

k +

k2 − k

− k2 − k
π
k2 − k

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:32)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:32)
(cid:13)(cid:13)(cid:13)(cid:13)kw∗ − k (cid:107)w∗(cid:107)
(cid:1)|(cid:107)wt(cid:107) − (cid:107)w∗(cid:107)| − k (cid:107)w∗(cid:107) θt − k (cid:107)w∗(cid:107)

π (cid:107)wt(cid:107) sin θt − k2 − k
− k (cid:107)w∗(cid:107)
(cid:13)(cid:13)(cid:13)(cid:13)(cid:13) − k (cid:107)w∗(cid:107)
(cid:33)
wt − kw∗
(cid:33)
wt − k (cid:107)w∗(cid:107)
(cid:13)(cid:13)(cid:13)(cid:13) − k (cid:107)w∗(cid:107)
(cid:107)wt(cid:107) wt
sin θt − kθt (cid:107)w∗(cid:107)

(cid:107)w∗(cid:107)
(cid:107)wt(cid:107)
− k2 − k

(cid:107)w∗(cid:107)
(cid:107)wt(cid:107)

(cid:107)wt(cid:107) wt

k +

π

π

π

π

π

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)−

≥

sin θt − kθt (cid:107)w∗(cid:107)

π

π

k2 > k2 (cid:107)∇(cid:96)(wt)(cid:107) =

(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)
(cid:32)

k2 − k

π

k +

(cid:0)k +

k2 − k

π

Therefore we get |(cid:107)wt(cid:107) − (cid:107)w∗(cid:107)| < O(). By the bounds on θt and |(cid:107)wt(cid:107) − (cid:107)w∗(cid:107)| and the

where the last inequality follows since the arc of a circle is larger than its corresponding segment.
inequality cos x ≥ 1 − x for x ≥ 0, we can give an upper bound on (cid:107)wt − w∗(cid:107):
(cid:107)wt − w∗(cid:107)2 = (cid:107)wt(cid:107)2 − 2(cid:107)wt(cid:107)(cid:107)w∗(cid:107) cos θt + (cid:107)w∗(cid:107)2 =

(cid:107)wt(cid:107) ((cid:107)wt(cid:107) − (cid:107)w∗(cid:107) cos θt) + (cid:107)w∗(cid:107) ((cid:107)w∗(cid:107) − (cid:107)wt(cid:107) cos θt) ≤

((cid:107)w∗(cid:107) + O())(O() + θt (cid:107)w∗(cid:107)) + (cid:107)w∗(cid:107) (O(2) + θt (cid:107)w∗(cid:107)) = O()

Finally, to prove the claim it suﬃces to show that (cid:96)(w) ≤ d(cid:107)w − w∗(cid:107)2. Denote the input vector

x = (x1, x2, ..., xk) where xi ∈ Rm for all 1 ≤ i ≤ k. Then we get

(cid:96)(w) = Ex

≤ Ex

≤ Ex

k

−

(cid:104)(cid:80)k
(cid:104)(cid:80)k
(cid:104)(cid:80)k
(cid:104)(cid:80)k

(cid:80)k
i=1 σ(w∗T xi)
i=1 σ(wT xi)
(cid:105)2
k
i=1 |σ(wT xi) − σ(w∗T xi)|
(cid:105)2
i=1 |wT xi − w∗T xi|
(cid:105)2
i=1 (cid:107)w − w∗(cid:107)(cid:107)xi(cid:107)

k

k

≤ Ex
≤ (cid:107)w − w∗(cid:107)2Ex(cid:107)x(cid:107)2
= d(cid:107)w − w∗(cid:107)2

k

(cid:105)2

(7)

where the second inequality follows from Lipschitz continuity of σ, the third inequality from the
Cauchy-Schwarz inequality and the last equality since (cid:107)x(cid:107)2 follows a chi-squared distribution with d
degrees of freedom.
(cid:3)

11

D Missing Proofs for Section 7.1

D.1 Proof of Proposition 7.1
Deﬁne wp = (w2, w1), w∗
Lemma D.1. Let l be deﬁned as in Eq. 16. Then
k2 − 3k + 2

= (0,−w∗) and w∗

p1

p2

(cid:34)(cid:0)k +

∇l(w) =

= (w∗, 0). We ﬁrst prove the following lemma.

π

π

+

1
k2
(k − 1)(π − θwr,wl )
− k (cid:107)w∗(cid:107) sin θw,w∗
− (k − 1) sin θwl,w∗
π (cid:107)w(cid:107)
− (k − 1) sin θwr,w∗
π (cid:107)w(cid:107)

π (cid:107)w(cid:107)

r

l

(cid:107)w∗(cid:107)

(cid:1)w +
2(k − 1) sin θwr,wl
wp − (k2 − 3k + 2)(cid:107)w∗(cid:107)

π

w

w

π (cid:107)w(cid:107)
w∗

w − k(π − θw,w∗ )
(cid:107)w∗(cid:107)

π

w − (k − 1)(π − θwl,w∗
w − (k − 1)(π − θwr,w∗

π

)

r

l

)

w∗

p2

(cid:35)

w∗

p1

π

Proof. The gradient does not follow immediately from Lemma 3.2 because the loss has expressions
with of the function g but with diﬀerent dependencies on the parameters in A. We will only calculate
∂g(wr,wl)

, the other expressions are calculated in the same manner.

∂w
Recall that

g(wr, wl) =

1
2π

(cid:107)w(cid:107)2(sin θwr,wl + (π − θwr,wl ) cos θwr,wl )

It follows that

∂g(wr, wl)

∂w

=

1
π

(sin θwr,wl + (π − θwr,wl ) cos θwr,wl )w +

(cid:107)w(cid:107)2(π − θwr,wl )

1
2π

∂ cos θwr,wl

∂w

(8)

Let w = (w1, w2) then cos θwr,wl = w1w2
1+w2
2
1 + w2
(w2

∂ cos θwr,wl

w2(w2

=

w2

∂w1

. Then,
2) − 2w2

1w2

1 + w2

2)2

and

∂ cos θwr,wl

∂w2
or equivalently ∂ cos θwr ,wl

∂w

2) − 2w2

2w1

=

w1(w2

1 + w2
(w2

1 + w2
2)2
= wp(cid:107)w(cid:107)2 − 2w cos θwr ,wl
∂g(wr, wl)

(cid:107)w(cid:107)2

. It follows that

sin θwr,wlw

=

π

(π − θwl,wr )

2π

+

wp

∂w

=

=

w2

(cid:107)w(cid:107)2 − 2w1 cos θwr,wl

(cid:107)w(cid:107)2

w1

(cid:107)w(cid:107)2 − 2w2 cos θwr,wl

(cid:107)w(cid:107)2

We will prove that wt+1 (cid:54)= 0 and that it is in the interior of the fourth quadrant. Denote w = wt

and ∇l(w) = 1

k2

(k2 − 3k + 2)(cid:107)w∗(cid:107)

π (cid:107)w(cid:107)

(cid:0)B1(w) + B2(w) + B3(w)(cid:1) where
(cid:1)w +

B1(w) =(cid:0)k +
w − k (cid:107)w∗(cid:107) sin θw,w∗

k2 − 3k + 2

π

π (cid:107)w(cid:107)

w − (k − 1) sin θwl,w∗
π (cid:107)w(cid:107)

r

12

2(k − 1) sin θwr,wl

w−

π
(cid:107)w∗(cid:107)

w − (k − 1) sin θwr,w∗
π (cid:107)w(cid:107)

l

(cid:107)w∗(cid:107)

w

B2(w) =

(k − 1)(π − θwr,wl )

π

wp

and

B3(w) = − k(π − θw,w∗ )

− (k − 1)(π − θwr,w∗
Let w = (w,−mw) for w, m ≥ 0. Straightforward calculation shows that cos θwl,w∗

w∗ − (k − 1)(π − θwl,w∗

w∗

p2

π

π

π

)

)

r

l

w∗

p1

1√

=

m√

4 ≤ θwl,w∗
and cos θwr,w∗
also have 3π
the worst case (the least possible increase of (cid:107)w(cid:107))

2(1+m2)
2 . Since w is in the fourth quadrant we
4 ≤ θw,w∗ ≤ π. Therefore, adding −λB3(w) can only increase (cid:107)w(cid:107). This follows since in

. Hence π

, θwr,w∗

≤ π

2(m2+1)

=

r

l

l

r

−B3(w) =

w∗ +

k
4

k − 1

2

w∗

p2

+

k − 1

2

w∗

p1

= (

4

k − 2

w∗,− k − 2

w∗)

4

which is in the fourth quadrant for k ≥ 2. In addition, since −wp is in the fourth quadrant then
adding −λB2(w) increases (cid:107)w(cid:107).
then −B1(w) points in the direction of w since in this case −B1(w) = αw where

If (cid:107)w(cid:107) <

(cid:107)w∗(cid:107)
16

α ≥(cid:16) k2 − 3k + 2

π

(k − 1)

− k − 1

− k2 − 3k + 2

+

π

8π

16π

− k
16

(cid:17)(cid:107)w∗(cid:107) > 0

for k ≥ 2. If −B1(w) points in the direction of −w then by the assumption that λ ∈ (0, 1
(cid:107)λB1(w)(cid:107) < (cid:107)w(cid:107). Thus we can conclude that wt+1 (cid:54)= 0.
that w1 > −w2. In this case −B3(w) least increases (or even most decreases) θt when

Now, let w = (w1, w2), θt be the angle between w = wt and the positive x axis and ﬁrst assume

3 ) we have

−B3(w) =

w∗ +

k
4

3(k − 1)

4

w∗

p2

+

k − 1

2

w∗

p1

=

(cid:16) 2k − 3

4

w∗,

2 − k
4

w∗(cid:17)

which is a vector in the fourth quadrant for k ≥ 2. Otherwise, −B3(w) is a vector in the fourth
4 ≤ θw,w∗ ≤ π. Since
quadrant as well. Note that we used the facts π
−λB1(w) does not change θt and −λB2(w) increases θt but never to an angle greater than or equal
2 , it follows that 0 < θt+1 < π
to π
2 .
If w1 ≤ −w2 then by deﬁning all angles with respect to the negative y axis, we get the same
argument as before. This shows that wt+1 is in the interior of the fourth quadrant, which concludes
our proof.

4 ≤ θwl,w∗

2 and 3π

, θwr,w∗

≤ π

r

l

D.2 Proof of Proposition 7.2

We will need the following auxiliary lemmas.
Lemma D.2. Let w be in the fourth quadrant, then g(wl, wr) ≥ 1
Proof. First note that the function s(θ) = sin θ + (π − θ) cos θ is decreasing as a function of θ ∈ [0, π].
Let w = (w,−mw) for w, m ≥ 0. Straightforward calculation shows that cos θwl,wr = − m
m2+1 . As a
function of m ∈ [0,∞), cos θwl,wr is minimized for m = 1 with value − 1
2 , i.e., when θ(wl, wr) = 2π
and this is the largest angle possible. Thus g(wl, wr) ≥ 1
Lemma D.3. Let

(cid:1)(cid:107)w(cid:107)2.
(cid:0)√
2 − π
(cid:0)√
3 )(cid:1)(cid:107)w(cid:107)2 = 1
2 − π
+ θ)(cid:1)+

f (θ) = 2k(cid:0) sin(

(cid:1)(cid:107)w(cid:107)2.

2π s( 2π

+ θ) + (

2π

2π

3

3

3

6

6

(cid:0)2k − 2(cid:1)(cid:0)(cid:115)

1 − cos θ2
2

+ (π − arccos

cos θ√
2

)

cos θ√
2

, then in the interval θ ∈ [0, π

4 ], f (θ) is maximized at θ = π

3π
4

π
4

− θ) cos(

(cid:1) +(cid:0)2k − 2(cid:1)(cid:0)(cid:115)

3π
4
1 − sin θ2
2
4 for all k ≥ 2.

(cid:1)

+ (π − arccos

sin θ√
2

)

sin θ√
2

13

k−1 f1(θ) + f2(θ) + f3(θ) where f1(θ), f2(θ), f3(θ)
1 − x2 + (π − arccos(x))x we have h(cid:48)(x) = π − arccos(x), it follows that
4 + θ). It

3(θ) = (π − arccos sin θ√

4 − θ) sin( 3π

1(θ) = −( π

and f(cid:48)

Since for h(x) =

2(k−1) = k

Proof. We will maximize the function f (θ)
√
correspond to the three summands in the expression of f (θ).
2(θ) = −(π − arccos cos θ√
f(cid:48)
) sin θ√
2
therefore suﬃces to show that
sin θ√
2

d1(θ) := (π − arccos

− (π − arccos

cos θ√
2

cos θ√
2

, f(cid:48)

− k

k − 1

(

π
4

− θ) sin(

3π
4

+ θ) ≥ 0

sin θ√
2

) cos θ√
2

)

)

2

2

2 − x for x ∈ [0, 1] and arccos(x) ≥ π

2 − x − 1

10 for

for θ ∈ [0, π
4 ].
x ∈ [ 1
] we get d1(θ) ≥ d2(θ) where

By applying the inequalities arccos(x) ≤ π
2 , 1√

2

d2(θ) =(cid:0) π

+

2

sin θ√
2

−(cid:0) π
(cid:1) cos θ√
cos θ −(cid:0) π

2

2

+

+

(cid:1) sin θ√
(cid:1) sin θ − k

1
10

2

+

cos θ√
2

√
1
10

2

√
2
We notice that d2(0) ≥ 0 and d2( 3

√
π

2

2

2

− k

k − 1

(

π
4

− θ) sin(

3π
4

+ θ) =

k − 1

(

π
4

− θ) sin(

3π
4

+ θ)

sin θ −(cid:0) π

4 ) ≥ 0 for all k ≥ 2. In addition,
√
1
10

(cid:1) cos θ +

3π
4

sin(

2

√
2(θ) = − π
d(cid:48)
2
2(0) > 0 for all k ≥ 2. It follows that in order to show that d2(θ) ≥ 0 for θ ∈ [0, 3

+ θ) − k

k
k − 1

k − 1

√
2

− θ) cos(

π
4

+

2

2

(

3π
4

+ θ)

4 ] and k ≥ 2, it

and d(cid:48)
suﬃces to show that d(cid:48)(cid:48)

2 (θ) ≤ 0 for θ ∈ [0, 3

4 ] and k ≥ 2. Indeed,

cos θ +(cid:0) π

√

√
2 (θ) = − π
d(cid:48)(cid:48)
2

+

√
1
10

(cid:1) sin θ +
(cid:1) max{sin θ, sin(

2k
k − 1

3π
4

cos(

+ θ) +

k
k − 1

(

π
4

− θ) sin(

+ θ) ≤

3π
4

2

2

2

2

+

√
1
10

k
k − 1

(cid:0)
4 ] and k ≥ 2. Note that the ﬁrst inequality follows since cos θ ≥ sin θ and the second
4 ]. This shows that d1(θ) ≥ 0 for θ ∈ [0, 3
4 ].
1(θ) ≤ 0 for
4 ) ≥ 0, it suﬃces to prove that d(cid:48)

4 +θ)}, both for θ ∈ [0, 3
4 ) ≥ 0 and d1( π

4 ]. Since d1( 3

+ θ) ≤ 0

+ θ)} +

2k
k − 1

3π
4

3π
4

cos(

π
4

2

for all θ ∈ [0, 3
Now assume that θ ∈ [ 3
4 , π

4 +θ) ≥ max{sin θ, sin( 3π
4 , π
4 ]

4 , π
4 ]. Indeed, for all θ ∈ [ 3

since cos( 3π
θ ∈ [ 3

1(θ) = −(π − arccos
d(cid:48)

cos θ√
2

)

cos θ√
2

− (π − arccos

sin θ√
2

)

sin θ√
2

+

(cid:113)

cos2 θ
1 − sin2 θ

2

2

(cid:113)

2

+

sin2 θ
1 − cos2 θ

2

+

k
k − 1

sin(

3π
4

+ θ) − k

k − 1

(

π
4

− θ) cos(

3π
4

+ θ) ≤

−(π − arccos

cos( π
4 )√
2

)

cos( π
4 )√
2

− (π − arccos

sin( 3
4 )√
2

)

sin( 3
4 )√
2

+

+ 2 sin(

3π
4

+

3
4

) − 2(

π
4

− 3
4

) cos(

3π
4

+

3
4

) < 0

(cid:113)

sin2( π
4 )
1 − cos2( 3
4 )
We conclude that d1(θ) ≥ 0 for all θ ∈ [0, π

cos2( 3
4 )
1 − sin2( π
4 )

+

2

2

2

2

(cid:113)

4 ] as desired.

14

Proof of Proposition 7.2.

positive x axis. Then cos θ = w1(cid:107)w(cid:107) and tan θ = − w2

w1

. Therefore we get

First assume that w1 ≥ −w2. Let θ be the angle between w and the

cos θwl,w∗

r

=

w1

(cid:107)w(cid:107)√

2

=

cos θ√
2

cos θwr,w∗

l

=

−w2
(cid:107)w(cid:107)√

2

=

cos θ tan θ√

2

=

sin θ√
2

and

We can rewrite (cid:96)(w) as

(cid:34)

k2 − 3k + 2

1
k2
2π
(cid:107)w(cid:107)(cid:107)w∗(cid:107)

(cid:16)

2k(cid:0) sin(

2π

(cid:0)2k − 2(cid:1)(cid:0)(cid:115)

1 − cos θ2
2

+ (π − arccos

cos θ√
2

)

cos θ√
2

(cid:96)(w) =

((cid:107)w(cid:107) − (cid:107)w∗(cid:107))2 +

(cid:107)w(cid:107)2 + 2(k − 1)g(wr, wl)−

k
2

+ θ)(cid:1)(cid:17)

+

3π
4

− θ) cos(

π
4

+ θ) + (

(cid:1) +(cid:0)2k − 2(cid:1)(cid:0)(cid:115)

3π
4

1 − sin θ2
(cid:35)
2

+ (π − arccos

sin θ√
2

)

sin θ√
2

(cid:1)(cid:17)

+

(cid:107)w∗(cid:107)2 + 2(k − 1)g(w∗

r, w∗
l )

k
2

(cid:34)

Hence by Lemma D.2 and Lemma D.3 we can lower bound (cid:96)(w) as follows

(cid:96)(w) ≥ 1
k2

k2 − 3k + 2

2π

((cid:107)w(cid:107) − (cid:107)w∗(cid:107))2 +

(cid:107)w(cid:107)2 +

k
2

(k − 1)(cid:107)w(cid:107)(cid:107)w∗(cid:107)

π

(cid:0)√

3 +

2π
3

(cid:1) +

(cid:107)w∗(cid:107)2 +

k
2

k − 1
π

3
2

− π
6

By setting (cid:107)w(cid:107) = α(cid:107)w∗(cid:107) we get

(cid:34)

(cid:96)(w)

(cid:107)w∗(cid:107)2 ≥ 1

k2

k2 − 3k + 2

(α − 1)2 +

k
2

α2 +

k − 1
π

(cid:0)√

k − 1
π

(cid:0)√

(cid:1)(cid:107)w(cid:107)2−
(cid:35)

3
2

− π
6

(cid:1)(cid:107)w∗(cid:107)2
(cid:1)α2−

− π
6

3
2

(cid:0)√
(cid:1)(cid:35)

(cid:0)√

3
2

− π
6

Solving for α that minimizes the latter expression we obtain

2π

(cid:0)√

(k − 1)

π

3 +

2π
3

k
2

(cid:1)α +
(cid:0)√

+ 2(k−1)

π

k − 1
π

+

(cid:1)
3 + 2π
√
3
2 − π
3

(

6

k2−3k+2

π

+ (k−1)

π

α∗ =

k + k2−3k+2
Plugging α∗ back to the inequality we get

π

(cid:16) h(k) + 1

h(k)

h(k) + 1

(cid:1) =
(cid:17)(cid:107)w∗(cid:107)2 =

(cid:96)(w) ≥ 1
k2

(α∗)2 − h(k)α∗ +

h(k) + 1

2

2

2h(k) + 1

k2(2h(k) + 2)

(cid:107)w∗(cid:107)2

and for ˜w = −α∗w∗ it holds that (cid:96)( ˜w) = 2h(k)+1

k2(2h(k)+2)(cid:107)w∗(cid:107)2.

15

Finally, assume w1 ≤ −w2. In this case, let θ be the angle between w and the negative y axis.

Then cos θ = −w2(cid:107)w(cid:107) and tan θ = − w1

w2

. Therefore

cos θwl,w∗

r

=

w1

(cid:107)w(cid:107)√

2

=

cos θ tan θ√

2

=

sin θ√
2

and

cos θwr,w∗

l

=

−w2
(cid:107)w(cid:107)√

2

=

cos θ√
2

Notice that from now on we get the same analysis as in the case where w1 ≥ −w2, where we switch
(cid:3)

between expressions with wl, w∗

r and expressions with wr, w∗

l . This concludes our proof.

E Uniqueness of Global Minimum in the Population Risk

Recall that (cid:96)(w) = EG(cid:2)(f (x; W ) − f (x; W ∗))2(cid:3) where f (x; W ) = 1

(cid:80)
Without loss of generality we assume that the ﬁlter is of size 2 and the stride is 1. The proof of the
general case follows the same lines. Assume that (cid:96)(w) = 0 and denote w = (w1, w2), w∗ = (w∗
1, w∗
2).
i σ (wi · x) and for all 1 ≤ i ≤ k
wi = (0i−1, w, 0d−i−1). By equating (cid:96)(w) to 0 we get that (f (x; W ) − f (x; W ∗))2 = 0 almost surely.
Since (f (x; W ) − f (x; W ∗))2 is a continuous function it follows that f (x; W ) − f (x; W ∗) = 0 for all
x. In particular this is true for x1 = (x, 0, 0, ..., 0), x ∈ R. Thus σ (xw1) = σ (xw∗
1) for all x ∈ R
1. The equality holds also for x2 = (0, x, 0, ..., 0), x ∈ R which implies that
which implies that w1 = w∗
1) for all x ∈ R. By the previous result, we get σ (xw2) = σ (xw∗
2) + σ (xw∗
σ (xw2) + σ (xw1) = σ (xw∗
2)
for all x ∈ R and thus w2 = w∗
2. We proved that w = w∗ and therefore w∗ is the unique global
minimum.

k

F Experimental Setup for Section 7.2

In our experiments we estimated the probability of convergence to the global minimum of a randomly
initialized gradient descent for many diﬀerent ground truths w∗ of a convolutional neural network
with overlapping ﬁlters. For each value of number of hidden neurons, ﬁlter size, stride length and
ground truth distribution we randomly selected 30 diﬀerent ground truths w∗ with respect to the
given distribution. We tested with all combinations of values given in Table 1.

Furthermore, for each combination of values of number of hidden neurons, ﬁlter size and stride
length we tested with deterministic ground truths: ground truth with all entries equal to 1, all entries
equal to -1 and with entries that form an increasing sequence from -1 to 1, -2 to 0 and 0 to 2 or
decreasing sequence from 1 to -1, 0 to -2 and 2 to 0.

For each ground truth, we ran gradient descent 20 times and for each run we recorded whether it
reached a point very close to the unique global minimum or it repeatedly (5000 consecutive iterations)
incurred very low gradient values and stayed away from the global minimum. We then calculated the
empirical probability ˆp = #times reached global minimum
. To compute the one-sided conﬁdence interval
we used the Wilson method (Brown et al. (2001)) which gives a lower bound

20

ˆp + z2

α

2n + zα

n + z2

α
4k2

(cid:113) ˆp(1− ˆp)

1 + z2
α
n

(9)

where zα is the Z-score with α = 0.05 and in our experiments n = 20. Note that we initialized gradient
descent inside a large hypercube such that outside the hypercube the gradient does not vanish (this
can be easily proved after writing out the gradient for each setting).
For all ground truths we got ˆp ≥ 0.15, i.e., for each ground truth we reached the global minimum
17 in all settings.

at least 3 times. Hence the conﬁdence interval lower bound Eq. 9 is greater than 1

16

Table 1: Parameters values for experiments in Section 7.2

Number of hidden neurons

Filter size

stride length

Ground truth distribution

50,100
2,8,16

4 , 1}, min{ f

1,min{ f
(For instance, for f = 16 we used strides 1,4,8

2 , 1} where f is the ﬁlter size

and for f = 2 we used stride 1)

uniform random variables over the interval [a, b]

The entries of the ground truth are i.i.d.
where (a, b) ∈ {(−1, 1), (−2, 0), (0, 2)}

This suggests that with a few dozen repeated runs of a randomly initialized gradient descent, with
high probability it will converge to the global minimum.

References

Brown, Lawrence D, Cai, T Tony, and DasGupta, Anirban. Interval estimation for a binomial pro-

portion. Statistical science, pp. 101–117, 2001.

Nesterov, Yurii. Introductory lectures on convex optimization. pp. 22–29, 2004.

17

