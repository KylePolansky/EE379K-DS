Selective Inference for Sparse High-Order Interaction Models

A. Proofs
A.1. Proof of Lamma 1

Proof. In the following, we abbreviate j in Lemma 1 for
the simplicity of the notation unless there is no confusion,
and prove the lemma in slightly general case of V[y] = Σ.
To prove the lemma, we ﬁrst state the polyhedral lemma in
Lee et al. (2016) as follows:

Lemma 7 (Polyhedral Lemma; (Lee et al., 2016)). Sup-
pose y ∼ N(µ, Σ). Let c = Ση(η(cid:62)Ση)−1 for any
η ∈ Rn, and let z = (In − cη(cid:62))y. Then we have
Pol(S) = {y ∈ Rn | Ay ≤ b}

(cid:12)(cid:12)(cid:12)(cid:12) L(S, z) ≤ η(cid:62)y ≤ U (S, z),

N (S, z) ≥ 0

y ∈ Rn

(cid:26)

=

where

(cid:27)

,

(13a)

(13b)

L(S, z) = max

j:(Ac)j <0

U (S, z) = min

j:(Ac)j >0

bj − (Az)j

(Ac)j

bj − (Az)j

,

(Ac)j

and N (S, z) = maxj:(Ac)j =0 bj − (Az)j.
(L(S, z), U (S, z), N (S, z)) is independent of η(cid:62)y.

In addition,

The polyhedral lemma allows us to construct a pivotal
quantity as a truncated normal distribution, that is, for any
z, we have

[F [L(S,z),U (S,z)]

0,η(cid:62)Ση

(η(cid:62)y)|y ∈ Pol(S)] ∼ Unif(0, 1),

(14)

where Unif(0, 1) denotes the standard (continuous) uni-
form distribution. In fact, by letting z0 be an arbitrary real-
ization of z, one can see that

d

[η(cid:62)y | y ∈ Pol(S), z = z0]
= [η(cid:62)y | L(S, z0) ≤ η(cid:62)y ≤ U (S, z0)]
∼ TN(η(cid:62)µ, η(cid:62)Ση, L(S, z0), U (S, z0)),

where d
= denotes the equality of random variables in distri-
bution. Therefore, probability integral transformation im-
plies

[F [L(S,z),U (S,z)]

η(cid:62)µ,η(cid:62)Ση

(η(cid:62)y) | y ∈ Pol(S), z = z0]

has a uniform distribution Unif(0, 1) for any z0. By inte-
grating out z0, the pivotal quantity Eq.(14) holds. In ad-
dition, an lower α-percentile of the distribution can be ob-
tained as

In the following, let us denote (S, z) by S for shorthand.
The remaining is to show that truncation points in Eqs.(13)
are equivalent to

L(S) = η(cid:62)y + θLη(cid:62)Ση

where θL = min

θ∈R θ s.t. y + θΣη ∈ Pol(S)

(15a)

and

U (S) = η(cid:62)y + θU η(cid:62)Ση

where θU = max

θ∈R θ s.t. y + θΣη ∈ Pol(S),
respectively. Simple calculation shows that, for any θ ∈ R,
we have

(15b)

y + θΣη ∈ Pol(S)
⇔ A(y + θΣη) ≤ b
⇔ θ · AΣη ≤ b − Ay.

 θ ≤ (b − Ay)j/(AΣη)j,

θ ≥ (b − Ay)j/(AΣη)j,
0 ≤ (b − Ay)j,

⇔

(AΣη)j > 0
(AΣη)j < 0
(AΣη)j = 0

.

On the other hand, by the deﬁnition of c and z in Lemma
7, it is easy to see that

L(S) = η(cid:62)y + η(cid:62)Ση max

j:(AΣη)j <0

(b − Ay)j
(AΣη)j

Therefore, for each j such that (AΣη)j < 0, we have

max

j:(AΣη)j <0

(b − Ay)j
(AΣη)j

≤ θ

and thus the minimum possible feasible θ would be
θL = min{θ ∈ R | y + θΣη ∈ Pol(S)}

= max

j:(AΣη)j <0

(b − Ay)j
(AΣη)j

.

Similarly, we see that the equivalency of U (S).
To complete the proof, let us consider a Gaussian random
variable y with mean Xβ∗ and covariance matrix σ2In
with some constant σ2. We can choose η = (X +
S )(cid:62)ej for
testing the null hypothesis H0,j : β∗S,j = 0 for each j ∈ S,
since η(cid:62)y reduces to the j-th element of an ordinary least
square estimator for the selected model, and in this case,
η(cid:62)Ση reduces to

S = σ2(cid:107)η(cid:107)2 = σ2(X(cid:62)S XS)−1
σ2
jj .

Then the critical values are computed as

qα = (F [L(S,z),U (S,z)]

η(cid:62)µ,η(cid:62)Ση

)−1(α).

α/2 = qα/2 = (F [L(S),U (S)]
(cid:96)S

0,σ2
S

)−1(α/2)

Selective Inference for Sparse High-Order Interaction Models

and

α/2 = q1−α/2 = (F [L(S),U (S)]
uS

0,σ2
S

)−1(1 − α/2),

respectively. From the above argument, there are no matter
to compute the truncation points in Eqs.(15) based on the
observations. In this case, Eqs.(15) can be written as

L(S) = η(cid:62)y + θLσ2(X(cid:62)S XS)−1
jj
where θL = min

θ∈R θ s.t. y + θσ2(X +

S )(cid:62)ej ∈ Pol(S)

and

U (S) = η(cid:62)y + θU σ2(X(cid:62)S XS)−1
jj
where θU = max

θ∈R θ s.t. y + θσ2(X +

S )(cid:62)ej ∈ Pol(S),

respectively, but we can ignore the scaling factor σ2 be-
cause

min{θ ∈ Rn | y + θ(X +
= min{σ2θ ∈ Rn | y + θσ2(X +

S )(cid:62)(cid:62)ej ∈ Pol(S)}

S )(cid:62)ej ∈ Pol(S)}

and

max{θ ∈ Rn | y + θ(X +
= max{σ2θ ∈ Rn | y + θσ2(X +

S )(cid:62)ej ∈ Pol(S)}

S )(cid:62)ej ∈ Pol(S)}.

A.3. Proof of Lemma 4
Proof. In MS, from Eq.(9), the constraint y +θη ∈ Pol(S)
is written as

≤ θ if (sjx·j + x·j(cid:48))(cid:62)η > 0,
(16a)
≥ θ if (sjx·j + x·j(cid:48))(cid:62)η < 0.
(16b)

≤ θ if (sjx·j − x·j(cid:48))(cid:62)η > 0
(16c)
≥ θ if (sjx·j − x·j(cid:48))(cid:62)η < 0.
(16d)

and

(−sjx·j − xj(cid:48))(cid:62)(y + θη) ≤ 0

⇔ −(sjx·j + x·j(cid:48))(cid:62)y
(sjx·j + x·j(cid:48))(cid:62)η
−(sjx·j + x·j(cid:48))(cid:62)y
(sjx·j + x·j(cid:48))(cid:62)η
(−sjx·j + xj(cid:48))(cid:62)(y + θη) ≤ 0
⇔ −(sjx·j − x·j(cid:48))(cid:62)y
(sjx·j − x·j(cid:48))(cid:62)η
−(sjx·j − x·j(cid:48))(cid:62)y
(sjx·j − x·j(cid:48))(cid:62)η
− sjx(cid:62)
·j (y + θη) ≤ 0
⇔ −sjx(cid:62)
≤ θ if sjx(cid:62)
·j y
sjx(cid:62)
·j η
−sjx(cid:62)
·j y
sjx(cid:62)
·j η

≥ θ if sjx(cid:62)

·j η > 0

·j η < 0

and

and

(16e)

(16f)

for all (j, j(cid:48)) ∈ S × ¯S. The conditions in Eqs.(16a), (16c),
and (16e) suggests that −θL must be at least smaller than
θ(a)
L in Eq.(11a), θ(b)
L in the second
last inequality in Eq.(11), respectively. Therefore, we have

L in Eq.(11c), and θ(c)

θL = − min{θ(a)

L , θ(b)

L }.
L , θ(c)

(cid:4)

Similarly, the conditions in Eqs.(16b), (16d), and (16f) im-
ply that

θL = − max{θ(a)

U , θ(b)

U }.
U , θ(c)

(cid:4)

A.4. Proof of Lemma 5
Proof. First, note that 0 ≤ xi˜j(cid:48) ≤ xij(cid:48) ≤ 1 for any
(j, j(cid:48), ˜j(cid:48)) ∈ S × ¯S × Desj(j(cid:48)). We ﬁrst prove Eq.(12a).

A.2. Proof of Lamma 3
Proof. Since xij ∈ [0, 1], for any pair (j, ˜j) such that ˜j ∈
Des(j), xj ≥ x˜j holds. Then,

|x(cid:62)

xi˜jyi +

i:yi>0

·˜j y| = | (cid:88)
 (cid:88)
 (cid:88)

≤ max

≤ max

i:yi>0

i:yi<0

xi˜jyi|

(cid:88)
xi˜jyi,− (cid:88)
xijyi, − (cid:88)

i:yi<0

i:yi>0

i:yi<0


 .

xi˜jyi

xijyi

(sjx·j + x·˜j(cid:48))(cid:62)y = sjx(cid:62)
≥ sjx(cid:62)

·j y +

·j y +

(cid:4)

≥ sjx(cid:62)

·j y +

(cid:88)

xi˜j(cid:48)yi

(cid:88)
(cid:88)
(cid:88)

i:yi<0

i:yi<0

i:yi>0

i:yi<0

xi˜j(cid:48)yi +

xi˜j(cid:48)yi.

xij(cid:48)yi = L(a)
E ,

which proves the ﬁrst line. Next, we prove Eq.(12b).

Selective Inference for Sparse High-Order Interaction Models

(cid:88)

xi˜j(cid:48)yi

B. Selectivxe inference for OMP
Lemma 8. Let η := (X +)(cid:62)ej. The solutions of the opti-
mization problems in (7) are respectively written as

(sjx·j + x·˜j(cid:48))(cid:62)y = sjx(cid:62)
≤ sjx(cid:62)

·j y +

·j y +

≤ sjx(cid:62)

·j y +

(cid:88)
(cid:88)
(cid:88)

i:yi>0

i:yi>0

i:yi>0

i:yi<0

xi˜j(cid:48)yi +

xi˜j(cid:48)yi.

xij(cid:48)yi = U (a)
E ,

which proves the second line. Eqs.
proved similarly.

(12c) to (12h) are
(cid:4)

θL = − min{θ(a)
θU = − max{θ(a)

L , θ(b)
U , θ(b)

L },
L , θ(c)
U },
U , θ(c)

A.5. Proof of Theorem 6
Proof. First, we prove (i). For any (j, j(cid:48), ˜j(cid:48)) ∈ S × ¯S ×
Desj(j(cid:48)), by using Lemma 5 directly, a lower and an upper
bound of sjx(cid:62)

·j y + x(cid:62)
E ≤ sjx(cid:62)
L(a)

·˜j(cid:48)y can be obtained as
·˜j(cid:48)y ≤ U (a)
Similarly, a lower and an upper bound of sjx(cid:62)
can be also obtained as

·j y + x(cid:62)

E

·j η + x(cid:62)
·˜j(cid:48)η

(17)

where

D ≤ sjx(cid:62)
L(a)

·j η + x(cid:62)

·˜j(cid:48)η ≤ U (a)

D

(18)

From Eq.(18), we have

D < 0 ⇒ (sjx·j + x·˜j(cid:48))(cid:62)η < 0
U (a)

for all (j, ˜j(cid:48)) ∈ S × Desj(j(cid:48)). It means that the (j, ˜j(cid:48))-th
constraint does not affect the solution of the optimization
problem in Eq.(11a). Now, we consider the case of U (a)
D >
0. If L(a)

D > 0, the value

(sjx·j + x·j(cid:48))(cid:62)y
(sjx·j + x·j(cid:48))(cid:62)η
E /U (a)

D when L(a)

can be bounded below by L(a)
D when L(a)
E /L(a)
L(a)
small values if L(a)
timal solution ˆθ(a)
solution of the optimization problem Eq.(11a), if

E > 0, and
E < 0, while the value can take any
D < 0. As a result, for the current op-
L , (j, j(cid:48))-th constraint does not affect the

L(a)
D > 0, L(a)

E > 0 and L(a)
E
U (a)
D

> ˆθ(a)
L ,

or

L(a)
D > 0, L(a)

E < 0 and L(a)
E
L(a)
D

> ˆθ(a)
L ,

because L(a)
prove (ii) – (iv) by the same argument.

D > 0 implies U (a)

D > 0. Similarly, we can
(cid:4)

θ(a)
L :=

θ(b)
L :=

min
(cid:48)
h∈[k], j
(s(h)x·(h)+x·j(cid:48) )

∈ ¯Sh,
(cid:62)

PSh η>0

min
(cid:48)
h∈[k], j
(s(h)x·(h)−x·j(cid:48) )

∈ ¯Sh,
(cid:62)

PSh η>0

,

(s(h)x·(h) + x·j(cid:48))(cid:62)PShy
(s(h)x·(h) + x·j(cid:48))(cid:62)PSh η
(19a)
(s(h)x·(h) − x·j(cid:48))(cid:62)PSh y
(s(h)x·(h) − x·j(cid:48))(cid:62)PSh η
(19b)

,

θ(c)
L :=

min
h∈[k],
·(h)PSh η>0

s(h)x(cid:62)

s(h)x(cid:62)
s(h)x(cid:62)

·(h)PShy
·(h)PSh η

,

(19c)

θ(a)
U :=

θ(b)
U :=

max
(cid:48)
h∈[k], j
(s(h)x·(h)+x·j(cid:48) )

∈ ¯Sh,
(cid:62)

PSh η<0

max
(cid:48)
h∈[k], j
(s(h)x·(h)−x·j(cid:48) )

∈ ¯Sh,
(cid:62)

PSh η<0

,

(s(h)x·(h) + x·j(cid:48))(cid:62)PShy
(s(h)x·(h) + x·j(cid:48))(cid:62)PSh η
(19d)
(s(h)x·(h) − x·j(cid:48))(cid:62)PSh y
(s(h)x·(h) − x·j(cid:48))(cid:62)PSh η
(19e)

,

θ(c)
U :=

max
h∈[k],
·(h)PSh η<0

s(h)x(cid:62)

s(h)x(cid:62)
s(h)x(cid:62)

·(h)PShy
·(h)PSh η

.

(19f)

Selective Inference for Sparse High-Order Interaction Models

(iii) Furthermore, consider solving the optimization prob-
lem in Eq.(19d), and let ˆθ(a)
U be the current optimal solu-
tion. If
{L(a)

U }
D > ˆθ(a)
U }
D > ˆθ(a)
is true, then the ˜j(cid:48)-th constraint in Eq. (10a) for any h ∈ [k]
and (j(cid:48), ˜j(cid:48)) ∈ ¯Sh × Des(h)(j(cid:48)) does not affect the optimal
solution in Eq.(19d).

D > 0} ∪ {U (a)
∪ {U (a)

D < 0, L(a)
D < 0, L(a)

E < 0, L(a)
E > 0, L(a)

E /U (a)
E /L(a)

(iv) Finally, consider solving the optimization problem in
Eq.(19e), and let ˆθ(b)
D > 0} ∪ {U (b)
∪ {U (b)

U be the current optimal solution. If
U }
D > ˆθ(b)
U }
D > ˆθ(b)

D < 0, L(b)
D < 0, L(b)

E < 0, L(b)
E > 0, L(b)

E /U (b)
E /L(b)

{L(b)

xij(cid:48)[PSh y]i

xij(cid:48)[PSh y]i

is true, then the (˜j(cid:48)-th constraint in Eq. (10b) for any
h ∈ [k] and (j(cid:48), ˜j(cid:48)) ∈ ¯Sh × Des(h)(j(cid:48)) does not affect
the optimal solution in Eq.(19e).

Lemma 9. For any h ∈ [k] and (j(cid:48), ˜j(cid:48)) ∈ ¯Sh×Des(h)(j(cid:48)),

L(a)
E := s(h)x(cid:62)

U (a)
E := s(h)x(cid:62)

xij(cid:48)[PSh y]i

xij(cid:48)[PSh y]i

i:[PSh y]i<0

i:[PSh y]i>0

L(a)
D := s(h)x(cid:62)

·(h)η +

xij(cid:48)[PShη]i

U (a)
D := s(h)x(cid:62)

·(h)η +

xij(cid:48)[PShη]i

i:[PSh η]i<0

·(h)PSh y +

·(h)PSh y +

(cid:88)
(cid:88)
≤ (s(h)x·(h) + x·˜j(cid:48))(cid:62)PSh y,
(cid:88)
≥ (s(h)x·(h) + x·˜j(cid:48))(cid:62)PSh y,
(cid:88)
≤ (s(h)x·(h) + x·˜j(cid:48))(cid:62)PSh η,
·(h)PSh y − (cid:88)
≥ (s(h)x·(h) + x·˜j(cid:48))(cid:62)PSh η,
·(h)PSh y − (cid:88)
·(h)η − (cid:88)
·(h)η − (cid:88)

≤ (s(h)x·(h) − x·˜j(cid:48))(cid:62)PSh η,

≥ (s(h)x·(h) − x·˜j(cid:48))(cid:62)PSh y,

≤ (s(h)x·(h) − x·˜j(cid:48))(cid:62)PSh y,

i:[PSh η]i>0

i:[PSh η]i>0

i:[PSh η]i<0

≥ (s(h)x·(h) − x·˜j(cid:48))(cid:62)PSh η.

i:[PSh y]i>0

i:[PSh y]i<0

L(b)
E := s(h)x(cid:62)

U (b)
E := s(h)x(cid:62)

L(b)
D := s(h)x(cid:62)

xij(cid:48)[PShη]i

U (b)
D := s(h)x(cid:62)

xij(cid:48)[PShη]i

Theorem 10. (i) Consider solving the optimization prob-
lem in Eq.(19a), and let ˆθ(a)
L be the current optimal solu-
tion, i.e., we know that the optimal θ(a)
L is at least no greater
than ˆθ(a)
{U (a)

L }
D > ˆθ(a)
L }
D > ˆθ(a)
is true, then the ˜j(cid:48)-th constraint in Eq. (10a) for any h ∈ [k]
and (j(cid:48), ˜j(cid:48)) ∈ ¯Sh × Des(h)(j(cid:48)) does not affect the optimal
solution in Eq.(19a).

L . If
D < 0} ∪ {L(a)
∪ {L(a)

D > 0, L(a)
D > 0, L(a)

E < 0, L(a)
E > 0, L(a)

E /L(a)
E /U (a)

{U (b)

(ii) Next, consider solving the optimization problem in
Eq.(19b), and let ˆθ(b)
D < 0} ∪ {L(b)
∪ {L(b)

L be the current optimal solution. If
L }
D < ˆθ(b)
D > 0, L(b)
L }
D > 0, L(b)
D < ˆθ(b)
is true, then the ˜j(cid:48)-th constraint in Eq. (10b) for any h ∈ [k]
and (j(cid:48), ˜j(cid:48)) ∈ ¯Sh × Des(h)(j(cid:48)) does not affect the optimal
solution in Eq.(19b).

E < 0, L(b)
E > 0, L(b)

E /L(b)
E /U (b)

