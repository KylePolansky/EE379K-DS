Supplementary Material: Multiple Clustering Views from Multiple

Uncertain Experts

Yale Chang Junxiang Chen Michael H. Cho

Peter J. Castaldi Edwin K. Silverman Jennifer G. Dy

1 Parameter Settings
All approaches need to specify K, the number of clusters.
When applying our MCVC to the synthetic and benchmark datasets:
• If there is only one ground-truth expert view, we set K to be the true number of clusters in that view.
• If there are more than one ground-truth expert view, we set K to be the maximal value among the true

number of clusters in all expert views.

For SemiCrowd, ITML, MPCKMeans, CSPA, since we only apply them to one expert view, we set K to be
the true number of clusters in that view.
For COPD data, we set K = 4 for all approaches according to a recent study on COPD [3].
Besides the number of clusters, the parameters that are speciﬁc to each approach are set as follows.
1.1 Proposed Approach: MCVC
For variational inference in our approach, we use the following parameter settings
1. G, the number of components in truncated Dirichlet Process, is set to be M/2, where M is the total
number of experts. In this way, we try to enforce the constraint that on average, there should be at least
two experts in each view. In all experiments, the number of expert views recovered by our approach is
smaller than G = M/2. Therefore, the value of G we use is large enough to discover the true number
of expert views.

2. For the parameters of prior distributions:

• p(αm), p(βm): set the parameters of prior Beta distributions to be (10, 1) to incorporate the prior
knowledge that each expert’s accuracy parameters should be far away from 0.5 (random guess) and
close to 1. The choice can be illustrated from Figure 1. Under this setting, there is very small
probability that the accuracy parameters can be close to 0.5.

Figure 1: Probability density function of Beta distribution Beta(α, 1), α = 10 can effectively make the accuracy
parameters have very small probability to be close to 0.5.

• p(νg): set its concentration parameter γ = 1, the recovered number of experts is stable across a

range of different γ values (from 0.5 to 10).

• p(W (g)

ij ), p(b(g)

respectively.

i

): set the mean and standard deviation of prior Gaussian distributions to be 0 and 1

– 1 –

0.00.20.40.60.81.00246810alpha=1alpha=2alpha=5alpha=10After learning our model using the training set, we can obtain the variational distributions of weight
q(W (g)) and offset q(b(g)). We can cluster xt, a sample from the testing set by integrating out W (g), b(g)
through Monte Carlo approximation to the integration:

p(Z (g)

t = k|xt) =

p(Z (g)

t

|W (g), b(g); xt)q(W (g))q(b(g))dW (g)db(g)

(cid:90) (cid:90)
L(cid:88)

l=1

≈ 1
L

l,(cid:100)b(g)

l; xt)

p(Z (g)

t

|(cid:91)
W (g)

(1)

(2)

3. Initializations of variational parameters:

• the parameters of variational Beta distribution are initialized to be equal to those of the prior distri-

bution;
• the means of variational Gaussian distributions are initialized by randomly drawing samples from
Gaussian distribution N (0, 1
D ), the standard deviations of variational Gaussian distributions are
initialized to be 0.001. This initialization strategy is similar to Xavier initialization [5] in order to
avoid gradient saturation;
i,: are initialized using W (g), b(g) according to the discriminative clustering model described in
the main paper;

• η(g)

• φm,: are initialized by sampling from a Dirichlet distribution with all its parameters being equal to

1, introducing most randomness for the initialization.

4. The number of random initializations for optimization is set to be 50 and the results are stable across

different runs in all our experiments.

5. Cluster samples in the testing set:

l,(cid:100)b(g)

where (cid:91)
W (g)
experiments. xt is assigned to the cluster corresponding to the largest probability:

l are the l-th sample from q(W (g)) and q(b(g)) respectively. We set L = 100 in all

ˆyt = arg max
k=1,···K

p(Z (g)

t = k|xt)

(3)

where ˆyt is the predicted cluster label for xt.

1.2 Meta Clustering

After computing the similarity matrix between multiple experts, we apply spectral clustering [6] to assign
experts to different views. The number of views are automatically determined using the eigen-gap heuristic
[7].
1.3 SemiCrowd

We follow parameter settings recommended by the author in [9]. In particular, d0, d1, two thresholds used
to ﬁlter out uncertain sample pairs in the average similarity matrix, are set to be 0 and 0.8 respectively.
Parameters of the (cid:96)1 regularized matrix completion algorithm is set according to heuristics in the literature
[8].
1.4 ITML

As is suggested by the author [4], we set lower and upper bounds associated with the constraint terms to
be the 5th and 95th percentiles of the observed distribution of distances between pairs of points within the
dataset. Other parameters related to the convergence of the algorithm are set to be default.
1.5 MPCKMeans

We specify the constraints according to the instructions listed on the author’s website and directly run the
author’s Java implementation [1]

– 2 –

1.6 CSPA

After computing the average similarity matrix, we use spectral clustering [6] to obtain the cluster labels.
2 Variational Inference
2.1 Variational Distribution
Denote h = {α1:M , β1:M , c1:M , ν1:G, Z (1:G), W (1:G), b(1:G)} as the collection of latent variables, where G
is the number of components of the truncated Dirichlet Process. We assume the variational distribution has
the following factorization formula:

q(α1:M , β1:M , c1:M , ν1:G, Z (1:G), W (1:G), b(1:G))
= q(α1:M ) · q(β1:M ) · q(c1:M ) · q(ν1:G) · q(Z (1:G)) · q(W (1:G)) · q(b(1:G))

M(cid:89)
M(cid:89)

m=1

=

=

[q(αm) · q(βm) · q(cm)]

[q(αm) · q(βm) · q(cm)]

G(cid:89)
G(cid:89)

g=1

(cid:104)
q(νg)

n(cid:89)

q(νg)q(Z (g))q(W (g))q(b(g))

d(cid:89)

K(cid:89)

q(Z (g)

i

)

q(W (g)
ij )



)

q(b(g)

i

K(cid:89)

i=1

m=1

g=1

i=1

i=1

j=1

(cid:105)

(4)
(5)

(6)

(7)

α1 , τ (m)
α2 );
β1 , τ (m)
β2 );

where the marginal variational distribution of each random variable is
• q(αm) = Beta(τ (m)
• q(βm) = Beta(τ (m)
• q(cm) = Cat(φm,:);
• q(νg) = Beta(τ (g)
ν1 , τ (g)
ν2 );
) = Cat(η(g)
• q(Z (g)
i,: );
ij ) = N (µ(g)
• q(W (g)
) = N (µ(g)
• q(b(g)
We use θ to denote all the variational parameters, which consist of the following
• For m = 1··· M, consider {τ (m)
dimensional space.
• For g = 1··· G, consider τ (g)
µ(g)
bi

α2 , τ (m)
i,: ∈ ∆K−1(i = 1··· n), µ(g)

(i = 1··· K)

, σ(g)2
Wij
).

Wij
, σ(g)2

ν1 , τ (g)

ν2 , η(g)

α1 , τ (m)

β1 , τ (m)

, σ(g)2

bi

bi

bi

);

i

i

β2 , φm,: ∈ ∆G−1}, where ∆G−1 is a simplex in G-
(i = 1··· d, j = 1··· K),

, σ(g)2
Wij

Wij

Besides the simplex constraints on the parameters of categorical distribution, both the parameters of Beta
distribution and the standard deviation of Gaussian distribution should have positive constraints.
2.2 Evidence Lower Bound

Given variational distribution q(h; θ), the log-likelihood log p(S(1:M )) can be decomposed as

(cid:104)

– 3 –

(cid:105)

(cid:105)

log p(S(1:M )) = Lq(h;θ) + KL

q(h; θ)| p(h|S(1:M ))

(cid:104)
(cid:104)
(cid:104)

≥ Lq(h;θ)
= Eq(h;θ)

= Eq(h;θ)

= Eq(h;θ)

log p(h, S(1:M )) − log q(h; θ)
log p(S(1:M )|h) + log p(h) − log q(h; θ)
log p(S(1:M )|h)

(cid:105) − KL [q(h; θ)| p(h)]

(cid:105)

(8)

(9)

(10)

(11)

(12)

The ﬁrst term prefers the variational distribution q(h; θ) that maximizes the expected conditional likelihood,
the second term forces the variational distribution to be close to the prior distribution.
The overall objective of variational inference is to maximize the evidence lower bound (ELBO) Lq(h;θ)
w.r.t. θ.
Let f (h; θ) = log p(S(1:M )|h) + log p(h) − log q(h; θ), the evidence lower bound can be expressed as

(cid:2)f (h; θ)(cid:3)

Lq(h;θ) = Eq(h;θ)

We ﬁrst compute the three terms in f (h; θ) and then evaluate their expectations w.r.t. the variational distri-
bution q(h; θ).
2.3 ELBO: The First Term log p(S(1:M )|h)
Consider the ﬁrst term:

log p(S(1:M )|h) = log

(13)

(14)

(15)

(16)

(17)
(18)
(19)

(20)

(21)

(22)

(23)

(24)

p(S(m)

ij

|h)

(i,j)∈E(m)

log p(S(m)

ij

|h)

=

=

(cid:89)

m=1

 M(cid:89)
(cid:88)
M(cid:88)
(cid:26)(cid:16) (cid:88)
M(cid:88)
(cid:16) (cid:88)
(cid:16) (cid:88)

(i,j)∈E(m)

(i,j)∈E(m)

m=1

m=1

+

+

(i,j)∈E(m)

(cid:17)

(cid:17)



(cid:16) (cid:88)

(cid:27)

(i,j)∈E(m)

1

log βm +

(cid:17)

(cid:104)

S(m)
ij A(m)

ij

log

S(m)
ij

(i,j)∈E(m)
αmβm

(1 − αm)(1 − βm)

log

(cid:105)

A(m)

ij

1 − αm
βm

log(

)

(cid:17)

(cid:16) 1 − βm

(cid:17)

βm

To compute Eq(h;θ)[log p(S(1:M ) | h)], the following formulas can be used:
α1 ) − ψ(τ (m)

Eq(h;θ)[log αm] = Eq(αm)[log αm] = ψ(τ (m)

Eq(h;θ)[log(1 − αm)] = Eq(αm)[log(1 − αm)] = ψ(τ (m)

Eq(h;θ)[log βm] = Eq(βm)[log βm] = ψ(τ (m)

Eq(h;θ)[log(1 − βm)] = Eq(βm)[log(1 − βm)] = ψ(τ (m)
= Z (cm)
)]

Eq(h;θ)[A(m)

ij

] = Eq(cm,Z(cm))[I(Z (cm)
= Eq(cm)

i

j
= Z (cm)

j

)](cid:3)

(cid:2)Eq(Z(cm ))[I(Z (cm)
(cid:105)
(cid:104) K(cid:88)
(cid:21)
K(cid:88)

η(cm)
ik

η(cm)
jk

k=1

i

ηg
ikηg

jk

= Eq(cm)

(cid:20)

G(cid:88)

=

φmg

α1 + τ (m)
α2 )
α2 ) − ψ(τ (m)
α1 + τ (m)
α2 )
β1 + τ (m)
β2 )
β2 ) − ψ(τ (m)

β1 + τ (m)
β2 )

β1 ) − ψ(τ (m)

g=1

k=1

– 4 –

2.4 ELBO: The Second Term log p(h)

Consider the second term:

(cid:16)

(cid:18) M(cid:89)
(cid:16)
M(cid:88)
∞(cid:88)

(cid:16)

m=1

m=1

+

(cid:17) ∞(cid:89)

(cid:16)

g=1

(cid:17)

log p(h) = log

p(αm)p(βm)p(cm|ν1:∞)

p(νg)p(Z (g)|W (g), b(g))p(W (g))p(b(g))

=

log p(αm) + log p(βm) + log p(cm|ν1:∞)

log p(νg) + log p(Z (g)|W (g), b(g)) + log p(W (g)) + log p(b(g))

g=1

where

log p(αm) = (τ (m)

α10 − 1) log αm + (τ (m)

α20 − 1) log(1 − αm) − log

(cid:17)(cid:19)

(cid:17)

(cid:18) Γ(τ (m)
(cid:18) Γ(τ (m)

Γ(τ (m)

Γ(τ (m)

α10 )Γ(τ (m)
α20 )
α10 + τ (m)
α20 )
β10 )Γ(τ (m)
β20 )
β10 + τ (m)
β20 )

(cid:19)
(cid:19)

(25)

(26)

(27)

(28)

(29)

(30)

(31)

(32)

(33)

(34)

(35)

(36)

(37)

log p(cm|ν1:∞) =

log p(βm) = (τ (m)

β20 − 1) log(1 − βm) − log

β10 − 1) log βm + (τ (m)
∞(cid:88)
g−1(cid:88)

(cid:18)

cmg

log νg +

g=1

j=1

(cid:19)
(cid:18) Γ(γ)

log(1 − νj)

(cid:19)

Γ(1 + γ)

log p(νg) = (γ − 1) log(1 − νg) − log
= (γ − 1) log(1 − νg) + log γ
|W (g), b(g))

p(Z (g)

log p(Z (g)|W (g), b(g)) = log

= log

k=1

i=1

i=1

N(cid:89)
K(cid:89)
N(cid:89)
K(cid:88)
K(cid:88)
K(cid:88)
(cid:18)

(cid:18)

k=1

k=1

j=1

i=1

N(cid:88)
N(cid:88)
d(cid:88)
K(cid:88)

i=1

i=1

i=1

i =k]

i

p(cid:0)Z (g)
i = k|W (g), b(g)(cid:1)I[Z(g)
(cid:18)

i = k] log p(Z (g)

I[Z (g)

I[Z (g)

i = k]

(w(g)T

k

i = k|W (g), b(g))
(cid:16) K(cid:88)
(cid:19)

k ) − log

xi + b(g)

j=1

Wij 0)2

ij − µ(g)
(cid:19)
2σ(g)2
Wij 0

Wij 0) − (W (g)
i − µ(g)
bi0)2
2σ(g)2
bi0

√
− log(

2πσ(g)

√
− log(

2πσ(g)

bi0) − (b(g)

=

=

log p(W (g)) =

log p(b(g)) =

(cid:17)(cid:19)

exp(w(g)T

j

xi + b(g)
j )

To compute Eq(h;θ)[log p(h)], the following formulas can be used besides the formulas used to compute

– 5 –

Eq(h;θ)[log p(S(1:M ) | h)]:

Eq(h;θ)[log νg] = Eq(νg)[log νg] = ψ(τ (g)

Eq(h;θ)[log(1 − νg)] = Eq(νg)[log(1 − νg)] = ψ(τ (g)

ν1 ) − ψ(τ (g)

ν1 + τ (g)
ν2 )
ν2 ) − ψ(τ (g)
ν1 + τ (g)
ν2 )

(cid:2)I[Z (g)
i = k](cid:3) = η(g)

ik

Eq(h;θ)

Eq(h;θ)[cmg] = Eq(cm)[cmg] = φmg

Eq(h;θ)[(W (g)

Eq(h;θ)[(b(g)

k ] = Eq(W (g))[w(g)
+ (µ(g)
Wij
+ (µ(g)
bi

Wij 0)2] = σ(g)2
bi0)2] = σ(g)2

k ] = µ(g)
W:k
− µ(g)
Wij 0)2
− µ(g)
bi0)2

Wij

bi

Eq(h;θ)[w(g)
ij − µ(g)
i − µ(g)
(cid:20)

(cid:16)(cid:80)K

(cid:17)(cid:21)

2.4.1 Upper Bound of Log-sum Function

(cid:16) K(cid:88)

does not have closed form and can only be
The computation of Eq(h;θ)
approximated through sampling. However, we can use the upper bound of log-sum function based on the
log concavity to derive a lower bound of ELBO [2].

j=1 exp(w(g)T

xi + bj)

log

j

log

exp(w(g)T

j

xi + b(g)
j )

j=1

j=1

exp(w(g)T

j

xi + b(g)

j ) − log(r(g)

i

) − 1

(45)

We need introduce new variational parameters r(g)
bounded as follows

i > 0 to optimize. The expectation term can also be upper

(cid:16) K(cid:88)
K(cid:88)

j=1

j=1

(cid:20)

log

(cid:20)
K(cid:88)
K(cid:88)

j=1

j=1

(cid:32)

= r(g)

i

= r(g)

i

Eq(h;θ)

exp(w(g)T

j

xi + bj)

≤ Eq(h;θ)

r(g)
i

xi + b(g)

j ) − log(r(g)

i

(cid:21)

) − 1

Eq(h;θ)[exp(

w(g)
lj xil + b(g)

j )] − log(r(g)

i

) − 1

Eq(b(g)

j

Eq(W (g)
lj )

− log(r(g)

i

) − 1

lj xil(cid:3)(cid:33)
(cid:2)ew(g)
(cid:17)

σ(g)2
bj
2

+

(cid:16)
(cid:16)

µ(g)
bj

µ(g)
Wlj

xil +

x2
il

σ(g)2
Wlj
2

(cid:17)

(cid:17) ≤ r(g)

i

K(cid:88)

(cid:17)(cid:21)

)

j

l=1

l=1

exp(w(g)T

d(cid:88)
j (cid:3) d(cid:89)
(cid:2)eb(g)
(cid:2)eb(g)
j (cid:3) = exp
lj xil(cid:3) = exp
(cid:2)ew(g)
(cid:17)(cid:21)
(cid:16)
d(cid:88)

xi + bj)

)

σ(g)2
bj
2

+

µ(g)
Wlj

l=1

– 6 –

(38)

(39)
(40)
(41)
(42)

(43)

(44)

(46)

(47)

(48)

(49)

(50)

where the expectation can be evaluated using the mean of log-normal distribution.

Therefore, the upper bound can be written as

Eq(b(g)

j

Eq(W (g)
lj )

(cid:16) K(cid:88)
(cid:32)

j=1

exp

(cid:18)

(cid:20)
K(cid:88)

j=1

Eq(h;θ)

log

exp(w(g)T

j

≤ r(g)

i

µ(g)
bj

+

(cid:17)(cid:19)(cid:33)

xil +

x2
il

σ(g)2
Wlj
2

− log(r(g)

i

) − 1

(51)

(cid:16)

µ(g)
Wlj

xil +

x2
il

σ(g)2
Wlj
2

(cid:17)(cid:19)(cid:33)

The upper bound becomes minimal when

r(g)
i =

j=1

exp

µ(g)
bj

+

Then the upper bound becomes

(cid:32)

(cid:18)

(cid:16) K(cid:88)
(cid:32)
(cid:18)

j=1

exp

(cid:80)K
(cid:20)
K(cid:88)

j=1

l=1

σ(g)2

bj

1

2 +(cid:80)d
(cid:17)(cid:21)
(cid:16)
d(cid:88)

xi + bj)

+

l=1

Eq(h;θ)

log

exp(w(g)T

j

≤ log

µ(g)
bj

+

σ(g)2
bj
2

(cid:17)(cid:19)(cid:33)

µ(g)
Wlj

xil +

x2
il

σ(g)2
Wlj
2

To avoid numerical overﬂow when computing this bound due to the potential large exponent of the expo-
nential function, we can use the logsumexp() function implemented in Scipy. The idea is to extract the
maximal value in the sequence and compute its log, then each exponent in the sequence will be less than 1,
leading to stable numerical behavior.
2.5 ELBO: The Third Term log q(h; θ)
Consider the third term:

(cid:19)

log q(αm) + log q(βm) + log q(cm)

n(cid:88)

K(cid:88)

log q(νg) +

I[Z (g)

i = k] log q(Z (g)

i = k) +

g=1

i=1

k=1

d(cid:88)

K(cid:88)

i=1

j=1

log q(W (g)

ij ) +

(55)

log q(b(g)

i

(cid:19)

)

K(cid:88)

i=1

(cid:18)
M(cid:88)
(cid:18)
G(cid:88)

m=1

+

log q(h; θ) =

where

(52)

(53)

(54)

(56)

(57)

(58)

(59)

(60)

(61)

(62)

(63)

(cid:16) Γ(τ (m)
(cid:16) Γ(τ (m)

Γ(τ (m)

Γ(τ (m)

α1 )Γ(τ (m)
α2 )
α1 + τ (m)
α2 )
β1 )Γ(τ (m)
β2 )
β1 + τ (m)
β2 )

(cid:17)
(cid:17)

(cid:16) Γ(τ (g)

ν1 )Γ(τ (g)
ν2 )
ν1 + τ (g)
ν2 )

Γ(τ (g)

(cid:17)

log q(αm) = (τ (m)

α1 − 1) log αm + (τ (m)

α2 − 1) log(1 − αm) − log

log q(βm) = (τ (m)

β1 − 1) log βm + (τ (m)
G(cid:88)

cmg log φmg

log q(cm) =

β2 − 1) log(1 − βm) − log

g=1

log q(νg) = (τ (g)

ν1 − 1) log νg + (τ (g)

ν2 − 1) log(1 − νg) − log

log q(Z (g)

log q(W (g)

ik

i = k) = log η(g)
√
ij ) = − log(
√
) = − log(

log q(b(g)

i

) − (W (g)

2πσ(g)
Wij

) − (b(g)

2πσ(g)
bi

)2

Wij

ij − µ(g)
2σ(g)2
Wij
i − µ(g)
bi
2σ(g)2

)2

bi

The computation of Eq(h;θ)[log q(h; θ)] can be done using formulas from the previous two subsections.

– 7 –

ij

, the weight of S(m)

3 Weights of Different Experts
As we can see from the derivation of the ﬁrst-term log p(S(1:M )|h) of ELBO, if we view Equation (16) as
a function of A(m)
References
[1] M. Bilenko. Java implementation of mpckmeans, 2004.
[2] G. Bouchard. Efﬁcient bounds for the softmax function and applications to approximate inference
in hybrid models. In NIPS 2007 workshop for approximate Bayesian inference in continuous/hybrid
systems. Citeseer, 2007.

(constraints provided by the m-th expert) is: log

αmβm

(1−αm)(1−βm).

ij

[3] P. J. Castaldi, J. Dy, J. Ross, Y. Chang, G. R. Washko, D. Curran-Everett, A. Williams, D. A. Lynch,
B. J. Make, J. D. Crapo, et al. Cluster analysis in the copdgene study identiﬁes subtypes of smokers
with distinct patterns of airway disease and emphysema. Thorax, pages thoraxjnl–2013, 2014.

[4] J. V. Davis, B. Kulis, P. Jain, S. Sra, and I. S. Dhillon.

Information-theoretic metric learning.

In

Proceedings of the 24th international conference on Machine learning, pages 209–216. ACM, 2007.

[5] X. Glorot and Y. Bengio. Understanding the difﬁculty of training deep feedforward neural networks.

In Aistats, volume 9, pages 249–256, 2010.

[6] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral clustering: Analysis and an algorithm. Advances in

neural information processing systems, 2:849–856, 2002.

[7] U. Von Luxburg. A tutorial on spectral clustering. Statistics and computing, 17(4):395–416, 2007.
[8] J. Yi, R. Jin, A. K. Jain, and S. Jain. Crowdclustering with sparse pairwise labels: A matrix completion

approach. In AAAI Workshop on Human Computation, volume 2. Citeseer, 2012.

[9] J. Yi, R. Jin, S. Jain, T. Yang, and A. K. Jain. Semi-crowdsourced clustering: Generalizing crowd
labeling by robust distance metric learning. In Advances in Neural Information Processing Systems,
pages 1772–1780, 2012.

– 8 –

