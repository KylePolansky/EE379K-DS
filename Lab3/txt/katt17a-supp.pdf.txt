Long Version of Proof of Theorem 3

Supplement to ICML submission “Learning in POMDPs with Monte Carlo Tree Search”

While RS-BA-POMCP is potentially more eﬃcient, it is not directly whether it still converges to an -optimal
value function. Here we show the main steps in proving that it is sound. These main steps are similar to the proof
in POMCP. We point out however, that the technicalities of proving the components are far more involved.

Notation
We will use the following notation.

Action-observation histories.
• hd is an action-observation history at depth d of a simulation,
• hd = (a0, z1, . . . , ad−1, zd).

‘Full’ histories.

In addition to actions and observations, full histories also include the states.

steps of ‘real’ experience H0 = (s−k, a−k, s−k+1, z−k−1, . . . , a−1, s0, z0)
(Hd−1, ad−1, sd, zd) = (cid:104)s0:d, hd(cid:105).

• H0 the (unknown) full history (of real experience) at the root of the simulation:
• Hd is a full history (of simulated experience) at depth d in the lookahead tree: Hd = (H0, a0, s1, z1, a1, s2, z2, . . . , ad−1, sd, zd) =
• H (i)
• In our proof, we will also need to indicate if a particular full history Hd is consistent with a full history at the

the full history at depth d corresponding to simulation i.

i.e., if there have been k

d

root of simulation:

(cid:40)

Cons(H0, Hd) =

1 if Hd is consistent with the full history at the root H0 ,
0 otherwise.

Dynamics Function. We fold transition and observations function into one:

sa = Dst−1,at−1(st, zt) = D(st, zt|st−1, at−1) = Pr(st, zt|st−1, at−1)

• D denotes the dynamics model
• Dstzt
• Dsa denotes the vector:

= Ds(cid:48)z

st−1at−1

(cid:68)

sa , . . . , Ds|S|z|Z|
Ds1z1

sa

(cid:69)

sa denotes how often (cid:104)s(cid:48), z(cid:105) occurred after (cid:104)s, a(cid:105)

• χ =(cid:10)χs1a1, . . . , χs|S|a|A|

Counts
• χs(cid:48)z
• χsa is the vector of counts for (cid:104)s, a(cid:105).
• χ(Hd) denotes the vector of counts at simulated full history Hd.
• If χ0 is the count vector at the root of simulation, we have that χ(Hd) = χ0 + ∆(Hd), with ∆(Hd) the vector

(cid:11) is the total collection of all such count vectors.

of counts of all (s, a, s(cid:48), z) quadruples occurring in Hd.

Dirichlet distributions

• Dir(x|α) = Pr(x; α) = B(α)(cid:81)K
• let x = (cid:104)x1 . . . xK(cid:105) ∈ ∆K and α = (cid:104)α1 . . . αK(cid:105) be a count vector
• with B(α) = Γ((cid:80)
(cid:81)
i αi)
i Γ(αi) the Dirichlet normalization constant, with Γ the gamma function.
– for a particular s, a: Dir(Dsa|χsa) = Pr(Dsa; χsa) = B(χsa)(cid:81)(cid:104)s(cid:48),z(cid:105)∈S×Z
• So, in translated in terms of dynamics function and counts, we have:

i=1 xαi−1

(cid:17)χs(cid:48) z

Ds(cid:48)z

(cid:16)

sa

sa

i

1

– we will also abuse notation and write Dir(D|χ) =(cid:81)(cid:104)s,a(cid:105) Dir(Dsa|χsa).

Var.

• ˙x denotes a root sampled quantity x.
• I{condition} is the indicator function which is 1 iﬀ condition is true and 0 otherwise.

Deﬁnitions
Deﬁnition 1. The expected full-history expected transition BA-POMDP rollout distribution is the distribution
over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π. It is given
by

P π(Hd+1) = Dχ(Hd)(sd+1, zd+1|as, sd)π(ad|hd)P π(Hd)

(1)

with P π(H0) = b0((cid:104)s0, χ0(cid:105)) the belief ‘now’ (at the root of the online planning).

Deﬁnition 2. The full-history root-sampling (RS) BA-POMDP rollout distribution is the distribution over full
histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π in combination
with root sampling of the dynamics model D. This distribution, for a particular stage d, is given by

where

K(Hd) (cid:44) 1
˜P π
Kd

Kd(cid:88)

i=1

I(cid:110)

Hd=H (i)

d

(cid:111),

• K is the number of simulations that comprise the empirical distribution,
• Kd is the number of simulations that reach depth d (not all simulations might be equally long),
• H (i)

is the history speciﬁed by the i-th particle at stage d.

d

throughout this proof we assume that there is only 1 initial count vector at the root. Or put better:
Remark:
we assume that there is one unique H0 at which all simulations start. However, for ‘real’ steps t > 0 we could be
. In this case, root sampling from the
all corresponding to the same observed real history hreal
in diﬀerent H real
belief can be thought of root sampling the initial full history H0 ∼ b(H real
). As such, our proof shows convergence
in probability of
p→ P π(Hd|H0).

(Hd|H0)

∀H0∀Hd

t

t

t

for each such sampled H0. It is clear that that directly implies that

˜P π
Kd

(cid:104) ˜P π

Kd

(cid:105) p→ EH0 [P π(Hd|H0)] = P π(Hd).

∀Hd

˜P π
Kd

(Hd) = EH0

(Hd|H0)

In the below, we omit the explicit conditioning on H0.

Proof of Main Theorem
The proof depends on a lemma that follows below.

Theorem 3. The full-history RS-BA-POMDP rollout distribution (Def. 2) converges in probability to full-history
BA-POMDP rollout distribution (Def. 1):

∀Hd

˜P π
Kd

(Hd)

p→ P π(Hd).

(2)

Proof. For ease of notation we prove this for stage d + 1. Note that a history Hd+1 = (Hd, ad, sd+1, zd+1), only
diﬀers from Hd in that it has one extra transition for the (sd, ad, sd+1, zd+1) quadruple, implying that χ(Hd+1) only
diﬀers from χ(Hd) in the counts χsdad for sdad. Therefore, the expression for ˜P π
(Hd) derived in Lemma 4 below
Kd
(cf. equation (20)) can be written in recursive form as

2

˜P π(Hd+1) = Cons(H0, Hd)

π(at|ht)

B(χsa(H0))
B(χsa(Hd+1))

(cid:89)

(cid:104)s,a(cid:105)

t=0

d(cid:89)
d−1(cid:89)
d−1(cid:89)
Cons(H0, Hd)
d−1(cid:89)

t=0

t=0

=

= Cons(H0, Hd)

π(at|ht)π(ad|hd)

= Cons(H0, Hd)

π(at|ht)π(ad|hd)

(cid:89)

π(at|ht)

t=0

(cid:104)s,a(cid:105)
B(χsdad (Hd))
B(χsdad (Hd+1))

(cid:89)
(cid:89)

(cid:104)s,a(cid:105)

(cid:104)s,a(cid:105)

B(χsa(H0))
B(χsa(Hd))

B(χsa(Hd))
B(χsa(Hd+1))

B(χsa(H0))
B(χsa(Hd))

(cid:89)
 π(ad|hd)

(cid:104)s,a(cid:105)

B(χsa(H0))
B(χsa(Hd))

B(χsdad (Hd))
B(χsdad (Hd+1))

B(χsa(Hd))
B(χsa(Hd+1))



= ˜P π(Hd)π(ad|hd)

with base case ˜P π(H0) = 1, and

B(χsdad (Hd))
B(χsdad (Hd+1))

=

B(χsdad (H0))
B(χsdad (Hd+1))

· B(χsdad (Hd))
B(χsdad (H0))

=

B(χsdad (H0))/B(χsdad (Hd+1))
B(χsdad (H0))/B(χsdad (Hd))

(3)

the result of dividing out the contribution of the old counts for sdad and multiplying in the new contribution, and
similar for the observations probabilities. Now, we investigate these terms more closely.

transition for the (sd, ad, sd+1, zd+1) quadruple. Let us write T =(cid:80)
only has 1 extra transition, we also know that for this history, the total counts is one higher: (cid:80)

Again remember that the sole diﬀerence between Hd+1 = (Hd, ad, sd+1, zd+1) andHd is that it has one extra
(Hd) for the total of the counts for
(Hd) for the number of counts for that such a transition was to (sd+1zd+1). Because Hd+1
(Hd+1) =
(Hd+1) = N + 1. Now let us expand the term

T + 1 and since that transition was to (sd+1zd+1) the counts χsd+1zd+1
from (3):

sd, ad and N = χsd+1zd+1

(s(cid:48),z) χs(cid:48)z

(s(cid:48),z) χs(cid:48)z

sdad

sdad

sdad

sdad

B(χsdad (Hd))
B(χsdad (Hd+1))

=

=

=

=

Γ(T )/(cid:81)
Γ(T + 1)/(cid:81)
s(cid:48)z Γ(χs(cid:48)z
(cid:81)
s(cid:48)z Γ(χs(cid:48)z
(cid:81)
s(cid:48)z Γ(χs(cid:48)z
s(cid:48)z Γ(χs(cid:48)
Γ(χsd+1zd+1

Γ(T + 1)

Γ(T )

Γ(T )

sdad

sdad

sdad

Γ(T + 1)

Γ(T )

Γ(T + 1)

sdad
Γ(χsd+1zd+1

sdad
Γ(χsd+1zd+1
Γ(χsd+1zd+1

sdad

sdad

(Hd))
(Hd+1))

sdad

(Hd+1))
(Hd))

(Hd+1))(cid:81)
(Hd))(cid:81)

(Hd+1))
(Hd))

=

s(cid:48)z(cid:54)=(sd+1zd+1) Γ(χs(cid:48)z
s(cid:48)z(cid:54)=(sd+1zd+1) Γ(χs(cid:48)z
sdad
Γ(N + 1)

Γ(T )

sdad

Γ(T + 1)

Γ(N )

(Hd+1))
(Hd))

Now, the gamma function has the property that Γ(x + 1) = xΓ(x) [DeGroot, 2004], which means that we get

=

Γ(T )
T Γ(T )

N Γ(N )
Γ(N )

=

N
T

.

Therefore we get

and thus

B(χsdad (Hd))
B(χsdad (Hd+1))

=

χsd+1zd+1
sdad
(s(cid:48),z) χs(cid:48)z

sdad

(Hd)

(Hd)

(cid:80)

(cid:80)

χsd+1zd+1
sdad
(s(cid:48),z) χs(cid:48)z

sdad

(Hd)

(Hd)

˜P π(Hd+1) = ˜P π(Hd)π(ad|hd)

.

(4)

the r.h.s. of this equation is identical to (1) except for the diﬀerence in between ˜P π(Hd) and P π(Hd). This can be
resolved by forward induction with base step: ˜P π(H0) = b0((cid:104)s0, χ0, ψ0(cid:105)) = P π(H0), and the induction step (show
˜P π(Hd+1) = P π(Hd+1) given ˜P π(Hd) = P π(Hd)) directly following from (1) and (4). Therefore we can conclude
that ∀d

˜P π(Hd) = P π(Hd).

3

Since Lemma 4 already established that ∀Hd
∀Hd

˜P π
Kd

(Hd)

˜P π
Kd

(Hd)

p→ ˜P π(Hd), we directly have
p→ P π(Hd),

thus proving the result.

The proof depends on the following lemma:

Lemma 4. The full-history RS-BA-POMDP rollout distribution converges in probability to the following quantity:

∀Hd

˜P π
Kd

(Hd)

p→ b0(s0)

π(at−1|ht−0)

B(χsa(H0))
B(χsa(Hd)

(5)

(cid:34) d(cid:89)

t=1

(cid:35)(cid:89)

(cid:104)s,a(cid:105)



Γ(α1)·...·Γ(αk) the normalization term of a Dirichlet distribution with parametric vector α.

with B(α) = Γ(α1+...·+αk)
Proof. Via the weak law of large numbers, we have that the empirical mean of a random variable converges in
probability to its expectation.

∀Hd

1
Kd

Hd=H (i)

d

Hd=H (i)

d

Kd(cid:88)

i=1

I(cid:110)
˜P π(cid:16)
(cid:88)

=

H (i)

d

(cid:111)(cid:21)

(cid:20)
I(cid:110)

(cid:111) p→ E
(cid:17) I(cid:110)

Hd=H (i)

d

H (i)
d

Hd=H (i)

d

(cid:111) = ˜P π (Hd)

(6)

(cid:111)(cid:21)

(cid:35)

This expectation can be rewritten as follows

(cid:20)
I(cid:110)

E

˜P π(cid:16)
ˆ
ˆ (cid:34)

Hd| ˙D

Dir( ˙D| ˙χ)d ˙D

(cid:17)

d(cid:89)
(cid:34) d(cid:89)

t=1

˜P π(Hd) =

=

where ˜P π(Hd) denotes the (true, non-empirical) probability that the RS-BA-POMDP rollout generates full history
Hd. This is an expectation over the root sampled model

˙D:

Cons(H0, Hd)

˙D(stzt|st−1, at−1)π(at−1|ht−1)

Dir( ˙D| ˙χ)d ˙D

(cid:35)(cid:32)ˆ (cid:34) d(cid:89)

(cid:35)

= Cons(H0, Hd)

π(at−1|ht−1)

˙D(stzt|st−1, at−1)

Dir( ˙D| ˙χ)d ˙D

(cid:33)

(7)

(8)

(9)

t=1

t=1

Where Cons(H0, Hd) is a term that indicates whether (takes value 1 if) Hd is consistent with the full history at the
root H0.1

1An earlier version of this proof [anonymous] contained a term b0(s0) instead of Cons(H0, Hd). This was incorrect since it failed to

recognize that this proof assumes H0 to be ﬁxed. See also the remark on page 2.

4

Now we can exploit the fact that only the Dirichlet for the transitions speciﬁed by Hd matter.

={reorder the transition probabilities: ∆sas(cid:48)z
ˆ

χ

(Hd)is the number of occurences of (s, a, s(cid:48), z)in Hd}

ˆ (cid:34) d(cid:89)

t=1

={split up the integral over one big vector into integrals over smaller vectors}
ˆ

. . .

˙Dst,zt

˙D(stzt|st−1, at−1)

χ

(Hd)

(cid:104)s,a(cid:105)

st−1,at−1

Dir( ˙D|χ0)d ˙D

Dir( ˙Dsa|χsa(H0))

 d ˙Ds1a1 . . . d ˙Ds|S|a|A|

(cid:35)
(cid:35)(cid:89)
(cid:89)
(cid:17)∆sas(cid:48) z
(cid:16) ˙Ds(cid:48)z
(cid:89)
(cid:89)
(cid:17)∆sas(cid:48) z
(cid:16) ˙Ds(cid:48)z
(cid:16) ˙Ds(cid:48)z
(cid:89)
(cid:89)
B( ˙χsa)
(cid:89)
(cid:16) ˙Ds(cid:48)z
(cid:16) ˙Ds(cid:48)z
(cid:17)∆sas(cid:48) z
(cid:89)
(cid:89)
B( ˙χsa)
(cid:89)
(cid:17)∆sas(cid:48) z
(cid:16) ˙Ds(cid:48)z
(cid:16) ˙Ds(cid:48)z
 d ˙Ds1a1 . . . d ˙Ds|S|a|A|
(cid:17)χsas(cid:48) z
(cid:16) ˙Ds(cid:48)z
(cid:89)

Dir( ˙Dsa|χsa(H0))

−1+∆sas(cid:48) z

B( ˙χsa)

B( ˙χsa)

(cid:104)s(cid:48),z(cid:105)

(cid:104)s(cid:48),z(cid:105)

(cid:104)s(cid:48),z(cid:105)

(cid:104)s(cid:48),z(cid:105)

(cid:104)s(cid:48),z(cid:105)

(cid:104)s(cid:48),z(cid:105)

(cid:104)s(cid:48),z(cid:105)

(cid:104)s,a(cid:105)

(cid:104)s,a(cid:105)

(Hd)

(Hd)

(Hd)

(Hd)

sa

sa

sa

sa

sa

sa

sa

sa

χ

χ

χ

χ

0

(cid:104)s,a(cid:105)

t=1

ˆ (cid:34) d(cid:89)
ˆ (cid:89)
ˆ (cid:89)
ˆ (cid:89)
ˆ (cid:89)
ˆ (cid:89)

(cid:104)s,a(cid:105)

(cid:104)s,a(cid:105)

(cid:104)s,a(cid:105)

(cid:104)s,a(cid:105)

(cid:104)s(cid:48),z(cid:105)

0

−1

 d ˙Ds1a1 . . . d ˙Ds|S|a|A|
(cid:17)χsas(cid:48) z
(cid:17)χsas(cid:48) z
(cid:17)χsas(cid:48) z

 d ˙Ds1a1 . . . d ˙Ds|S|a|A|
 d ˙Ds1a1 . . . d ˙Ds|S|a|A|
 d ˙Ds1a1 . . . d ˙Ds|S|a|A|

−1

−1

0

0

Now we reverse the order of integration and multiplication, which is possible since the diﬀerent s, a pairs over which
we integrate are disjoint.2 We obtain:

=

B(χsa(H0))

(cid:16) ˙Dsa(s(cid:48), z)

(cid:17)χsas(cid:48) z

0

ˆ (cid:89)

(cid:104)s(cid:48),z(cid:105)

+∆sas(cid:48) z

χ

(Hd)−1

d ˙Dsa

={since we integrate over the entire vector ˙Dsa, the integral equals 1/B(χsa(H0) + ∆sa

χ (Hd))}

. . .

. . .

. . .

. . .

ˆ

ˆ

ˆ

ˆ

=

=

=

=

. . .

(cid:104)s,a(cid:105)

(cid:89)
(cid:89)
(cid:89)

(cid:104)s,a(cid:105)

(cid:104)s,a(cid:105)

(10)

(11)

(12)

(13)

(14)

(15)

(16)

(17)

(18)

(19)

(20)

B(χsa(H0))

1

B(χsa(H0) + ∆sa

χ (Hd))

B(χsa(H0))
B(χsa(Hd))

˜P π(Hd) = Cons(H0, Hd)

(cid:34)d−1(cid:89)

t=0

=

Therefore

proving (5).

(cid:35)(cid:89)

(cid:104)s,a(cid:105)

 ,

B(χsa(H0))
B(χsa(Hd))

π(at|ht)
(cid:111)

(cid:88)

(cid:88)

2(cid:89)

a1∈A1

a2∈A2

i=1

ai =

(cid:110)

(cid:110)

(cid:111)
(cid:88)
 (cid:88)

a2∈A2
a(1)
2 + a(2)

a1

a1∈A1

(cid:88)
(cid:16)
 (cid:88)

1

a1∈A1

a2∈A2

=

(cid:17)
 =

1

2(cid:89)

(cid:16)
(cid:88)

a2

ai

i=1

ai∈Ai

5

2E.g, consider two sets A1 =

a(1)
1 , a(2)

1

and A2 =

a(1)
2 , a(2)

2 , a(3)

2

. Equation (16) is of the same form as

a1a2 = a(1)

1 a(1)

2 + a(1)

1 a(2)

2 + a(1)

1 a(3)

2 + a(2)

1 a(1)

2 + a(2)

1 a(2)

2 + a(2)

1 a(3)

2

(cid:17)

(cid:16)

(cid:17)(cid:16)

(cid:17)

= a(1)

2 + a(3)

2

+ a(2)

a(1)
2 + a(2)

2 + a(3)

2

=

a(1)
1 + a(2)

1

a(1)
2 + a(2)

2 + a(3)

2

References
Morris H. DeGroot. Optimal Statistical Decisions. Wiley-Interscience, April 2004.

6

