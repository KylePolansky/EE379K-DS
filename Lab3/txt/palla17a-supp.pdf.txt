Supplementary material

Introduction

1
This paper contains supplementary material to the main paper “A birth-death
process for feature allocation”.

2 Proofs
This section provides the proofs to theorems and equations provided in the
paper. Before providing the actual proofs, we present some useful propositions
that will help the reader understand the material that follows.

Poisson approximation to the Binomial distribution

Proposition 1. (Poisson approximation) Suppose Xn is a random variable fol-
lowing the Binomial distribution with number of trials as n and success ratio as
pn, denoted by Binomial(n, pn), such that n → ∞, pn → 0 and npn → λ > 0.
Then, for k = 0, 1, 2, . . . ,

In other words,

Xn → Poisson(λ),

in distribution.

P (Xn = k) → λke−λ

k!

,

as n → ∞.

Normal approximation to the Poisson distribution

Proposition 2. (Normal approximation) Suppose X is a random variable fol-
lowing the Poisson distribution with mean λ such that X ∼ Poisson(λ). Then,
for λ → ∞,

,

in distribution.

1

X → N(cid:16)

λ,

λ

(cid:17)

√

√

,where

λ is the standard deviation.

Dirac approximation to Normal distribution

Proposition 3. (Dirac approximation) Suppose X is a random variable fol-
lowing the Normal distribution with mean 1 and standard deviation σ = 1
λ such
that X ∼ N (1, σ). Then, for σ → 0,
X → δ(1),

in distribution.

In other words,

X → 1,

as σ → 0.

in the main paper:

Proof of proposition (1)
Assume that z(t) is a realization of the BDFP (Z(t)) over the ﬁnite interval
[0, T ], T > 0 and we write (z(t))0≤t≤T . With probability one the sample path
(z(t))0≤t≤T will only contain a ﬁnite number of jump events, each of which is
either a birth or a death event. We write B and Q to denote the set of the
features created or turned oﬀ by birth or death events respectively. We denote
as t1, . . . , tJ the times when the chain jumps, where J = |B| + |Q|.

The probability of observing a sample (z(t))0≤t≤T can be written as the
product of three factors; the probability of the initial state, the probability of
each jump (event) and the probability of the interarrival times between the
events. More speciﬁcally,

• Probability of the initial state at time t = 0:

At time t = 0, the feature allocation z(0) follows an IBP distribution.

Kz(0)+(cid:89)

P(z(0)) =

(cid:81)Hz(0)

αKz(0)+

h=1 Kh!

exp (−αHN )

(N − mk)!(mk − 1)!

(1)

k=1

N !

where Kz(0)+ and Hz(0) are the number of total and distinct nonzero
features correspondingly in state z(0) and mk is the number of objects
that possess feature k.

• Probability of all the interarrival times between events:

The probability that the chain will not jump in the interval [tl, tl+1] can
be derived as follows, by dividing the time from tl to tl+1 into k intervals
of duration tl+1−tl

, and letting k → ∞:

(cid:18)no jump

k

(cid:19)

P

in [tl, tl+1)

= lim
k→∞

(cid:18)

(cid:32)

i=0

k−1(cid:89)
(cid:32)
(cid:16) −

1 − q

k−1(cid:88)
(cid:90) tl+1

i=0

lim
k→∞

tl +

(cid:18)

(cid:17)

i(tl+1 − tl)

k

(cid:18)

1 − q

log

tl +

= exp

= exp

(cid:19) (tl+1 − tl)

k
i(tl+1 − tl)

(cid:33)
(cid:19) (tl+1 − tl)

(cid:19)(cid:33)

k

k

q(t)dt

= exp(−qz(tl)(tl+1 − tl))

tl

2

where we wrote q(t) = qz(tl) and we also considered that the rate qz(tl)
is constant in the interval tl+1 − tl. The interarrival times in [0, T ] are
independent of each other, so the probability of all the interarrival times
is a product as follows:

(cid:18)no jump

(cid:19)

P

in [tl, tl+1)

=

|B|+|D|(cid:89)

l=0

(cid:17)

q(τ )dτ

(cid:90) tl+1
(cid:17)

tl

l=0

|B|+|Q|(cid:89)
(cid:16) −
(cid:16) −

exp

(cid:16) −
(cid:90) T
|B|+|Q|(cid:88)

0

= exp

= exp

q(t)dt

(tl+1 − tl)qz(tl)

(cid:17)

(2)

where t0 = 0 and t|B|+|Q|+1 = T . Note that the above product includes
the probability of not transiting from the last jump time to T .

l=0

• Probability of all the events taking place during the interval [0, T ]:

The probability density that the chain jumps at time tl to a new state
is qz(tl−)z(tl), where tl− denotes an inﬁnitesimal time prior to tl. The
probability density of all the events taking place is:

|B|+|Q|(cid:89)

qz(tl−)z(tl)

(3)

l=1

To compute the probability of the path we take the product of Equations (1),(2)
and (3).

in the main paper:

Proof of corollary (1)
The IBP is deﬁned as the limit of the corresponding distribution over matrices
with M columns as M → ∞. The ﬁnite model, which gives the IBP in the limit
as M → ∞ (Griﬃths & Ghahramani, 2011) is
ωk|α ∼ Beta

(cid:16) α

(cid:17)

,
Znk|ωk ∼ Bernoulli(ωk)

M

(cid:90)

(cid:90)

ρ(dω)

[0,1]

µ0(dx)

X

D

(4)

, K → ∞ (5)

=

K
D

E[Nf ] =

We introduced K → ∞ because (cid:82)

νt(dωdx) =

[0,1]⊗X

of features at any t ∈ T is given by K
and D = R

[0,1] ρ(dω) = ∞. Since the expected number
D for K → ∞
D , the substitution of M = K
α , results in the generative model in Equation (12) in the main paper.

for k = 1, . . . , M and n = 1, . . . , N.

The expected number of features at any time t ∈ T is

(cid:90)

3

in the main paper:

b ∈ δ,(cid:80)N

Proof of proposition (4)
For what follows, we use NB(δ) to denote the number of birth events (actual
appearance of a feature with at least one member in it) that is NB(δ) = |{B =
{fk} : tk
n=1 znk(t) ≥ 1}| and NF (δ) to denote the number of feature
events tb at time interval δ, that is NF (δ) = |{F = {fk} : tk
b ∈ δ}|. Note here
the diﬀerence between a birth and a feature event. Not all feature events are
birth events.

distribution with intensity ν(δ) =(cid:82)

• The number of features present in a time interval [t, t+δ] follows a Poisson
0 dtω =
Kδ for K → ∞, i.e. Nf (δ) ∼ Poisson(Kδ). Since K → ∞, use of Propo-
sition (2) and Proposition (3) result in

X µ0(dx)(cid:82) t+δ

[0,1] ρ(dω)(cid:82)

(cid:82) ∞

dtb

t

(6)
• The probability of activating a feature fk given that it is created in the

Nf (δ) → Kδ, as K → ∞.

time interval δ is:

π =

P (NB(δ) = 1|NF (δ) = 1)
P (NB(δ) = 1, ωk|NF (δ) = 1)p(ωk)dωk

=

where, based on Equation (12) in the main paper, ωk ∼ Beta(cid:0) R

over, P (NB(δ) = 1, ωk|NF (δ) = 1) is the probability that at time interval
δ at least one object out of N grants membership of the feature given that
only one feature is created at the interval δ, i.e
P (NB(δ) = 1, ωk|NF (δ) = 1) = P (n ≥ 1) = 1 − P (n < 1) = 1 − (1 − ωk)N
where we used the fact that P (n ≤ m) is given by the Binomial cumulative
k(1 −
ωk)N−i. Here (cid:98)m(cid:99) is the “ﬂoor” under m, i.e. the greatest integer less
than or equal to m. Finally, Equation (7) becomes

distribution function form F(m; N, ωk) = P (n ≤ m) =(cid:80)(cid:98)m(cid:99)

(cid:1)ωi

(cid:0)N

i=0

i

(7)

K , 1(cid:1). More-

In order to compute the birth rate at [t, t+δ] we need to compute the probability
of having one only birth event at the time interval δ, i.e.

P (NB(δ) = 1) =

P (NB(δ) = 1|NF (δ))P (NF (δ))dNF .

(9)

4

(cid:90)

(cid:90)

(cid:90)

π =

(1 − (1 − ωk)N )Beta
K −1

(cid:90) (1 − ωN

R

k )N ω
k
K , 1)

= 1 −

B( R
K , N + 1)
B( R
where B(·) is the Beta function.

= 1 − B( R

K , 1)

(cid:18)

(cid:19)

ωk;

R
K

, 1

dωk

dωk

(8)

The number of actual births given the number of features present follows a
Binomial distribution, i.e. NB(δ)|NF (δ) ∼ Binomial(NB(δ); NF (δ), π). Since
NF (δ) → Kδ → ∞ and π → 0 as K → ∞, following Proposition (1), this
distribution can be approximated by

NB(δ)|NF (δ) ∼ Poisson(NB(δ); πKδ), K → ∞

(10)

Based on Equation (10), Equation (9) now becomes

P (NB(δ) = 1|NF (δ))P (NF (δ))dNF

P (NB(δ) = 1) =

=

(11)
We need to consider the case when K → ∞ and we will focus on the product

=

Poisson(1; πKδ)
πKδe−πKδ

(cid:90)

(cid:16)

term πKδ.

K→∞ πKδ
lim

=

β= 1
K=

L’Hopital

=

β→0
=

=

K→∞ δK
lim

1 − B( R

K , N + 1)
B( R

K , 1)

B(βR, 1) − B(βR, N + 1)

lim
β→0

δ(
RB(βR, 1)(ψ(βR) − ψ(βR + 1)) − RB(βR, N + 1)(ψ(βR) − ψ(βR + N + 1))

βB(βR, 1)

)

lim
β→0
R(ψ(0) − ψ(1)) − R(ψ(0) − ψ(N + 1))

βRB(βR, 1)(ψ(βR) − ψ(βR + 1)) + B(βR, 1)

δ

1

δR(ψ(N + 1) − ψ(1)) = δR

1
n

= δRHN

(12)

(cid:17)

N(cid:88)

n=1

where we used the deﬁnition of the derivative of the Beta function dB(x,y)
dx =
B(x, y)(ψ(x) − ψ(x + y)). Considering again the probability in Equation (11)
and taking its limit as K → ∞ and δ → 0 we have

P (NB(δ) = 1) = δRHN e−δRHN δ→0= δRHN

(13)

If we divide the result by δ, then the resulting rate is equal to the overall birth
rate RHN in BDFP. The probability of only one feature out of the NF dying in
α equal to the death rate in BDF process.
the time interval δ is trivial NF (δ) R
We assumed that the probability of observing more than one events in δ → 0 is
negligible.

Proof of proposition (5)
In the ﬁnite model, at any t ∈ T, the constrained projection is computed as an
integral over the space deﬁned by the updated set of constraints t− tω < tb < t,
0 < tω < ∞ and 0 < tb < T which can be summarised as t − tω < tb < t and

in the main paper:

5

0 < tω < t. As such,

νt(dωdx) = ρ(dω)µ0(dx)

(cid:90) t
(cid:90) t
(cid:16) − te−Dt +

t−tω

0

g(dtb)β(dtω)
1 − e−Dt

(cid:17)

.

D

= ρ(dω)µ0(dx)

(14)
The exponential terms add a dependency on the index t ∈ T. This is a result
of constraining tb ∈ [0, T ]; the number of features present near the origin t = 0
is smaller than the number of features present later in time, since no features
are allowed to be born in tb < 0. This eﬀect diminishes as t (cid:29) 1
D since then
e−Dt → 0 and νt(dωdx) coincides with the restricted projection in the inﬁnite
case, i.e. νt(dωdx) = ρ(dω) µ0(dx)

present at any t ∈ T is (cid:82) 1

(cid:82)
X νt(dωdx) = K(−te−Dt + 1−e−Dt
D , the expected number of features at any t approaches K

In the same fashion, in the ﬁnite model, the expected number of features
D ). Again, for
t (cid:29) 1
D and K → ∞ in
the inﬁnite case. This is conﬁrmed in Figure 1(a). It takes some time until the
empirical mean number of features converges to the true expected value under
the inﬁnite model. In order to make sure that at any random time point in the
range considered the process has IBP marginals, we have to make sure that the
expected number of features at any time is equal to the true expected value,
that is K
R . The discussion above ensures this considering the process at
t (cid:29) 1
D , where the D is the death rate. In this case, we only allow data to live
at the time range where IBP marginals are satisﬁed.

D = Kα

D

0

3 Posterior Simulation
The BDF process is a continuous-time Markov process and use of the forward-
backward algorithm would facilitate inference. However, the exponentially large
space of feature allocations makes inference hard. However, the equivalent BEP
simpliﬁes inference by allowing the application of a simple, eﬃcient Gibbs sam-
pling approach. For what follows, we present the posterior distributions for the
linear-Gaussian likelihood model in Equation 15 and presented in Figure 3.
Likelihood term p(Y|S, tb, tw, A, σ) Given all the unknown parameters,
the computation of the likelihood is straightforward. More precisely, given the
values of S, tb and tw the computation of the feature allocation matrices Zt for
t = 1, . . . , L is deterministic, as presented earlier, and as such, observing the
data matrix Yt is equal in distribution to observing matrix Yt − ZtA for which

k=1 ztnkAkd = tnd ∼ N (0, σ) holds. Consequently,

ytnd −(cid:80)KT

p(Y|S, tb, tw, A, σ) =

=

p(Yt|S, tb, tw, A, σ)

(cid:32)
ytnd − KT(cid:88)

(cid:33)

ztnkAkd; 0, σ

(15)

N

L(cid:89)
L(cid:89)

t=1

N(cid:89)

D(cid:89)

t=1

n

d=1

k=1

6

t
n
e
s
e
r
p

s
e
r
u
t
a
e
f

f
o

r
e
b
m
u
n

time

Figure 1: Traceplot and running mean plot of number of features present as
a function of time for a sample from the ﬁnite BEP model. The plots were
produced using a dataset of N = 3 and 5000 feature allocation matrices were
drawn from the process over a period of [0, 100]. The number of features present
is plotted over time. The values of hyperparameters chosen is α = 2, R = 20
R = 100 as indicated
and K = 1000. Note the convergence to the mean µ = Kα
by the red traceplot; it takes some time until the number of features present
converge to the expected mean.

7

0102030405060708090100020406080100120140Sample the parameters α, R, tb and tw. Due to lack of conjugacy, the
posterior of these parameters has no closed form and exact computation is not
feasible. For that reason, we used slice sampling (Neal, 2003). For completeness,
we provide the conditional posteriors.

p(α|tk

w, R) ∝ p(tw|α, R)p(α) = Gamma(α; κα, θα)

(cid:18)

(cid:19)

k=1

tk
w;

R
α

Exponential

p(R|ω, tw, α) ∝ p(ω|R, α)p(tw|R, α)p(R)

KT(cid:89)
(cid:18)
KT(cid:89)
(cid:32)
k
ytnd −(cid:88)
D(cid:89)
b , tw, A, σ) ∝ p(Y|S, tb, tw, A, σ)p(tk
b )
N
(cid:32)
(cid:19) L(cid:89)
ytnd −(cid:88)
D(cid:89)
N(cid:89)
w , tb, A, α, σ) ∝ p(Y|S, tw, tb, A, σ)p(tk
w|R, α)
R
α

L(cid:89)
(cid:18)

= Gamma(R; κR, θR)

= Exponential

= U(tk

N(cid:89)

b ; 0, T )

(cid:19)

Beta

R
K

tk
w;

t=1

n

ωk;

N

d=1

k

t=1

n=1

d=1

k

ztnkAkd; 0, σ

(cid:18)

(cid:33)

, 1

Exponential

tk
w;

(cid:19)

R
α

(cid:33)

ztnkAkd; 0, σ

p(tk

b|Y, S, t¬k

p(tk

w|Y, R, S, t¬k

Sample the noise standard deviation σ We put a Gamma prior over
the precision, that is τ ∼ Gamma(a, β), where a, β are the shape and rate
hyperparameters. The posterior over the precision is then,

p(τ|Y, S, tb, tw, A) ∝ p(Y|S, tb, tw, A, τ)p(τ|a, β)

The Gamma prior is conjugate to the Gaussian likelihood and thus, the condi-
tional posterior is a Gamma distribution too. More precisely, the posterior has
the form

(cid:32)

L(cid:88)

N(cid:88)

D(cid:88)

(cid:33)

2
tnd
2

, β +

p(τ|Y, S, tb, tw, A) = Gamma

a +

LN D

.

2

t=1

d=1

n=1

k=1 ztnkAkd. Finally, we compute the standard deviation

where tnd = ytnd −(cid:80)KT
K , 1(cid:1) is conjugate to the Binomial likelihood p(S|ω) =(cid:81)KT
Beta(cid:0) R
resulting in the conditional posterior distribution wk ∼ Beta(cid:0)|mk| + R

as σ = 1√
τ
Sample the weights ω Gibbs sampling is simple since the Beta prior wk ∼

1, . . . , KT where |mk| is the number of objects that have feature fk in their po-
tential, i.e. Snk = 1.

k=1 Binomial(mk; N, ωk)

K , N − |mk| + 1(cid:1), i =

p(ωk|S) ∝ p(S(:, k)|ωk)p(ωk)

∝ Binomial(mk; N, ωk)Beta

8

(cid:19)

, 1

(cid:18) R

K

(cid:18) R

K

∝ Beta

(cid:19)

,

+ mk, 1 + N − mk

for k = 1, . . . , KT .

Sample the feature potential matrix S The prior over the feature poten-
tial matrix S is a product of Bernoulli distributed parameters. More precisely,

N(cid:89)

KT(cid:89)

N(cid:89)

KT(cid:89)

p(S|ω) =

p(Snk|ω) =

Bernoulli(Snk; ωk)

n=1

k=1

n=1

k=1

The posterior over each matrix element Snk is given by

p(Snk|Y, ω, S¬nk, tb, tw, A, σ) ∝ p(Y|S, ω, tb, tw, A, σ)p(Snk|ω)

We only need to consider Snk ∈ {0, 1}, so we evaluate the right hand side for
Snk = 0 and Snk = 1, normalize, and sample Snk from the resulting Bernoulli
posterior.

3.1 Getting it right
To validate our sampling alorithm for BEP we follow the joint distribution
testing methodology of Geweke (2004). There are two ways to sample from
the joint distribution, P (Y, θ) over parameters θ = {R, ω, S, tb, tw, A, α, σ},
and data, Y deﬁned by a probabilistic model such as BEP. The ﬁrst we will
refer to as “marginal-conditional” sampling, shown in Algorithm 1. Both steps
here are straightforward: sampling from the prior followed by sampling from
the likelihood model. The second way, referred to as “successive-conditional”
sampling is shown in Algorithm 2, where Q represents a single (or multiple)
iteration(s) of our MCMC sampler. To validate our sampler we can then check,
either informally or using hypothesis tests, whether the samples drawn from the
joint P (Y, θ) in these two diﬀerent ways appear to have come from the same
distribution.
We apply this method to our sampler with just N = 10, D = 2 and |F| = 5,
two time points and all hyperparameters ﬁxed as follows: For the shape and scale
of α we set κα = 4, θα = 1, for the shape and rate of  we set α = 2, β = 2,
for the shape and scale of R we chose κR = 4, θR = 1 and ﬁnally for the mean
and standard deviation of A we set µA = 0, σA = 1.

We draw 80K samples using both the marginal-conditional and successive-
conditional procedures. We look at various characteristics of the samples, in-
cluding the number of features at every time point, the mean of the factor
loading matrix A.

The distribution of the number of features under the successive-conditional
sampler matches that under the marginal-conditional sampler almost perfectly
as shown in Figure 2. Both the histogram and the quantile-quantile plot show
the similarity of the two distributions, with the straight line in the later indi-
cating an almost perfect match. The deviation from a straight line in the upper

9

corner of the qq-plot is a result of there being fewer samples available to esti-
mate these quantiles accurately. Under the successive-conditional sampler the
average number of features is 0.85, 0.95 for the two locations while under the
marginal-conditional is 0.83, 0.96 respectively with standard deviations 0.91,
1.01 and 0.92, 1.02 respectively: a hypothesis test did not reject the null hy-
pothesis that the means of the two distributions are equal. While this cannot
completely guarantee correctness of the algorithm and code, 80K samples is a
large number for such a small model and thus provides strong evidence that our
algorithm is correct. Figure 3 provides the same evidence.

(a) First time location

(b) Second time location

Figure 2: Comparing the distribution of the number of features under the
marginal-conditional and successive conditional samplers. Figures in column (a)
show the empirical distribution over the number of features under the marginal-
conditional (ﬁrst row) and successive conditional (second row) for the ﬁrst time
location. Respectively for the second time location in column (b). Figure in the
last row show the qq-plots of the two empirical distributions for the two time
locations. The agreement of the two distributions is evidence for the correctness
of our MCMC sampler for the ﬁnite model.

4 Experiments
In the main paper, we presented results on real world dataset. To complete the
analysis, we provide here further experiments on synthetic dataset.

10

01234501234x 10401234501234x 1040123456024601234501234x 10401234501234x 10401234560246(a) α

(b) R

(c) σ

Figure 3: Comparing the distribution α, R and σ under the marginal-
conditional and successive conditional samplers. Figure shows the qq-plots of
the two empirical distributions. The agreement of the two distributions is evi-
dence for the correctness of our MCMC sampler for the ﬁnite model.

4.1 Synthetic dataset
We ﬁrst explored the ability of our model to recover underlying structure us-
ing synthetic data. We generated observations Yt from the BEP at 7 dis-
tinct time points, t = 1, 2, . . . , 7. More speciﬁcally, we assumed N = 20 data-
points and 4 features with birth times, tb = [0.1, 0.4, 0.8, 1.2] and life spans
tw = [0.8, 0.8, 0.8, 0.8]. We also assumed a potential matrix S and derived the
feature allocation matrices Zt as determined by the BEP and shown in Figure 4.
We used the four 6 × 6 images shown in Figure 4 (middle row, left) as features
and collected them to construct the feature loading matrix A of size 4 × 36.
Each row of this matrix corresponds to one of the 4 features. The synthetic
data at each time point is then generated by superimposing the images using
the linear Gaussian likelihood, i.e. Yt = ZtA + , where  is the noise term
which we take as Gaussian with standard deviation 0.5.

We ran both models, that is the BEP and a set of 7 independent IBP models
(one at each time location). To evaluate predictive performance, we held out
10% of the data (elements in each Yt). For inference, we ran the BEP sampler
derived in Section 3 for 1000 MCMC iterations, which appeared suﬃcient for
burnin. The total number of features is KT = 12, where we took K = 6 and T =
2. For the independent IBP model, we considered the same Gaussian likelihood

11

0204005101520250204005101520250204005101520253035Algorithm 1: Marginal condi-
tional

1: for m = 1 to M do
θ(m) ∼ P (θ)
2:
Y (m) ∼ P (Y |θ(m))
3:
4: end for

Algorithm 2: Successive condi-
tional

1: θ(1) ∼ P (θ)
2: Y (1) ∼ P (Y |θ(1))
3: for m = 2 to M do
θ(m) ∼ Q(θ|θ(m−1), Y (m−1))
4:
Y (m) ∼ P (Y |θ(m))
5:
6: end for

Train error
Test error
Train log likelihood
Test log likelihood

BEP

3.3934 ± 0.0714
3.4229 ± 0.1771
−3, 348 ± 9.7123

independent IBP
3.3428 ± 0.0738
4.7367 ± 0.4283
−3, 311 ± 33.8831
−381.7972 ± 4.4620 −605.6935 ± 68.9909

Table 1: Results for synthetic Gaussian superimposition data

as for the BEP but with independent A and noise variance at each location. The
quantitative results are presented in Table 4.1 where the likelihood and the error
are averaged over the last 200 MCMC samples. Figure 4 shows the solutions
found by the two models. The MCMC sample with the highest log probability
under the posterior was used. The BEP successfully ﬁnds the true features and
Z matrix, whereas the independent IBP model ﬁnds a solution where additional
features are used in all of the 7 locations (see second and third row of Figure
4). The clean solution provided by BEP shows that leveraging the dependence
among consecutive feature allocations greatly improves performance. Table 4.1
conﬁrms this quantitatively: the BEP model performs considerably better both
in terms of test error and likelihood. While the independent IBP looks good
in terms of training error/likelihood, the big diﬀerence in the performance in
terms of the test likelihood suggests this is overﬁtting.

Next, we explore the ability of BEP to recover latent features in a synthetic
time series network data. We hand-constructed a set of six square binary ma-
trices which encode the friendship links among N = 20 people evolving through
time, as shown in Figure 5. People form groups which determine the links and
non-links between them. As time passes, the partitioning of people changes:
new friendship links are created while others break. The closer in time two
snapshots are, the more similar we expect the related partitions to be. We ran
BEP using the network likelihood model in Equation (16) for 2000 MCMC iter-
ations keeping the last 400 samples for estimation and holding 10% of the data
out for prediction. For comparison, we used independent LFRM models at each
timepoint. The BEP model outperforms the independent LFRM in terms of
both test error and likelihood (Table 4.1) while, analogously to the linear Gaus-
sian setting, the independent LFRM seems to overﬁt yielding “better” values in

12

True Y

inferred Y for BEP

Inferred Y for IBP

True Features

Inferred Features for BEP Inferred Features for IBP

True Z

inferred Z for BEP

Inferred Z for IBP

Figure 4:
Linear Gaussian synthetic data experiment. Top row: the true
and reconstructed observations for one datapoint (n = 16th) at the 7 diﬀerent
time locations. Middle row: the true and inferred features. The inferred
features for BEP have been plotted along with a number on top indicating how
often they are used, that is the total number of time locations at which they
are active. Note that for the independent IBP model there are diﬀerent factor
loading matrices for each time location. Bottom row: true and inferred feature
allocation matrices for the diﬀerent time locations.

13

True datapoint 123456Inferred datapoint for BEP123456Inferred datapoint for static IBP123456333200000000Inferred Z for BEPt1123456789101112t2123456789101112t3123456789101112t4123456789101112t5123456789101112t6123456789101112t7123456789101112Inferred Z over covariate for static IBPt11234t21234t3123456789101112t4123456789101112t5123456789t612345t71234Figure 5: Synthetic network data.

BEP

Train error
Test error
Train log likelihood −209.2373 ± 5.8391 −203.8954 ± 13.7807
Test log likelihood

0.0688 ± 0.0148
0.0745 ± 0.0132
−9.2547 ± 1.7358

independent LFRM

0.0968 ± 0.0286
0.1211 ± 0.0194
−19.4331 ± 5.3122

Table 2: Results for synthetic link data

the train likelihood. Figures 6 and 7 oﬀer a qualitative overview of the solution.
Figure 6 shows the features found in the sample with the highest log probability
under the posterior. Both show ﬁgures the slow evolution of the feature alloca-
tion dictated by the BEP; once objects are allocated to a feature, the feature
membership remains the same until the feature dies. For instance, moving from
time location 1 to 2 explains the data by keeping feature 17 alive (with the same
10 members) and introducing features 12 and 15. The LFRM is overly ﬂexible
since it assigns objects to features at each location independently. As such, it
explains the observations at the second time location using two features created
independently from the ones in the previous location.

4.2 ChIP-seq dataset
shows genomic annotations, from ChromHMM (Ernst et al., 2011) for the region
we model in the main paper. The inferred solution for both the BEP and the
independent IBP’s are shown in Figure 9 and Figure 10 respectively. The BEP
model allows for a smooth evolution of the latent feature allocation inferred as
opposed to the independe IBP’s where the latent structure is explained with
rapid changing allocations and with considerable diﬀerences in the number of

14

01020304050Figure 6:
Inferred features using the BEP in he synthetic network data. Time
is denoted on the x-axis along with the 6 time locations (dasehd red line), i.e
[3 4 4.3 6 6.3 7]. The total number of features is |F| = 30. Red colour is used
to indicated the features that are alive for more than one location. The features
that cross the vertical grey line at each time location are the ones present at
that time.

15

012345678051015202530Figure 7: Inferred feature allocation matrices for the six locations (from left
to right) in the synthetic link dataset. First two rows: Feature allocation
matrices inferred by BEP. Last two rows: Feature allocation matrices inferred
by independent LFRM.

16

5101520253024681012141618205101520253024681012141618205101520253024681012141618205101520253024681012141618205101520253024681012141618205101520253024681012141618200.511.522.533.524681012141618200.511.522.533.524681012141618200.511.522.533.544.524681012141618200.511.522.524681012141618200.511.522.524681012141618200.50.60.70.80.911.11.21.31.41.52468101214161820features found at each location. As such, covariate dependence over the alloca-
tion is a better modelling approach.

Figure 8: ChIP-seq data: Chromatin states for the genomic region we model.
From the BEP reconstruction in Figure 5(b) we see the promoter region around
18kb-19kb with high H3K27ac, the transcribed region of the WASH7P gene from
8kb-18kb, and the repressive H3K29me3 and H3K9me3 marks further down-
stream, corresponding to polycomb repression and heterochromatin.

4.3 van de Bunt’s dataset
In van de Bunt et al. (1999), 32 university freshman students in a given discipline
at a Dutch university were surveyed at seven time points about who in their class
they considered as friends. Initially, i.e. t1, most of the students were unknown
to each other. The ﬁrst four time points are three weeks apart, whereas the last
three time points are six weeks apart as showin in Figure 11.

References
Ernst, Jason, Kheradpour, Pouya, Mikkelsen, Tarjei S, Shoresh, Noam, Ward,
Lucas D, Epstein, Charles B, Zhang, Xiaolan, Wang, Li, Issner, Robbyn,
Coyne, Michael, et al. Mapping and analysis of chromatin state dynamics in
nine human cell types. Nature, 473(7345):43–49, 2011. 14

Geweke, John. Getting it right: Joint distribution tests of posterior simulators.

Journal of the American Statistical Association, 99:799–804, 2004. 9

Griﬃths, Thomas L. and Ghahramani, Zoubin. The indian buﬀet process: An
introduction and review. Journal of Machine Learning Research, 12:1185–
1224, July 2011. 3

Neal, R M. Slice sampling. The Annals of Statistics, 31(3):705–741, 2003. 8

van de Bunt, Gerhard G, Van Duijn, Marijtje AJ, and Snijders, Tom AB. Friend-
ship networks through time: An actor-oriented dynamic statistical network
model. Computational & Mathematical Organization Theory, 5:167–192, 1999.
17

17

1100014000170002000023000260002900032000350003800041000440004700050000530005600059000WASH7PFAM138AFAM138FOR4F5p36.33chr1SampleAssayHMECchromatinstateHUVEC chromatin stateK562 chromatin stateHEPG2 chromatin stateH1ES chromatin stateGM12878chromatinstateNHEK chromatin stateNHLF chromatin stateHSMM chromatin stateRefSeq genesActive promoterWeak promoterInactive/poised promoterStrong enhancerWeak/poised enhancerInsulatorTranscriptional transitionWeak transcribedPolycomb repressedHeterochrom; low signalRepetitive/CNVFigure 9: ChIP-seq dataset: Inferred feature allocation matrices for the BEP
model. The allocation matrices for 50 locations (out of the 500) from left to
right are shown. Each pair of adjacent matrices correspond to locations with 10
bins distance, i.e. 103 base pairs.

18

2468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214246810121416182024681012142468101214161820246810121424681012141618202468101214Figure 10: ChIP-seq dataset: Inferred feature allocation matrices for the inde-
pendent IBP model. The allocation matrices for 50 locations (out of the 500)
from left to right are shown. Each pair of adjacent matrices correspond to
locations with 10 bins distance, i.e. 103 base pairs.

19

2468101214246810121412345678910112468101214246810121416246810121424681012141618246810121451015202524681012142468101214161820222468101214510152025246810121424681012141624681012145101520252468101214510152025303540455024681012145101520252468101214246810121416182022246810121451015202468101214246810121416182024681012145101520253024681012145101520253024681012141020304050607024681012141020304050607080902468101214102030405060702468101214102030405060246810121451015202524681012142468101214161820222468101214510152024681012145101520252468101214510152025302468101214510152025246810121451015202468101214510152025246810121451015202524681012142468101214161820246810121424681012141624681012142468101214161820246810121424681012141624681012145101520246810121424681012142468101214510152025246810121451015202524681012142468101214162468101214246810121416182468101214246810121416182022246810121424681012141618246810121424681012141618202468101214246810122468101214246810121416182022246810121451015202530246810121424681012141618202224681012142468101214162468101214123456789101124681012142468101214246810121424681012141618202468101214Figure 11: van de Bunt network data.

20

