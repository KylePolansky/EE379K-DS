Supplemental Material: Scaling Up Sparse Support Vector Machines by

Simultaneous Feature and Sample Reduction

Weizhong Zhang * 1 2 Bin Hong * 1 3 Wei Liu 2 Jieping Ye 3 Deng Cai 1 Xiaofei He 1 Jie Wang 3

1State Key Lab of CAD&CG, Zhejiang University, China

2 Tencent AI Lab, Shenzhen, China, 3 University of Michigan, USA

In this supplement, we ﬁrst present the detailed proofs of all the theorems in the main text and then report the rest experi-
ment results which are omitted in the experiment section due to the space limitation.
A. Proof for Theorem 1
Proof. of Theorem 1:
(i) : Let ¯X = (¯x1, ¯x2, ..., ¯xn) and z = 1 − ¯XT w, the primal problem (P∗) then is equivalent to

||w||2 + β||w||1 +

α
2

min

w∈Rp,z∈Rn
s.t. z = 1 − ¯XT w.

n(cid:88)

i=1

1
n

(cid:96)([z]i),

The Lagrangian then becomes

L(w, z, θ) =

α
2

||w||2 + β||w||1 +

1
n
||w||2 + β||w||1 − 1
n

(cid:123)(cid:122)

:=f1(w)

=

α
2

(cid:124)

n(cid:88)

i=1

(cid:96)([z]i) +

(cid:104)1 − ¯XT w − z, θ(cid:105)

(cid:104) ¯Xθ, w(cid:105)

(cid:125)

+

1
n

(cid:124)

(cid:96)([z]i) − 1
n

(cid:123)(cid:122)

:=f2(z)

(cid:104)1, θ(cid:105)

+

1
n

(cid:104)z, θ(cid:105)

(cid:125)

1
n

n(cid:88)

i=1

We ﬁrst consider the subproblem minw L(w, z, θ):

0 ∈ ∂wL(w, z, θ) = ∂wf1(w) = αw − 1
n
¯Xθ ∈ αw + β∂||w||1 ⇒ w =
1
1
¯Xθ)
n
n

Sβ(

1
α

¯Xθ + β∂||w||1 ⇔

By substituting (19) into f1(w), we get

f1(w) =

α
2

||w||2 + β||w||1 − (cid:104)αw + β∂||w||1, w(cid:105) = − α
2

||w||2 = − 1
2α

||Sβ(

1
n

¯Xθ)||2.

Then, we consider the problem minz L(w, z, θ):



1

− 1
n[θ]i,
γn[z]i − 1
n − 1
n[θ]i,

1

n[θ]i,

if [z]i < 0,

if 0 ≤ [z]i ≤ γ,
if [z]i > γ.

0 =∇[z]i L(w, z, θ) = ∇[z]if2(z) =

 0,

1,

1

γ [z]i,

if [z]i < 0,

if 0 ≤ [z]i ≤ γ,
if [z]i > γ.

⇒[θ]i =

Thus, we have

(17)

(18)

(19)

(20)

(21)

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

(cid:26) − γ

f2(z) =

2n||θ||2,
−∞,

if [θ]i ∈ [0, 1],∀i ∈ [n],

otherwise .

Combining Eq. (17), Eq. (20) and Eq. (22), we obtain the dual problem:

min

θ∈[0,1]n

1
2α

||Sβ(

1
n

¯Xθ)||2 +

γ
2n

||θ||2 − 1
n

(cid:104)1, θ(cid:105)

(ii) : From Eq. (19) and Eq. (21), we get the KKT conditions:

Sβ(

¯XT θ∗(α, β))

1
n

w∗(α, β) =

1
α
[θ∗(α, β)]i =



0,

1

γ (1 − (cid:104)¯xi, w∗(α, β)(cid:105)),

1,

if 0 ≤ 1 − (cid:104)¯xi, w∗(α, β)(cid:105) ≤ γ,

if 1 − (cid:104)¯xi, w∗(α, β)(cid:105) < 0,
if 1 − (cid:104)¯xi, w∗(α, β)(cid:105) > γ.

i = 1, ..., n.

(22)

(23)

The proof is complete.
B. Proof for Lemma 1
Proof. of Theorem 1:
1) It is the conclusion of the analysis above.
2) After feature screening, the primal problem (P∗) is scaled into:

α
2

min

|| ˜w||2 + β|| ˜w||1 +

1
n
Thus, we can easily derive out the dual problem of (scaled-P ∗-1):
n ˆF c[ ¯X]˜θ)||2 +

˜D(˜θ; α, β) =

||Sβ(

˜w∈R| ˆF c|

1
2α

min

1

˜θ∈[0,α]n

n(cid:88)

i=1

(cid:96)(1 − (cid:104)[¯xi] ˆF c, ˜w(cid:105)),

(scaled-P ∗-1)

γ
2n

||˜θ||2 − 1
n

(cid:104)1, ˜θ(cid:105).

(scaled-D∗-1)

and also the KKT conditions:
Sβ(

˜w∗(α, β) =

1
α
[˜θ∗(α, β)]i =



1

n ˆF c[ ¯X]˜θ∗(α, β))
γ (1 − (cid:104)[¯xi] ˆF c, ˜w∗(α, β)),

0,

1

1,

if 1 − (cid:104)[¯xi] ˆF c, ˜w∗(α, β)(cid:105) < 0,
if 0 ≤ 1 − (cid:104)[¯xi] ˆF c , ˜w∗(α, β) ≤ γ,
if 1 − (cid:104)[¯xi] ˆF c, ˜w∗(α, β) > γ,

(scaled-KKT-1)

(scaled-KKT-2)

Then, it is obvious that ˜w∗(α, β) = [w∗(α, β)] ˆF c, since essentially, problem (scaled-P ∗-1) can be derived by substituting
0 to the weights for the eliminated features in problem (P∗) and optimize over the rest weights.
Since the solutions w∗(α, β) and θ∗(α, β) satisfy the conditions KKT-1 and KKT-2 and (cid:104)[¯xi] ˆF c, ˜w∗(α, β)(cid:105) =
(cid:104)¯xi, w∗(α, β)(cid:105) for all i , we know ˜w∗(α, β) and θ∗(α, β) satisfy the conditions scaled-KKT-1 and scaled-KKT-2. So
they are the solutions of problems (scaled-P ∗-1) and (scaled-D∗-1). Thus, due to the uniqueness of the solution of prob-
lem (scaled-D∗-1), we have

θ∗(α, β) = ˜θ∗(α, β)

(24)
From 1) we have, [˜θ∗(α, β)] ˆRc = 0 and [˜θ∗(α, β)] ˆLc = 1. Therefore, from the dual problem (scaled-D∗), we can see that
[˜θ∗(C, α)] ˆDc can be recovered from the following problem:
1
n

||ˆθ||2 − 1
n

ˆG21)||2 +

(cid:104)1, ˆθ(cid:105),

||Sβ(

1
2α

γ
2n

ˆθ +

ˆG1

min

1
n

ˆθ∈[0,1]| ˆDc|

Since [˜θ∗(α, β)] ˆDc = [θ∗(α, β)] ˆDc, the proof is therefore completed.

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

C. Proof for Lemma 2
Proof. Due to the α-strong convexity of the objective P (w; α, β), we have

P (w∗(α0, β0); α, β0) ≥ P (w∗(α, β0); α, β0) +

||w∗(α0, β0) − w∗(α, β0)||2

α
2

P (w∗(α, β0); α0, β0) ≥ P (w∗(α0, β0); α0, β0) +

||w∗(α0, β0) − w∗(α, β0)||2

α0
2

which are equivalent to

α
2

≥ α
2

||w∗(α0, β0)||2 + β0||w∗(α0, β0)||1 +

||w∗(α, β0)||2 + β0||w∗(α, β0)||1 +

1
n

(cid:96)(1 − (cid:104)¯xi, w∗(α0, β0)(cid:105))

(cid:96)(1 − (cid:104)¯xi, w∗(α, β0)(cid:105))

i=1

1
n

n(cid:88)
n(cid:88)
n(cid:88)
n(cid:88)

i=1

i=1

1
n

(cid:96)(1 − (cid:104)¯xi, w∗(α, β0)(cid:105))

(cid:96)(1 − (cid:104)¯xi, w∗(α0, β0)(cid:105))

i=1

α0 + α

2

||w∗(α0, β0) − w∗(α, β0)||2

||w∗(α0, β0) − w∗(α, β0)||2
α
2
||w∗(α, β0)||2 + β0||w∗(α, β0)||1 +

1
n
||w∗(α0, β0)||2 + β0||w∗(α0, β0)||1 +

+

α0
2

≥ α0
2

+

α0
2

||w∗(α0, β0) − w∗(α, β0)||2

Adding the above two inequalities together, we get

α − α0

2

||w∗(α0, β0)||2 ≥ α − α0

||w∗(α, β0)||2 +
w∗(α0, β0)||2 ≤ (α − α0)2

2

4α2

⇒||w∗(α, β0) − α0 + α
2α

||w∗(α0, β0)||2

Substitute the prior that [w∗(α, β0)] ˆF = 0 into (25), we get
||[w∗(α, β0)] ˆF c − α0 + α
2α
≤ (α − α0)2

[w∗(α0, β0)] ˆF c||2

||w∗(α0, β0)||2 − (α0 + α)2

||[w∗(α0, β0)] ˆF||2.

4α2

4α2

The proof is complete.
D. Proof for Lemma 3
Proof. Firstly, we need to extend the deﬁnition of D(θ; α, β) to Rn:

(cid:26) D(θ; α, β),

+∞,

if θ ∈ [0, 1]n,
otherwise

˜D(θ; α, β) =

Due to the strong convexity of objective ˜D(θ; α, β), we have

˜D(θ∗(α0, β0), α, β0) ≥ ˜D(θ∗(α, β0), α, β0) +
γ
2n
˜D(θ∗(α, β0), α0, β0) ≥ ˜D(θ∗(α0, β0), α0, β0) +

||θ∗(α0, β0) − θ∗(α, β0)||2,
||θ∗(α0, β0) − θ∗(α, β0)||2.
γ
2n

Since θ∗(α0, β0), θ∗(α, β0) ∈ [0, 1]n, the above inequalities are equivalent to

(25)

(26)

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

1
2α
≥ 1
2α

+

1
2α0
≥ 1
2α0

1
n
1
n

¯XT θ∗(α0, β0))||2 +
¯XT θ∗(α, β0))||2 +

||Sβ0(
||θ∗(α0, β0)||2 − 1
(cid:104)1, θ∗(α0, β0)(cid:105)
γ
n
2n
||Sβ0(
||θ∗(α, β0)||2 − 1
(cid:104)1, θ∗(α, β0)(cid:105)
γ
2n
n
||θ∗(α0, β0) − θ∗(α, β0)||2,
γ
2n
||θ∗(α, β0)||2 − 1
¯XT θ∗(α, β0))||2 +
||Sβ0 (
γ
2n
n
||Sβ0 (
¯XT θ∗(α0, β0))||2 +
||θ∗(α0, β0)||2 − 1
γ
n
2n

(cid:104)1, θ∗(α, β0)(cid:105)
(cid:104)1, θ∗(α0, β0)(cid:105) +

1
n
1
n

||θ∗(α0, β0) − θ∗(α, β0)||2.

γ
2n

Adding the above two inequalities, we get

γ(α − α0)
≥ γ(α − α0)

2n

2n

That is equivalent to

||θ∗(α0, β0)||2 − α − α0
||θ∗(α, β0)||2 − α − α0

n

(cid:104)1, θ∗(α0, β0)
(cid:104)1, θ∗(α, β0)(cid:105) +

n

γ(α0 + α)

2n

||θ∗(α0, β0) − θ∗(α, β0)||2

||θ∗(α, β0)||2 − (cid:104) α − α0
≤ − α0
α

1 +
||θ∗(α0, β0)||2 − α − α0

γα

γα

α0 + α

θ∗(α0, β0), θ∗(α, β0)(cid:105)

α
(cid:104)1, θ∗(α0, β0)(cid:105)

That is

||θ∗(α, β0) − (

α − α0
2γα

1 +

α0 + α

2α

θ∗(α0, β0))||2 ≤ (

α − α0
2α

)2||θ∗(α0, β0) − 1
γ

1||2

Substitute the priors that [θ∗(α, β0)] ˆR = 0 and [θ∗(α, β0)] ˆL = 1 into (28), we have

α − α0
2γα

||[θ∗(α, β0)] ˆDc − (
α − α0
≤(
2α
− || α − α0

)2||θ∗(α0, β0) − 1
γ
[θ∗(α0, β0)] ˆR||2.

α0 + α

1 +

2γα

2α

α0 + α

[θ∗(α0, β0)] ˆDc)||2

2α

1 +
1||2 − || (2γ − 1)α + α0

2γα

1 − α0 + α

2α

[θ∗(α0, β0)] ˆL||2

The proof is complete.
E. Proof for Lemma 4
Before the proof of Lemma 4, we should prove that the optimization problem in (1) is equivalent to

, i ∈ ˆF c.

To avoid notational confusion, we denote the feasible region Θ in (1) as ˜Θ. Then,

n

(cid:27)
|(cid:104)[¯xi] ˆDc , θ(cid:105) + (cid:104)[¯xi] ˆL, 1(cid:105)|

(cid:26) 1
(cid:12)(cid:12)(cid:12)(cid:12)(cid:27)
(cid:12)(cid:12)[¯xi] ˆDc[θ] ˆDc + [¯xi] ˆL[θ] ˆL + [¯xi] ˆR[θ] ˆR

(cid:27)
|(cid:104)[¯xi] ˆDc, [θ] ˆDc(cid:105) + (cid:104)[¯xi] ˆL, 1(cid:105)|

(cid:12)(cid:12)¯xiθ(cid:12)(cid:12)(cid:27)

(cid:26) 1

= max
θ∈Θ

[ ¯Xθ]i

n

(cid:12)(cid:12)(cid:27)

= si(α, β0).

si(α, β0) = max
θ∈Θ

(cid:26)(cid:12)(cid:12)(cid:12)(cid:12) 1
(cid:26) 1
(cid:26) 1

n

n

n

max
θ∈ ˜Θ

= max
θ∈ ˜Θ

= max
θ∈Θ

(27)

(28)

(29)

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

The last equation holds since [θ] ˆL = 1, [θ] ˆR = 0 and [θ ˆDc] ∈ Θ.
Proof. of Lemma 4:

si(α, β0) = max

θ∈B(c,r)

{ 1
n

|(cid:104)[¯xi] ˆDc, θ(cid:105) + (cid:104)[¯xi] ˆL, 1(cid:105)|}.

= max

{ 1
n

|(cid:104)[¯xi] ˆDc, c(cid:105) + (cid:104)[¯xi] ˆL, 1(cid:105) + (cid:104)[¯xi] ˆDc , η(cid:105)|}

(cid:0)|(cid:104)[¯xi] ˆDc , c(cid:105) + (cid:104)[¯xi] ˆL, 1| + (cid:107)[¯xi] ˆDc(cid:107)r(cid:1)

η∈B(0,r)
1
n

=

The last equality holds since −(cid:107)[¯xi] ˆDc(cid:107)r ≤ (cid:104)[¯xi] ˆDc, η(cid:105)| ≤ (cid:107)[¯xi] ˆDc(cid:107)r. The proof is complete.
F. Proof for Theorem 4
Proof. (1) It can be obtained from the the rule (R1).
(2) It is from the deﬁnition of ˆF.

G. Proof for Lemma 5
Firstly, we need to point out that the optimization problems in (2) and (3) are equivalent to the problems:

ui(α, β0) = max

li(α, β0) = min

w∈W{1 − (cid:104)[¯xi] ˆF c, w(cid:105)}, i ∈ ˆDc,
w∈W{1 − (cid:104)[¯xi] ˆF c , w(cid:105)}, i ∈ ˆDc

(30)

(31)

They follow from the fact that [w] ˆF c ∈ W and

{1 − (cid:104)w, ¯xi(cid:105)}
={1 − (cid:104)[w] ˆF c , [¯xi] ˆF c(cid:105) − (cid:104)[w] ˆF , [¯xi] ˆF(cid:105)}
={1 − (cid:104)[w] ˆF c , [¯xi] ˆF c(cid:105)} (since [w] ˆF = 0).

Proof. of Lemma 5:

ui(α, β0) = max

w∈B(c,r)

{1 − (cid:104)[¯xi] ˆF c, w(cid:105)}
{1 − (cid:104)[¯xi] ˆF c , c(cid:105) − (cid:104)[¯xi] ˆF c, η(cid:105)}
{−(cid:104)[¯xi] ˆF c , η(cid:105)}

= max
η∈B(0,r)
=1 − (cid:104)[¯xi] ˆF c, c(cid:105) + max
η∈B(0,r)
=1 − (cid:104)[¯xi] ˆF c, c(cid:105) + (cid:107)[¯xi] ˆF c(cid:107)r

li(α, β0) = min

w∈B(c,r)

{1 − (cid:104)[¯xi] ˆF c, w(cid:105)}
{1 − (cid:104)[¯xi] ˆF c, c(cid:105) − (cid:104)[¯xi] ˆF c, η(cid:105)}
{−(cid:104)[¯xi] ˆF c, η(cid:105)}

= min
η∈B(0,r)
=1 − (cid:104)[¯xi] ˆF c , c(cid:105) + min
η∈B(0,r)
=1 − (cid:104)[¯xi] ˆF c , c(cid:105) − (cid:107)[¯xi] ˆF c(cid:107)r

The proof is complete.
H. Proof for Theorem 5
Proof. (1) It can be obtained from the the rule (R2).

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

(2) It is from the deﬁnitions of ˆR and ˆL.
I. Proof for Theorem 2
Proof. of Theorem 2:
We prove this theorem by verifying that the solutions w∗(α, β) = 0 and θ∗(α, β) = 1 satisfy the conditions KKT-1 and
KKT-2.
Firstly, since β ≥ βmax = || 1
¯X1) = 0. Thus w∗(α, β) = 0 and θ∗(α, β) = 1 satisfy the condition
KKT-1.
Then, for all i ∈ [n], we have

¯X1||∞, we have Sβ( 1

n

n

1 − (cid:104)¯xi, w∗(α, β)(cid:105) = 1 − 0 > γ.

Thus w∗(α, β) = 0 and θ∗(α, β) = 1 satisfy the condition KKT-2. Hence, they are the solutions for the primal problem
(P∗) and the dual problem (D∗), respectively.
J. Proof for Theorem 3
Proof. of Theorem 3:
Similar with the proof of Theorem 2, we prove this theorem by verifying that the solutions w∗(α, β) = 1
and θ∗(α, β) = 1 satisfy the conditions KKT-1 and KKT-2.
1. Case 1: αmax(β) ≤ 0. Then for all α > 0, we have
{1 − (cid:104)¯xi, w∗(α, β)(cid:105)}

¯Xθ∗(α, β))

αSβ( 1

n

min
i∈[n]

(cid:104)¯xi,Sβ(

{1 − 1
α
(cid:104)¯xi,Sβ(

max
i∈[n]

1
n
¯X1)(cid:105) = 1 − (1 − γ)
1
n

¯Xθ∗(α, β))(cid:105)} = min
i∈[n]
1
α

αmax(β)

{1 − 1
α

(cid:104)¯xi,Sβ(

¯X1)(cid:105)}

1
n

= min
i∈[n]
=1 − 1
α
≥1 > γ

Then, L = [n] and w∗(α, β) = 1
Hence, they are the optimal solution for the primal and dual problems (P∗) and (D∗).

¯Xθ∗(α, β)) and θ∗(α, β) = 1 satisfy the conditions KKT-1 and KKT-2.

αSβ( 1

n

2. Case 2: αmax(β) > 0. Then for any α ≥ αmax(β), we have

{1 − (cid:104)¯xi, w∗(α, β)(cid:105)}

min
i∈[n]

= min
i∈[n]
=1 − 1
α

(cid:104)¯xi,Sβ(

{1 − 1
α
(cid:104)¯xi,Sβ(

max
i∈[n]

1
n
¯X1)(cid:105) = 1 − (1 − γ)
1
n

¯Xθ∗(α, β))(cid:105)} = min
i∈[n]
1
α

{1 − 1
α

(cid:104)¯xi,Sβ(

¯X1)(cid:105)}

1
n

αmax(β) ≥ 1 − (1 − γ) = γ.

Thus, E ∪ L = [n] and w∗(α, β) = 1
Hence, they are the optimal solution for the primal and dual problems (P∗) and (D∗).

¯Xθ∗(α, β)) and θ∗(α, β) = 1 satisfy the conditions KKT-1 and KKT-2.

αSβ( 1

n

The proof is complete.
K. Proof for Theorem 6
Proof. of Theorem 6:
(1) Given the reference solutions pair w∗(αi−1,j, βj) and θ∗(αi−1,j, βj), if we do ISS ﬁrst in SIFS and apply ISS and IFS
for inﬁnite times. If after p times of triggering, no new inactive features or samples are identiﬁed, then we can denote the
sequence of ˆF, ˆR and ˆL as:
ˆF A
0 = ˆRA

0 = ∅ ISS−→ ˆF A

IF S/ISS−→ ...

ISS−→ ... ˆF A

0 = ˆLA

IF S−→ ˆF A

1 , ˆRA

2 , ˆRA

p , ˆRA

2 , ˆLA

p , ˆLA

(32)

1 , ˆLA

1

p

2

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

with ˆF A

p = ˆF A

p+1 = ˆF A

p+2 = ..., ˆRA

p = ˆRA

p+1 = ˆRA

p+2 = ..., and ˆLA

p = ˆLA

p+1 = ˆLA

p+2 = ...

(33)

In the same way, if we do IFS ﬁrst in SIFS and no new inactive feature or samples are identiﬁed after q times of triggering
of ISS and IFS, then the sequence can be denoted as:
1 , ˆRB

0 = ∅ IF S−→ ˆF B

ˆF B
0 = ˆRB

IF S/ISS−→ ...

IF S−→ ... ˆF B

0 = ˆLB

ISS−→ ˆF B

2 , ˆRB

q , ˆRB

1 , ˆLB

2 , ˆLB

q , ˆLB

(34)

1

2

q

q = ˆLB

q+1 = ˆLB

q+2 = ..., and ˆLB

q+1 = ˆRB
k+1 hold for all k ≥ 0 by induction.
0 ⊆ ˆLA
0 = ˆLB

q = ˆRB
k ⊆ ˆLA
1 and ˆLB
k+1 hold, by the synergy effect of ISS and IFS, we have ˆF B

1 hold since ˆF B

0 = ˆRB

0 = ∅.

q+2 = ...

k+1 ⊆

(35)

k+1 hold for all k ≥ 0.

k ⊆ ˆF B

k+1, ˆRA

k ⊆ ˆRB

k+1 and ˆLA

k ⊆ ˆLB

k+1 hold for all k ≥ 0.

q+2 = ..., ˆRB

q = ˆF B
k ⊆ ˆF A

q+1 = ˆF B
k+1, ˆRB

with ˆF B
We ﬁrst prove that ˆF B
1) When k = 0, the equalities ˆF B
k ⊆ ˆRA
k+1, ˆRB
2) If ˆF B
k+2, ˆRB
ˆF A
k+2 and ˆLB
k+1, ˆRB
k ⊆ ˆRA
Thus, ˆF B
Similar with the analysis in (1), we can also prove that ˆF A
Combine (1) and (2), we can get

k ⊆ ˆRA
0 ⊆ ˆF A
1 , ˆRB
k+1 and ˆLB
k+1 ⊆ ˆLA
k+2 hold.
k+1 and ˆLB
k ⊆ ˆLA

k+1 and ˆLB
0 ⊆ ˆRA
k ⊆ ˆLA

k ⊆ ˆF A
k+1 ⊆ ˆRA
k ⊆ ˆF A

ˆF B
0 ⊆ ˆF A
ˆF A
0 ⊆ ˆF B
ˆRB
0 ⊆ ˆRA
ˆRA
0 ⊆ ˆRB
ˆLB
0 ⊆ ˆLA
ˆLA
0 ⊆ ˆLB
by the ﬁrst equality of (33), (36) and (37), we can get ˆF A
(2) If p is odd, then by (36), (38 and (40), we have ˆF A
Else if p is even, then by (37), (39) and (41), we have ˆF A
Do the same analysis for q, we can get p ≤ q + 1.
Hence, |p − q| ≤ 1.
The proof is complete.

1 ⊆ ˆF B
1 ⊆ ˆF A
1 ⊆ ˆRB
1 ⊆ ˆRA
1 ⊆ ˆLB
1 ⊆ ˆLA
p = ˆF B
p ⊆ ˆF B

3 ....
3 ....
3 ....
3 ....
3 ....
3 ....

2 ⊆ ˆF A
2 ⊆ ˆF B
2 ⊆ ˆRA
2 ⊆ ˆRA
2 ⊆ ˆLA
2 ⊆ ˆLB
q . Similarly, we can get ˆRA
p ⊆ ˆLB
p+1, ˆRA

p+1 and ˆLA

p+1 and ˆLA

p ⊆ ˆRB

p ⊆ ˆRB

p+1, ˆRA

p ⊆ ˆF B

q and ˆLA

p = ˆLB
p = ˆRB
q .
p+1. Thus q ≤ p + 1.
p+1. Thus q ≤ p + 1.

p ⊆ ˆLB

(36)
(37)
(38)
(39)
(40)
(41)

L. Experiment Result
L.1. Veriﬁcation of the Synergy Effect
Here, we verify the synergy effect between ISS and IFS in SIFS from the experiment results on the dataset real-sim. In
Fig. 4, SIFS performs ISS (sample screening) ﬁrst, while in Fig. 5, it performs IFS (feature screening) ﬁrst. All the rejection
ratios (Fig. 4(a)-(d)) of the 1st triggering of IFS when SIFS performs ISS ﬁrst are much higher than (at least equal to) those
(Fig. 5(a)-(d)) when SIFS performs IFS ﬁrst. In turn, all the rejection ratios (Fig. 5(e)-(h)) of the 1st triggering of ISS when
SIFS performs IFS ﬁrst are also much higher than those (Fig. 4(e)-(h)) when SIFS performs ISS ﬁrst. This demonstrates
that the screening result of ISS can reinforce the capability of IFS and vice versa, which is the so called synergy effect. At
last, in Fig. 5 and Fig. 4, we can see that the overall rejection ratios at the end of SIFS are the same, so no matter which (ISS
or IFS) we perform ﬁrst in SIFS, SIFS has the same screening performances in the end. This is consistent with Theorem 6.
L.2. The Rest Experiment Result
Below, we report the rejection ratios of SIFS on syn1 (Fig. 6), syn3 (Fig. 7), rcv1-train (Fig. 8), rcv1-test(Fig. 9), url
(Fig. 10) and kddb (Fig. 11), which are omitted in the main text due to the space limitation.

Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 4. Rejection ratios of SIFS on real-sim when it performs ISS ﬁrst (ﬁrst row: Feature Screening, second row: Sample Screening).

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 5. Rejection ratios of SIFS on real-sim when it performs IFS ﬁrst(ﬁrst row: Feature Screening, second row: Sample Screening).

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 6. Rejection ratios of SIFS on syn1 (ﬁrst row: Feature Screening, second row: Sample Screening).

α/αmax0.010.030.10.41Rejection Ratio0.9850.990.9951Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.9850.990.9951Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.970.980.991Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.9850.990.9951Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.960.981Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.960.981Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.960.981Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.9850.990.9951Trigger 1Trigger 2Trigger 3Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 7. Rejection ratios of SIFS on syn3 (ﬁrst row: Feature Screening, second row: Sample Screening).

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 8. Rejection ratios of SIFS on rcv1-train dataset (ﬁrst row: Feature Screening, second row: Sample Screening).

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 9. Rejection ratios of SIFS on rcv1-test dataset (ﬁrst row: Feature Screening, second row: Sample Screening).

α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio00.51Trigger 1Trigger 2Trigger 3α/αmax0.010.030.10.41Rejection Ratio0.9850.990.9951Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio0.990.9951Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio0.990.9951Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio0.990.9951Round 1Round 2Round 3Scaling Up Sparse Support Vector Machines by Simultaneous Feature and Sample Reduction

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 10. Rejection ratios of SIFS on url dataset (ﬁrst row: Feature Screening, second row: Sample Screening).

(a) β/βmax=0.05

(b) β/βmax=0.1

(c) β/βmax=0.5

(d) β/βmax=0.9

(e) β/βmax=0.05

(f) β/βmax=0.1

(g) β/βmax=0.5

(h) β/βmax=0.9

Figure 11. Rejection ratios of SIFS on kddb dataset (ﬁrst row: Feature Screening, second row: Sample Screening).

α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3α/αmax0.010.030.10.41Rejection Ratio00.51Round 1Round 2Round 3