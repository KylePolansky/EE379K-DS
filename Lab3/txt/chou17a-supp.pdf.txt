Improving Stochastic Policy Gradients in Continuous Control with Deep Reinforcement Learning using the Beta Distribution

A. Fisher information matrix for the Normal Distribution
Under regularity conditions (Wasserman, 2013), the Fisher information matrix can also be obtained from the second-order
partial derivatives of the log-likelihood function

I(θ) = −E[

∂2l(θ)
∂θ2 ] ,

where l(θ) = log πθ(a|s). This gives us the Fisher information for the Normal distribution

I(µ, σ) = −Ea∼πθ� ∂2l
= −Ea∼πθ� − 1
−2 (a−µ)

∂µ2
∂2l
∂σ∂µ

σ2

σ3

∂2l
∂µ∂σ
∂2l

∂σ2 �

−2 (a−µ)

σ3
−3(a−µ)2

σ4

+ 1

σ2 � =� 1

σ2
0

0
2

σ2 � .

(D1)

(D2)

B. Fisher information matrix for the Beta Distribution
To see how variance changes as the policy converges and becomes more deterministic, let us ﬁrst compute the partial
derivative of log πθ(a|s) with respect to shape parameter α

∂ log πθ(a|s)

∂α

=

=

∂
∂α
∂
∂α

log� Γ(α + β)

Γ(α)Γ(β)

aα−1(1 − a)β−1�

(log Γ(α + β) − log Γ(α) + (α − 1) log a)

= log a + ψ(α + β) − ψ(α) ,

where ψ(·) = d

dz log Γ(·) is the digamma function. Similar results can also be derived for β. From (D1), we have

∂α2
∂2l
∂β∂α

∂2l
∂α∂β
∂2l

∂β2 �

I(α, β) = −Ea∼πθ� ∂2l
= −Ea∼πθ� ∂
=� ψ�(α) − ψ�(α + β)
−ψ�(α + β)
dzm+1 log Γ(z) is the polygamma function of order m. Figure 7 shows how ψ�(z)

∂β (ψ(α + β) − ψ(β)) �

ψ�(β) − ψ�(α + β) � ,
−ψ�(α + β)

∂α (ψ(α + β) − ψ(α))

∂
∂α (ψ(α + β))

∂
∂β (ψ(α + β))

∂

where ψ�(z) = ψ(1)(z) and ψ(m)(z) = dm+1
goes to 0 as z → ∞.

Figure 7. Graphs of the digamma function, the polygamma function, and the harmonic series. The harmonic series and the digamma

k=1

1

k = ψ(z + 1) + γ, where γ = 0.57721··· is the Euler-Mascheroni constant.

function are related by�z

